{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dec0f5a",
   "metadata": {},
   "source": [
    "# Process Subdirectories III\n",
    "\n",
    "#### Updated: Dec 14, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5c58f",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a2cd08",
   "metadata": {},
   "source": [
    "Test multithreaded processing of subdirectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf593cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from random import randrange\n",
    "import threading\n",
    "import glob\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a72f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****ADD scan2data folder to sys.path*****\n",
    "sys.path.append('C:/Users/rnaidoo/Documents/Projects/Alouette_I/code/Alouette_extract/scan2data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e924b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a4171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b80355",
   "metadata": {},
   "source": [
    "Set parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dafe27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'Rav_local'\n",
    "process_on_VDI = False\n",
    "wait = 2 #in minutes\n",
    "stop_loop_threshold = 2640 #max while loops to prevent infinite loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a084fe90",
   "metadata": {},
   "source": [
    "Set directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "279c1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDir_local = 'C:/Users/rnaidoo/Documents/Projects_data/Alouette_I/' #C: is not persistent on VDI\n",
    "rootDir_U = 'U:/Data_Science/Projects_data/Alouette_I/'\n",
    "downloadedDir = rootDir_local + '02_downloaded/'\n",
    "processingDir = rootDir_local + '03_processing/'\n",
    "if process_on_VDI:\n",
    "    processedDir = rootDir_U + '04_processed/' \n",
    "    resultDir = rootDir_U + '05_result/' \n",
    "    logDir = '//scientific/L-MP-Data/Massive files/Python/rnaidoo/Alouette_I/' #DO NOT CHANGE\n",
    "    move_to_U = True\n",
    "else:\n",
    "    processedDir = rootDir_local + '04_processed/' \n",
    "    resultDir = rootDir_local + '05_result/' \n",
    "    logDir = resultDir\n",
    "    move_to_U = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fecfb33",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c273d3e",
   "metadata": {},
   "source": [
    "#### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b26eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_images(old_dir, new_dir, roll, subdir, copy_to_other_drive=False):\n",
    "    oldDir = old_dir + roll + '/' + subdir + '/'\n",
    "    newDir = new_dir + roll + '/' + subdir + '/'\n",
    "    os.makedirs(newDir, exist_ok=True)\n",
    "    \n",
    "    if copy_to_other_drive:\n",
    "        for file in os.listdir(oldDir):\n",
    "            shutil.copyfile(oldDir+file, newDir+file)\n",
    "    else:\n",
    "        for file in os.listdir(oldDir):\n",
    "            os.rename(oldDir+file, newDir+file)\n",
    "    \n",
    "    shutil.rmtree(old_dir + roll + '/' + subdir + '/')\n",
    "    if len(os.listdir(old_dir + roll + '/')) == 0:\n",
    "        shutil.rmtree(old_dir + roll + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce30e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Alouette_processor(thread, user, processingDir):\n",
    "    \n",
    "    stop_condition = False\n",
    "    stop_condition_counter = 0\n",
    "\n",
    "    while stop_condition == False:\n",
    "        if len(os.listdir(downloadedDir)) > 0:\n",
    "            df_result = pd.DataFrame()\n",
    "            for roll in os.listdir(downloadedDir):\n",
    "                if 'R' in roll:\n",
    "                    for subdirectory in os.listdir(downloadedDir + roll):\n",
    "                        start = time.time()\n",
    "                        subdir_path_end = roll + '/' + subdirectory + '/'\n",
    "\n",
    "                        #Move to '03_processing'\n",
    "                        move_images(old_dir=downloadedDir, new_dir=processingDir, roll=roll, subdir=subdirectory)\n",
    "\n",
    "                        #Process\n",
    "                        print('')\n",
    "                        print('[Thread ' + str(thread) + '] Processing ' + subdir_path_end + ' subdirectory...')\n",
    "                        #!python scan2data/user_input.py $processingDir $resultDir\n",
    "                        regex_raw = '*.png'\n",
    "                        list_all_subdir = glob.glob(processingDir + 'R*/[0-9]*/')\n",
    "                        for sample_subdir in list_all_subdir:\n",
    "                            sample_subdir = sample_subdir.replace('/', '\\\\')\n",
    "                            sample_subdir = sample_subdir.replace('\\\\', '/')\n",
    "                            process_directory.process_extract_management(resultDir, processingDir, regex_raw, sample_subdir)\n",
    "                        \n",
    "                        #Consolidate results\n",
    "                        if os.path.exists(resultDir + 'df_dot.csv'):\n",
    "                            df_dot = pd.read_csv(resultDir + 'df_dot.csv')\n",
    "                            n_dot = len(df_dot)\n",
    "                            df_dot['processed_image_class'] = 'dot'\n",
    "                            os.remove(resultDir + 'df_dot.csv')\n",
    "                        else:\n",
    "                            df_dot = pd.DataFrame()\n",
    "                            n_dot = 0\n",
    "\n",
    "                        if os.path.exists(resultDir + 'df_num.csv'):\n",
    "                            df_num = pd.read_csv(resultDir + 'df_num.csv')\n",
    "                            n_num = len(df_num)\n",
    "                            df_num['processed_image_class'] = 'num'\n",
    "                            os.remove(resultDir + 'df_num.csv')\n",
    "                        else:\n",
    "                            df_num = pd.DataFrame()\n",
    "                            n_num = 0\n",
    "\n",
    "                        if os.path.exists(resultDir + 'df_loss.csv'):\n",
    "                            df_loss = pd.read_csv(resultDir + 'df_loss.csv')\n",
    "                            n_loss = len(df_loss)\n",
    "                            df_loss['processed_image_class'] = 'loss'\n",
    "                            os.remove(resultDir + 'df_loss.csv')\n",
    "                        else:\n",
    "                            df_loss = pd.DataFrame()\n",
    "                            n_loss = 0\n",
    "\n",
    "                        if os.path.exists(resultDir + 'df_outlier.csv'):\n",
    "                            df_outlier = pd.read_csv(resultDir + 'df_outlier.csv')\n",
    "                            n_outlier = len(df_outlier)\n",
    "                            df_outlier['processed_image_class'] = 'outlier'\n",
    "                            os.remove(resultDir + 'df_outlier.csv')\n",
    "                        else:\n",
    "                            df_outlier = pd.DataFrame()\n",
    "                            n_outlier = 0\n",
    "\n",
    "                        df_tot = pd.concat([df_dot, df_num, df_loss, df_outlier])\n",
    "                        df_tot['Roll'] = roll\n",
    "                        df_tot['Subdirectory'] = subdirectory\n",
    "                        df_tot['filename'] = df_tot['file_name'].str.replace(processingDir + roll + '/' + subdirectory, '')\n",
    "                        df_tot['filename'] = df_tot['filename'].str.replace('\\\\', '')\n",
    "                        df_tot['filename'] = df_tot['filename'].str.replace('/', '')\n",
    "                        df_tot = df_tot.drop(columns=['file_name', 'mapped_coord', 'subdir_name', 'raw', 'ionogram', 'raw_metadata', \n",
    "                                                      'trimmed_metadata', 'padded', 'dilated_metadata'], errors='ignore')\n",
    "                        os.makedirs(resultDir + roll + '/', exist_ok=True)\n",
    "                        df_tot.to_csv(resultDir + roll + '/' + 'result-' + roll + '_' + subdirectory + '.csv', index=False)\n",
    "\n",
    "                        end = time.time()\n",
    "                        t = end - start\n",
    "                        print('[Thread ' + str(thread) + '] Processing time for subdirectory: ' + str(round(t/60, 1)) + ' min')\n",
    "                        print('')\n",
    "\n",
    "                        #Record performance\n",
    "                        df_result_ = pd.DataFrame({\n",
    "                            'Roll': roll,\n",
    "                            'Subdirectory': subdirectory,\n",
    "                            'Images_processed': n_dot + n_num + n_loss + n_outlier,\n",
    "                            'Images_dot': n_dot,\n",
    "                            'Images_num': n_num,\n",
    "                            'Images_loss': n_loss,\n",
    "                            'Images_outlier': n_outlier,\n",
    "                            'Process_time': t,\n",
    "                            'Process_timestamp': datetime.fromtimestamp(end),\n",
    "                            'User': user,\n",
    "                            'subdir_id': roll + '_' + subdirectory\n",
    "                        }, index=[0])\n",
    "                        df_result = pd.concat([df_result, df_result_], axis=0, ignore_index=True)\n",
    "                        if os.path.exists(logDir + 'process_log.csv'):\n",
    "                            df_log = pd.read_csv(logDir + 'process_log.csv')\n",
    "                            df_update = pd.concat([df_log, df_result], axis=0, ignore_index=True)\n",
    "                            df_update.to_csv(logDir + 'process_log.csv', index=False)\n",
    "                        else:\n",
    "                            if len(df_result) > 0:\n",
    "                                df_result.to_csv(logDir + 'process_log.csv', index=False)\n",
    "\n",
    "                        #Backup 'process_log' (10% of the time)\n",
    "                        if randrange(10) == 7:\n",
    "                            df_log = pd.read_csv(logDir + 'process_log.csv')\n",
    "                            datetime_str = datetime.now().strftime(\"%Y%m%d_%Hh%M\")\n",
    "                            os.makedirs(logDir + 'backups/', exist_ok=True)\n",
    "                            df_log.to_csv(logDir + 'backups/' + 'process_log-' + datetime_str + '.csv', index=False)\n",
    "\n",
    "                        #Move to '04_processed'\n",
    "                        #print(\"Moving images to '04_processed'\")\n",
    "                        move_images(old_dir=processingDir, new_dir=processedDir, roll=roll, subdir=subdirectory, copy_to_other_drive=move_to_U)\n",
    "\n",
    "                        stop_condition_counter += 1\n",
    "\n",
    "        else:\n",
    "            #Wait\n",
    "            print('[Thread ' + str(thread) + '] Wait ' + str(wait) + ' min')\n",
    "            time.sleep(wait*60)\n",
    "\n",
    "\n",
    "        #Check stop conditions\n",
    "        if stop_condition_counter == stop_loop_threshold:\n",
    "            print('Stop!')\n",
    "            stop_condition = True\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c8f0e",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e974df",
   "metadata": {},
   "source": [
    "#### Test multithreading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65e1e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Thread 1] Processing R014207942/1973-5B/ subdirectory...\n",
      "\n",
      "[Thread 2] Processing R014207979F/288/ subdirectory...\n",
      "[Thread 2] Processing time for subdirectory: 0.0 min\n",
      "\n",
      "[Thread 2] Wait 2 min\n",
      "\n",
      "[Thread 2] Processing R014207844/2917-43B/ subdirectory...\n",
      "[Thread 2] Processing time for subdirectory: 0.0 min\n",
      "\n",
      "[Thread 2] Wait 2 min\n"
     ]
    }
   ],
   "source": [
    "t1 = threading.Thread(target=Alouette_processor, args=(1, user, processingDir))\n",
    "\n",
    "thread = 2\n",
    "t2 = threading.Thread(target=Alouette_processor, args=(thread, user + ' [thread ' + str(thread) + ']/', rootDir_local + '03_processing [thread ' + str(thread) + ']/'))\n",
    "\n",
    "t1.start()\n",
    "time.sleep(10)\n",
    "t2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b5074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
