{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISIS Gridding for Metadata (in-progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a random image\n",
    "Generates and displays a random image from subdirectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:/DATA/ISIS/ISIS_101300030772\\b33_R014207877\\B1-34-64 ISIS A C-1124\\Image0077.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def gen_ran_subdir (subdir_path):\n",
    "    \"\"\"\" Generates a random subdirectory\n",
    "    Requires: \n",
    "    subdir_path: name of the path for the subdirectory\n",
    "     L:/DATA/ISIS/ISIS_101300030772/b*/B1* \"\"\"\n",
    "\n",
    "    all_subs = glob.glob(subdir_path) #creates a list of all subdirects \n",
    "    selected_sub = all_subs[random.randint(0, len(all_subs)-1)] #picks a random one from list\n",
    "    return (selected_sub)\n",
    "\n",
    "def gen_ran_img (subdir_path, img):\n",
    "    \"\"\"\" Generates a random image \"\"\"\n",
    "\n",
    "    all_img = glob.glob(subdir_path + img) #creates list of all images\n",
    "\n",
    "    selected_img = all_img[random.randint(0, len(all_img) - 1)]\n",
    "    return (selected_img)\n",
    "\n",
    "img_path = ((gen_ran_img(gen_ran_subdir(\"L:/DATA/ISIS/ISIS_101300030772/b*/B1*\"),\"/*\")))\n",
    "\n",
    "# Display\n",
    "print (img_path)\n",
    "img = mpimg.imread(img_path)\n",
    "cv2.imshow(\"image\", img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Crop image\n",
    "# Need to crop bottom ~20%\n",
    "\n",
    "height, width = img.shape[0:2]\n",
    "\n",
    "y, x = img.shape[0], img.shape[1]\n",
    "h = int(y*0.85)\n",
    "\n",
    "crop_img = img[h:y, 0:x]\n",
    "cv2.imshow(\"cropped image\", crop_img)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:/DATA/ISIS/ISIS_101300030772\\b7_R014207896\\B1-34-50 ISIS A C-279-37\n",
      "L:/DATA/ISIS/ISIS_101300030772\\b7_R014207896\\B1-34-50 ISIS A C-279-37\\Image0112.png\n"
     ]
    }
   ],
   "source": [
    "#SD_PATH = gen_ran_subdir(\"L:/DATA/ISIS/ISIS_101300030772/b*/B1*\")\n",
    "#I_PATH = gen_ran_img(SD_PATH, '/*')\n",
    "#print ((SD_PATH))\n",
    "#print (I_PATH)\n",
    "\n",
    "#For testing purposes\n",
    "SD_PATH = \"L:/DATA/ISIS/ISIS_101300030772/b7_R014207896/B1-34-50 ISIS A C-258\"\n",
    "I_PATH = gen_ran_img(SD_PATH, '/*')\n",
    "print ((SD_PATH))\n",
    "print (I_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From helper_functions.py\n",
    "def record_loss (df, function_name, subdir_location, columns_to_extract=['file_name'], loss_extraction=[]):\n",
    "    \"\"\"Create dataframe that records loss.\"\"\"\n",
    "\n",
    "    if len(loss_extraction) == 0:\n",
    "        #should return NA if there's an error\n",
    "        loss_extraction = df.isna().any(axis = 1)\n",
    "\n",
    "    df_loss_extraction = df[loss_extraction].copy()\n",
    "    df_loss_extraction = df_loss_extraction[columns_to_extract]\n",
    "    df_loss_extraction[\"func_name\"] = function_name\n",
    "    df_loss_extraction[\"subdir_name\"] = subdir_location\n",
    "\n",
    "    return df_loss_extraction, loss_extraction\n",
    "\n",
    "\n",
    "# location?\n",
    "def metadata_location(meta, min_count = 50, max_count = 1000):\n",
    "    \"\"\"\"Use connected component algorithm to find the location of the metadata\"\"\"\n",
    "    \n",
    "    #run algorithm on metadata section\n",
    "    _, labelled = cv2.connectedComponents(meta)\n",
    "\n",
    "    #Dictionary of label:counts\n",
    "    unique, counts = np.unique(labelled, return_counts = True)\n",
    "    dict_components = dict(zip(unique, counts))\n",
    "\n",
    "    #Remove outliers // Remove pixels not part of metadata\n",
    "    dict_subset = {}\n",
    "    dict_outlier = {}\n",
    "    for k,v, in dict_components.items():\n",
    "        if v > min_count and v < max_count:\n",
    "            dict_subset[k] = v\n",
    "        else:\n",
    "            dict_outlier[k] = v\n",
    "    \n",
    "    key_list_to_remove = list(dict_outlier.keys())\n",
    "    if len(key_list_to_remove) != 0:\n",
    "        for k in key_list_to_remove:\n",
    "            labelled[labelled == k] = 0\n",
    "    \n",
    "    return labelled\n",
    "\n",
    "#From scan2data > image_segmentation > trim_raw_metadata\n",
    "#test provided values and change if needed\n",
    "def bottomside_metadata_trimming(connected_meta, opened_meta,\n",
    "                                 h_window = 100, w_window = 700, starting_y = 0, starting_x = 15, step_size = 10, trim_if_small = 10):\n",
    "    \"\"\"Sliding window method to locate and trim bottomside metadata\"\"\"\n",
    "    \n",
    "    def sliding_window(image, starting_y, starting_x, h_window, w_window, step_size):\n",
    "        \"\"\"sliding window method\"\"\"\n",
    "        h_img, w_img = np.shape(image)\n",
    "        for y in range(starting_y, h_img - h_window, step_size):\n",
    "            for x in range(starting_x, w_ing - w_window, step_size):\n",
    "                yield y,x,image[y:y + h_window, x:x + w_window]\n",
    "    \n",
    "    h_raw,w_raw = opened_meta.shape\n",
    "    \n",
    "    if h_window + step_size  >= h_raw:\n",
    "        h_window = h_raw -trim_if_small\n",
    "    if w_window + step_size>= w_raw:\n",
    "        w_window = w_raw -trim_if_small\n",
    "    \n",
    "    s = sliding_window(connected_meta, starting_y, starting_x, h_window, w_window, step_size)\n",
    "    \n",
    "    max_window = connected_meta[starting_y:h_window+starting_y,\n",
    "                 starting_x:w_window+starting_x ]\n",
    "    max_mean = np.mean(max_window)\n",
    "    y_max= starting_y\n",
    "    x_max = starting_x\n",
    "    \n",
    "    for y, x, window in s:\n",
    "        tmp = window\n",
    "        mean = np.mean(tmp)\n",
    "        if mean > max_mean:\n",
    "            max_window = tmp\n",
    "            max_mean  = mean\n",
    "            y_max = y\n",
    "            x_max = x\n",
    "\n",
    "    trimmed_metadata =  opened_meta[y_max:y_max + h_window, x_max:x_max + w_window]\n",
    "\n",
    "    return trimmed_metadata\n",
    "\n",
    "#From scan2data > image_segmentation > trim_raw_metadata\n",
    "def trimming_metadata(raw_metadata, opening_kernal_size = (3,3), median_kernal_size = 5):\n",
    "    \"\"\"\"Trim the rectangle containing metadata to smallest workable area.\"\"\"\n",
    "\n",
    "    try:\n",
    "        #Filtering to reduce noise\n",
    "        median_filtered_meta = cv2.medianBlur(raw_metadata, median_kernal_size)\n",
    "        \n",
    "        #Opening operation: Eroision + Dilation\n",
    "        kernal_opening = np.ones(opening_kernal_size, dtype = np.uint8)\n",
    "        opened_meta = cv2.morphologyEx(median_filtered_meta, cv2.MORPH_OPEN, kernel_opening)\n",
    "\n",
    "        # Binarization\n",
    "        _, metadata_binary = cv2.threshold(opened_meta, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        #Run connected components algorithm\n",
    "        connected_meta = metadata_location(metadata_binary)\n",
    "\n",
    "        trimmed_metadata = bottomside_metadata_trimming(connected_meta, metadata_binary)\n",
    "        #function from somewhere else\n",
    "\n",
    "        #Checking\n",
    "        #cv2.imshow(\"test\", trimmed_metadata)\n",
    "        #cv2.waitKey(0)\n",
    "        return (trimmed_metadata)\n",
    "    except:\n",
    "        return (np.nan)\n",
    "    \n",
    "\n",
    "#location?\n",
    "#function is possible not needed -- can potentially remove\n",
    "def indices_highest_bin(list_coords):\n",
    "    \"\"\"\" returns indices of most common values using binning\n",
    "    list_coords: (np.arrray)\"\"\"\n",
    "\n",
    "    nbins = 50\n",
    "    peak_threshold = 0.2\n",
    "    distance_bwtn_peaks = 30\n",
    "\n",
    "    mean_coords = np.mean(list_coords)\n",
    "    std_coords = np.std(list_coords)\n",
    "    no_outlier_coords = list_coords[np.abs(list_coords - mean_coords) < 3 * std_coords]\n",
    "\n",
    "    #Binning\n",
    "    counts, bin_edges = np.histogram(no_outlier_coords, bins=nbins)\n",
    "\n",
    "    #Detect peaks\n",
    "    counts_norm = (counts - np.min(counts)) / (np.map(counts) - np.min(counts))\n",
    "    select_peaks = find_peaks(counts_norm, distance = distance_bwtn_peaks, promience = peak_threshold)    \n",
    "\n",
    "    return select_peaks, bin_edges, counts\n",
    "\n",
    "#location? \n",
    "def extract_centroids(cut_metadata):\n",
    "    \"\"\"Takes in cut metadata and extracts centroids\n",
    "    \n",
    "    cut_metadata: np.array\"\"\"\n",
    "\n",
    "    min_pixels = 50\n",
    "    max_pixels = 1000\n",
    "\n",
    "    _, __, stats, centroids = cv2.connectedComponentsWithStats(cut_metadata, 8) \n",
    "    area_centroids = stats[:,-1]\n",
    "\n",
    "    centroids_metadata = centroids[np.logical_and(area_centroids > min_pixels, area_centroids < max_pixels)]\n",
    "    col_centroids, row_centroids = zip(*centroids_metadata)\n",
    "\n",
    "    #Round to nearest integer\n",
    "    col_centroids = list(map(round, col_centroids))\n",
    "    row_centroids = list(map(round, row_centroids))\n",
    "\n",
    "    return col_centroids, row_centroids\n",
    "\n",
    "#From scan2data > image_segmentation > extract_ionogram_from_scan\n",
    "#For segment_metadata\n",
    "def limits_ionogram(raw_img, row_or_col, starting_index_col = 15):\n",
    "\n",
    "    mean_values = np.mean(raw_img, row_or_col)\n",
    "\n",
    "    #normalize mean\n",
    "    norm_mean = (mean_values - np.min(mean_values)) / np.max(mean_values)\n",
    "    thresh = np.mean(norm_mean)\n",
    "\n",
    "    if row_or_col == 0:\n",
    "        #Protect against scans that includes cuts from another ionogram\n",
    "        limits = [i for i, mean in enumerate(norm_mean) if mean > thresh and i > starting_index_col]\n",
    "    else:\n",
    "        limits = [i for i, mean in enumerate(norm_mean) if mean > thresh]\n",
    "\n",
    "    return limits[0], limits[-1]\n",
    "\n",
    "\n",
    "#From scan2data > image_segmentation > extract_ionogram_from_scan\n",
    "# For segment_metadata\n",
    "def extract_ionogram(raw_img_array):\n",
    "    \"\"\"\"this function is here for now to get limits of ionogram. \n",
    "    can later be changed to include ionogram graph\"\"\"\n",
    "    try:\n",
    "\n",
    "    #Extract coordinate delimiting the graph\n",
    "        x_left, x_right = limits_ionogram(raw_img_array, 0)\n",
    "        y_upper, y_lower = limits_ionogram(raw_img_array, 1)\n",
    "\n",
    "        limits = [x_left, x_right, y_upper, y_lower]\n",
    "        #ionogram = raw_img_array[y_upper:y_lower,x_left_:x_right]\n",
    "        return (limits)\n",
    "    except:\n",
    "        return (np.nan)\n",
    "\n",
    "\n",
    "#From scan2data > image_segmentation > extract_metadata_from_scan\n",
    "def extract_metadata (raw_img, limits_iono):\n",
    "    \"\"\"Extract metadata from raw scanned image and return coordinates delimiting its limits\"\"\"\n",
    "    \n",
    "    #Limits for ionogram\n",
    "    x_left_lim = limits_iono[0][0]\n",
    "    x_right_lim = limits_iono[0][1]\n",
    "    y_upper_lim = limits_iono[0][2] \n",
    "    y_lower_lim = limits_iono[0][3]\n",
    "\n",
    "    #Extract retangular block below** ionogram\n",
    "    rect_left = raw_img[:,0:x_left_lim]\n",
    "    rect_right = raw_img[:,x_right_lim::]\n",
    "    rect_top = raw_img[0:y_upper_lim, :]\n",
    "    rect_bottom = raw_img[y_lower_lim::,:]\n",
    "\n",
    "    #Assumption: the location of the metadata will correspond to rectangle with the highest area\n",
    "    rect_list = [rect_left, rect_right, rect_top, rect_bottom]\n",
    "    rect_areas = [rect.shape[0] * rect.shape[1] for rect in rect_list]\n",
    "    dict_mapping_meta = {0:'left', 1:\"right\", 2:\"top\", 3:'bottom'}\n",
    "\n",
    "    type_metadata_idx = np.argmax(rect_areas)\n",
    "    raw_metadata = rect_list[type_metadata_idx]\n",
    "    type_metadata = dict_mapping_meta[type_metadata_idx]\n",
    "\n",
    "    return (type_metadata, raw_metadata)\n",
    "\n",
    "def get_leftside_metadata_grid_mapping(list_x_dot,list_y_dot,list_x_digit,list_y_digit,dir_name,\n",
    "                      difference_ratio=0.75,use_defaults=True):\n",
    "    \n",
    "    \"\"\"Determines and returns the the mapping between coordinate values on a metadata image and metadata labels in a subdirectory, \n",
    "    for metadata of types dot and digits, as well as returns the histogram used to generate each mapping\n",
    "    \n",
    "    \"\"\"\n",
    "    # Dictionary of dictionaries that map labels to coordinate point in metadata\n",
    "    all_labels = [LABELS_CAT_DOT,LABELS_NUM ,LABELS_CAT_DIGIT,LABELS_NUM]\n",
    "    all_dict_mapping = {}\n",
    "    all_dict_hist = {}\n",
    "    # Different protocols depending on the type of dictionary mappings\n",
    "    for i, list_coord in enumerate([list_x_dot,list_y_dot,list_x_digit,list_y_digit]):\n",
    "        type_dict = LABELS_DICT[i]\n",
    "        labels = all_labels[i]\n",
    "        try:\n",
    "            if 'cat' in type_dict:\n",
    "                if type_dict == 'dict_cat_digit':\n",
    "                    if any([dir_dot in dir_name for dir_dot in LIST_DIRECTORY_DOTS]):\n",
    "                        mean_dist_default,first_peak_default,last_peak_default=DEFAULT_DICT_CAT_DIGIT_F\n",
    "                    else:\n",
    "                        mean_dist_default,first_peak_default,last_peak_default=DEFAULT_DICT_CAT_DIGIT\n",
    "            \n",
    "                elif type_dict == 'dict_cat_dot':\n",
    "                    mean_dist_default,first_peak_default,last_peak_default=DEFAULT_DICT_CAT_DOT\n",
    "                try:\n",
    "                    idx_peaks,bin_edges,counts = indices_highest_bin(list_coord)\n",
    "                    peaks = bin_edges[np.array(idx_peaks)] #coordinate values on a metadata image probably corresponding to metadata\n",
    "                    \n",
    "                    n_labels = len(labels)\n",
    "                    first_peak = peaks[0]\n",
    "                    last_peak = peaks[-1]\n",
    "\n",
    "                    if use_defaults and abs(last_peak -last_peak_default)  > difference_ratio*mean_dist_default:\n",
    "                        last_peak = last_peak_default\n",
    "                    if use_defaults and abs(first_peak -first_peak_default)  > difference_ratio*mean_dist_default:\n",
    "                        first_peak = first_peak_default\n",
    "                        \n",
    "                    mean_dist_btw_peaks = (last_peak - first_peak)/(n_labels -1)\n",
    "                    list_peaks = [int(round(first_peak + i* mean_dist_btw_peaks)) for i in range(0,n_labels )]\n",
    "                    \n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] = (idx_peaks,bin_edges,counts)\n",
    "                \n",
    "\n",
    "                except:\n",
    "                    last_peak = last_peak_default\n",
    "                    first_peak = first_peak_default\n",
    "                    mean_dist_btw_peaks = mean_dist_default\n",
    "                    list_peaks = [int(round(first_peak + i* mean_dist_btw_peaks)) for i in range(0,n_labels )]\n",
    "                    \n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] = {}\n",
    "                \n",
    "            elif 'num' in type_dict:\n",
    "                if  type_dict == 'dict_num_digit':\n",
    "                    if any([dir_dot in dir_name for dir_dot in LIST_DIRECTORY_DOTS]):\n",
    "                        mean_dist_default,peak_0_default,dist_btw_peaks = DEFAULT_DICT_NUM_DIGIT_F\n",
    "                    else:\n",
    "                        mean_dist_default,peak_0_default,dist_btw_peaks = DEFAULT_DICT_NUM_DIGIT\n",
    "                elif type_dict == 'dict_num_dot':\n",
    "                    mean_dist_default,peak_0_default,dist_btw_peaks= DEFAULT_DICT_NUM_DOT\n",
    "\n",
    "                    \n",
    "                try:\n",
    "                    idx_peaks,bin_edges,counts = indices_highest_peaks_hist_binning(list_coord,peak_prominence_threshold=0.3,nbins=100,distance_between_peaks=dist_btw_peaks)\n",
    "                \n",
    "                    peaks = bin_edges[np.array(idx_peaks)]                \n",
    "                    peak_0 = peaks[0]\n",
    "                    if use_defaults and abs(peak_0 -peak_0_default)  > difference_ratio*mean_dist_default:\n",
    "                        peak_0 = peak_0_default\n",
    "                \n",
    "                    # only first three peaks are deemed relevant\n",
    "                    if len(peaks) < 3:\n",
    "                        max_idx = 2\n",
    "                    else:\n",
    "                        max_idx = 3\n",
    "                \n",
    "                    mean_dist_btw_peaks = np.mean([peaks[i+1]-peaks[i] for i in range(0,max_idx)])\n",
    "                    if use_defaults and abs(mean_dist_btw_peaks - mean_dist_default)  > difference_ratio*dist_btw_peaks:\n",
    "                        mean_dist_btw_peaks = mean_dist_default\n",
    "                    list_peaks = [int(round(peak_0 + i* mean_dist_btw_peaks)) for i in range(0,len(labels))]\n",
    "                \n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] = (idx_peaks,bin_edges,counts)\n",
    "                except:\n",
    "                    peak_0 = peak_0_default\n",
    "                    mean_dist_btw_peaks = mean_dist_default\n",
    "                    list_peaks = [int(round(peak_0 + i* mean_dist_btw_peaks)) for i in range(0,len(labels))]\n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] =  {}\n",
    "        except:\n",
    "            all_dict_mapping[type_dict] ={}\n",
    "            all_dict_hist[type_dict] =  {}\n",
    "\n",
    "            \n",
    "    return all_dict_mapping,all_dict_hist\n",
    "\n",
    "#From scan2data> image_segmentation > segment_images_in_subdir.py\n",
    "#some variables here are not necessary and can be removed, ie. height, width...\n",
    "def segment_metadata(subdir_location, regex_img, min_bottom_height = 25, cutoff_width=300, cutoff_height=150):\n",
    "    \"\"\"Should only segment metadata. Can be adjusted to include ionogram. \"\"\"\n",
    "    regex_raw_image =  SD_PATH + (\"/*\")\n",
    "    #print (\"the raw images path is:\", regex_raw_image)\n",
    "    list_images = glob.glob(regex_raw_image)\n",
    "    \n",
    "    #Dataframe is processing\n",
    "    df_img = pd.DataFrame(data = {\"file_name\": list_images})\n",
    "    #Read each image in a 2D UTF-8 grayscale array\n",
    "    df_img[\"raw\"] = df_img['file_name'].map(lambda file_name: cv2.imread(file_name, 0))\n",
    "\n",
    "\n",
    "    # Extract ionogram and coordinates delimiting its limits\n",
    "    df_img['limits']= list(zip(df_img['raw'].map(lambda raw_img: extract_ionogram(raw_img)))) \n",
    "    \n",
    "    # Record the files whose ionogram extraction was not successful\n",
    "    df_loss_ion_extraction, loss_ion_extraction = record_loss(df_img,'image_segmentation.extract_ionogram_from_scan.extract_ionogram',subdir_location)\n",
    "    df_img = df_img[~loss_ion_extraction]\n",
    "    #df_img['height'],df_img['width'] = list(zip(df_img['ionogram'].map(lambda array_pixels: array_pixels.shape)))\n",
    "    \n",
    "    #Raw metadata\n",
    "    df_tmp = (df_img.apply(lambda row: extract_metadata(row['raw'], row['limits']), axis = 1, result_type = 'expand'))\n",
    "    df_img = df_img.assign(metadata_type = df_tmp[0])\n",
    "    df_img = df_img.assign(raw_metadata = df_tmp[1])\n",
    "    #extract_metadata is function from extract_metadata_from_scan\n",
    "    \n",
    "    # There should be no metadata on left and top, especially after flipping\n",
    "    outlier_metadata_location = np.any([df_img['metadata_type'] == 'right',df_img['metadata_type']=='top', df_img['metadata_type'] == 'left'], axis=0)\n",
    "    df_outlier_metadata_location ,_ =  record_loss(df_img,'image_segmentation.segment_images_in_subdir.segment_images: metadata not on left or bottom',subdir_location,\n",
    "                                         ['file_name','metadata_type'],outlier_metadata_location )\n",
    "    \n",
    "    if not df_outlier_metadata_location.empty:\n",
    "        df_outlier_metadata_location['details'] = df_outlier_metadata_location.apply(lambda row: str(row['metadata_type']),1)\n",
    "        df_outlier_metadata_location = df_outlier_metadata_location[['file_name','func_name','subdir_name','details']]\n",
    "    else:\n",
    "        df_outlier_metadata_location = df_outlier_metadata_location[['file_name','func_name','subdir_name']]\n",
    "    \n",
    "    # Remove loss from detected metadata not being on the left or bottom\n",
    "    df_img = df_img[~outlier_metadata_location]\n",
    "\n",
    "    #Trimmed metadata\n",
    "    #df_img['trimmed_metadata'] = list(zip(*df_img.apply(lambda row: trimming_metadata(row['raw_metadata'], row['metadata_type']), axis = 1, result_type = 'expand')))\n",
    "    df_trim_tmp = (df_img.apply(lambda row: trimming_metadata(row[\"raw_metadata\"], row['metadata_type']), axis = 1))\n",
    "    df_img = df_img.assign(trimmed_metadata = df_trim_tmp)\n",
    "    df_loss_trim, loss_trim = record_loss(df_img, 'image_segmentation.trim_raw_metadata.ntrimming_metadata', subdir_location)\n",
    "    #trimming_metadata is a function from trim_raw_metadata\n",
    "    #record_loss is a function from helper_functions\n",
    "    \n",
    "    df_img = df_img[~loss_trim]\n",
    "\n",
    "    # Check if metadata too small\n",
    "    #df_map_tmp = df_img['trimmed_metadata'].map(lambda array_pixels: np.shape(array_pixels), axis = 1)\n",
    "    df_map_tmp = df_img.apply(lambda row: np.shape(row['trimmed_metadata']), axis = 1)\n",
    "    df_img = df_img.assign(meta_height = df_map_tmp[0][0])\n",
    "    df_img = df_img.assign(meta_width = df_map_tmp[0][1])\n",
    "\n",
    "    outlier_size_metadata = (np.logical_and(df_img['metadata_type'] == 'bottom', df_img['meta_height'] < min_bottom_height))    \n",
    "        \n",
    "    df_outlier_metadata_size, _ = record_loss(df_img,'image_segmentation.segment_images_in_subdir.segment_images: metadata size outlier',subdir_location,\n",
    "                                           ['file_name','metadata_type','meta_height','meta_width'],outlier_size_metadata)\n",
    "\n",
    "    if not df_outlier_metadata_size.empty:\n",
    "        df_outlier_metadata_size['details'] = df_outlier_metadata_size.apply(lambda row: row['metadata_type'] + '_height: ' + \\\n",
    "                                                    str(row['meta_height'])+',width: ' + str(row['meta_width']),1)\n",
    "        df_outlier_metadata_size = df_outlier_metadata_size[['file_name','func_name','subdir_name','details']]\n",
    "        \n",
    "    else:\n",
    "        df_outlier_metadata_size = df_outlier_metadata_size[['file_name','func_name','subdir_name']]\n",
    "    \n",
    "    # Remove files whose metadata too small\n",
    "    df_img = df_img[~outlier_size_metadata]\n",
    "    \n",
    "    # Dataframe recording loss from programming errors\n",
    "    df_loss = pd.concat([df_loss_ion_extraction, df_loss_trim])\n",
    "    \n",
    "    # Dataframe recording loss from various filters i.e. metadata too small, ionogram too small/big\n",
    "    df_outlier = pd.concat([df_outlier_metadata_location, df_outlier_metadata_size])\n",
    "\n",
    "    ##FOR TESTING:\n",
    "    df_img.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/out.csv\")\n",
    "    print (df_img)\n",
    "\n",
    "    return df_img,  df_loss, df_outlier\n",
    "# function can also return df_loss, df_outlier\n",
    "\n",
    "#From scan2data > metadata_translation > translate_leftside_metadata.py\n",
    "def get_bottomside_metadata (df_img, subdir_location, kernal_size =(1, 1)):\n",
    "    \"\"\"Reads the metadata on the bottom\"\"\"\n",
    "\n",
    "    kernel_dilation = np.ones(kernal_size, np.uint8)\n",
    "\n",
    "    df_dilate_tmp = df_img.apply(lambda trimmed_meta: cv2.dilate(trimmed_meta, kernel_dilation))\n",
    "    df_img = df_img.assign(dilated_metadata = df_dilate_tmp)\n",
    "\n",
    "    #df_img['x_centroids'], df_img['y_centroids'], df_img['is_dot'] = zip(*df_img.apply(lambda row: extract_centroids(row['dilated_metadata'], row['file_name']), 1))\n",
    "    df_cent_tmp = df_img.apply(lambda row: extract_centroids(row['dilated_metadata'], row['file_name']), axis = 1)\n",
    "    df_img = df_img.assign(x_centroids = df_cent_tmp[0])\n",
    "    df_img = df_img.assign(y_centroids = df_cent_tmp[1])\n",
    "\n",
    "    df_loss_centroids_extraction, loss_centroids_extraction = record_loss(df_img,'metadata_translation.determine_metadata_grid_mapping.extract_centroids_',subdir_location)\n",
    "    # extract_centroids and record_loss are two other functions \n",
    "\n",
    "    # Remove files where the extraction didn't work\n",
    "    df_img = df_img[~loss_centroids_extraction]\n",
    "    # ^removes them from the main dataframe\n",
    "\n",
    "    df_num_subset = df_img[np.invert(np.array(df_img['is_data']))]\n",
    "\n",
    "    list_x_digit = list(chain(*df_num_subset['x_centroids'].tolist()))\n",
    "    list_y_digit = list(chain(*df_num_subset['y_centroids'].tolist()))\n",
    "    dict_mapping, dict_hist = get_leftside_metadata_grid_mapping(list_x_dot, list_y_dot, list_x_digit, list_y_digit,\n",
    "                                                                 subdir_location)\n",
    "\n",
    "    # Determine the value of metadata based on the mappings\n",
    "    df_img['dict_metadata'] = df_img.apply(lambda row:\n",
    "                                           map_coord_to_metadata(row['x_centroids'], row['y_centroids'],\n",
    "                                                                 dict_mapping['dict_cat_dot'],\n",
    "                                                                 dict_mapping['dict_num_dot']) if row['is_dot']\n",
    "                                           else map_coord_to_metadata(row['x_centroids'], row['y_centroids'],\n",
    "                                                                      dict_mapping['dict_cat_digit'],\n",
    "                                                                      dict_mapping['dict_num_digit']), 1)\n",
    "    df_loss_mapping, loss_mapping = record_loss(df_img,\n",
    "                                                'metadata_translation.translate_leftside_metadata.map_coord_to_metadata',\n",
    "                                                subdir_location)\n",
    "    df_img = df_img[~loss_mapping]\n",
    "\n",
    "    df_loss = pd.concat([df_loss_centroids_extraction, df_loss_mapping])\n",
    "\n",
    "    return df_img, df_loss, dict_mapping, dict_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\spunchiwickrama\\Documents\\Projects\\ISIS_I\\Alouette_extract\\ISIS Metadata Gridding.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     df_all_loss \u001b[39m=\u001b[39m df_all_loss\u001b[39m.\u001b[39mappend(df_loss_coord)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m df_processed, df_all_loss, df_outlier\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m (process_subdir(SD_PATH, \u001b[39m'\u001b[39;49m\u001b[39m/*\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mG:/spunchiwickrama/ISIS_I/output\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "\u001b[1;32mc:\\Users\\spunchiwickrama\\Documents\\Projects\\ISIS_I\\Alouette_extract\\ISIS Metadata Gridding.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Transform raw scanned images in subdir into information\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#Run segment_images on subdir\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m df_img, df_loss, df_outlier \u001b[39m=\u001b[39m segment_metadata(subdir_path, regex_images)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#Translate metadata on bottom\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m df_img_bottom \u001b[39m=\u001b[39m df_img[df_img[\u001b[39m'\u001b[39m\u001b[39mmetadata_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbottom\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;32mc:\\Users\\spunchiwickrama\\Documents\\Projects\\ISIS_I\\Alouette_extract\\ISIS Metadata Gridding.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=236'>237</a>\u001b[0m df_img[\u001b[39m'\u001b[39m\u001b[39mlimits\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(df_img[\u001b[39m'\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m raw_img: extract_ionogram(raw_img)))) \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=238'>239</a>\u001b[0m \u001b[39m# Record the files whose ionogram extraction was not successful\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=239'>240</a>\u001b[0m \u001b[39m#df_loss_ion_extraction, loss_ion_extraction = record_loss(df_img,'image_segmentation.extract_ionogram_from_scan.extract_ionogram',subdir_location)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=240'>241</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=241'>242</a>\u001b[0m \u001b[39m#df_img['height'],df_img['width'] = list(zip(df_img['ionogram'].map(lambda array_pixels: array_pixels.shape))) \u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=242'>243</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=243'>244</a>\u001b[0m \u001b[39m#Raw metadata\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=245'>246</a>\u001b[0m df_img[\u001b[39m'\u001b[39m\u001b[39mmetadata_type\u001b[39m\u001b[39m'\u001b[39m], df_img[\u001b[39m'\u001b[39m\u001b[39mraw_metadata\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(df_img\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: extract_metadata(row[\u001b[39m'\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39mlimits\u001b[39m\u001b[39m'\u001b[39m]), axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)) \u001b[39m### this guy \u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=246'>247</a>\u001b[0m \u001b[39m#extract_metadata is function from extract_metadata_from_scan\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=247'>248</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mdataframe with metdata stuff\u001b[39m\u001b[39m\"\u001b[39m, df_img)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "#From process_directory.py\n",
    "def process_subdir(subdir_path, regex_images, output_folder_if_pickle,\n",
    "                   to_pickle=True):\n",
    "    \"\"\"Transform raw scanned images in subdir into information\"\"\"\n",
    "\n",
    "    #Run segment_images on subdir\n",
    "    df_img, df_loss, df_outlier = segment_metadata(subdir_path, regex_images)\n",
    "\n",
    "    #Translate metadata on bottom\n",
    "    df_img_bottom = df_img[df_img['metadata_type'] == 'bottom']\n",
    "\n",
    "    df_img, df_loss_meta, dict_mapping, dict_hist = get_bottonside_metadata(df_img, subdir_path)\n",
    "    #get_bottomside_metadata is another function\n",
    "    df_all_loss = df_loss\n",
    "\n",
    "    #pickle \n",
    "    if to_pickle:\n",
    "        start, subdir_name = ntpath.split(subdir_path[:-1])\n",
    "        start, dir_name = ntpath.split(start)\n",
    "        df_processed.to_pickle(os.pardir + '/pickle/' + str(dir_name) + '_' + str(subdir_name) + '.pkl')\n",
    "\n",
    "    df_all_loss = df_all_loss.append(df_loss_coord)\n",
    "\n",
    "    return df_processed, df_all_loss, df_outlier\n",
    "\n",
    "print (process_subdir(SD_PATH, '/*', \"G:/spunchiwickrama/ISIS_I/output\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonV31013",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
