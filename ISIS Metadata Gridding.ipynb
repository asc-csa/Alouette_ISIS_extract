{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISIS Gridding for Metadata (in-progress)\n",
    "\n",
    "20231109 - Currently takes the first ISIS subdirectory and process all the images. The output shows up to the centroids extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_ran_subdir (subdir_path):\n",
    "    \"\"\"\" Generates a random subdirectory\n",
    "    Requires: \n",
    "    subdir_path: name of the path for the subdirectory\n",
    "     L:/DATA/ISIS/ISIS_101300030772/b*/B1* \"\"\"\n",
    "\n",
    "    all_subs = glob.glob(subdir_path) #creates a list of all subdirects \n",
    "    selected_sub = all_subs[random.randint(0, len(all_subs)-1)] #picks a random one from list\n",
    "    return (selected_sub)\n",
    "\n",
    "def gen_ran_img (subdir_path, img):\n",
    "    \"\"\"\" Generates a random image \"\"\"\n",
    "\n",
    "    all_img = glob.glob(subdir_path + img) #creates list of all images\n",
    "\n",
    "    selected_img = all_img[random.randint(0, len(all_img) - 1)]\n",
    "    return (selected_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subdir path: L:/DATA/ISIS/ISIS_101300030772/b7_R014207896/B1-34-50 ISIS A C-258\n",
      "Random image in subdir path L:/DATA/ISIS/ISIS_101300030772/b7_R014207896/B1-34-50 ISIS A C-258\\Image0039.png\n"
     ]
    }
   ],
   "source": [
    "#Sub-directory path\n",
    "#SD_PATH = gen_ran_subdir(\"L:/DATA/ISIS/ISIS_101300030772/b*/B1*\")\n",
    "\n",
    "#Image path\n",
    "#I_PATH = gen_ran_img(SD_PATH, '/*')\n",
    "\n",
    "#For testing - only use same sub-directory (subdir)\n",
    "SD_PATH = \"L:/DATA/ISIS/ISIS_101300030772/b7_R014207896/B1-34-50 ISIS A C-258\"\n",
    "I_PATH = gen_ran_img(SD_PATH, '/*')\n",
    "print (\"Subdir path:\", SD_PATH)\n",
    "print (\"Random image in subdir path\", I_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DICTIONARIES\n",
    "# Labelling of coordinates\n",
    "LABELS_NUM = [1,2,4,8]\n",
    "LABELS_CAT_DOT = ['day_1','day_2','day_3','hour_1','hour_2','minute_1','minute_2','second_1', 'second_2','station_code']\n",
    "#LABELS_CAT_DIGIT = ['satellite_number','year','day_1','day_2','day_3','hour_1','hour_2','minute_1','minute_2',\n",
    "                    #'second_1', 'second_2', 'station_number_1','station_number_2']\n",
    "LABELS_CAT_DIGIT = ['Operating Mode 1','Operating Mode 2','Station Number 1', 'Station Number 2', 'Year 1', 'Year 2', 'Day 1', 'Day 2', 'Day 3'\n",
    "                    'Hour 1', 'Hour 2', 'Min 1', 'Min 2', 'Sec 1', 'Sec 2']\n",
    "LABELS_DICT = ['dict_cat_dot','dict_num_dot','dict_cat_digit','dict_num_digit']\n",
    "\n",
    "#Defaults for dictionary mappings of coordinates to labels\n",
    "DEFAULT_DICT_CAT_DIGIT = (53,21,661) #mean_dist_default,first_peak_default,last_peak_default\n",
    "DEFAULT_DICT_NUM_DIGIT = (47,41,20) #mean_dist_default,first_peak_default,dist_btw_peaks for peak detection\n",
    "\n",
    "#DEFAULT_DICT_CAT_DIGIT_F = (43,23,540) #mean_dist_default,first_peak_default,last_peak_default for those in LIST_DIRECTORY_DOTS \n",
    "#DEFAULT_DICT_NUM_DIGIT_F = (40,37,20) #mean_dist_default,first_peak_default,dist_btw_peaks for peak detection for those in LIST_DIRECTORY_DOTS \n",
    "\n",
    "#DEFAULT_DICT_CAT_DOT = (59,20,549)##mean_dist_default,first_peak_default,last_peak_default\n",
    "#DEFAULT_DICT_NUM_DOT = (15,32,10) #mean_dist_default,first_peak_default,dist_btw_peaks for peak detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From helper_functions.py\n",
    "def record_loss (df, function_name, subdir_location, columns_to_extract=['file_name'], loss_extraction=[]):\n",
    "    \"\"\"Create dataframe that records loss.\"\"\"\n",
    "\n",
    "    if len(loss_extraction) == 0:\n",
    "        #should return NA if there's an error\n",
    "        loss_extraction = df.isna().any(axis = 1)\n",
    "\n",
    "    df_loss_extraction = df[loss_extraction].copy()\n",
    "    df_loss_extraction = df_loss_extraction[columns_to_extract]\n",
    "    df_loss_extraction[\"func_name\"] = function_name\n",
    "    df_loss_extraction[\"subdir_name\"] = subdir_location\n",
    "\n",
    "    return df_loss_extraction, loss_extraction\n",
    "\n",
    "\n",
    "# location?\n",
    "def metadata_location(meta, min_count = 50, max_count = 1000):\n",
    "    \"\"\"\"Use connected component algorithm to find the location of the metadata\"\"\"\n",
    "    \n",
    "    #run algorithm on metadata section\n",
    "    _, labelled = cv2.connectedComponents(meta)\n",
    "\n",
    "    #Dictionary of label:counts\n",
    "    unique, counts = np.unique(labelled, return_counts = True)\n",
    "    dict_components = dict(zip(unique, counts))\n",
    "\n",
    "    #Remove outliers // Remove pixels not part of metadata\n",
    "    dict_subset = {}\n",
    "    dict_outlier = {}\n",
    "    for k,v, in dict_components.items():\n",
    "        if v > min_count and v < max_count:\n",
    "            dict_subset[k] = v\n",
    "        else:\n",
    "            dict_outlier[k] = v\n",
    "    \n",
    "    key_list_to_remove = list(dict_outlier.keys())\n",
    "    if len(key_list_to_remove) != 0:\n",
    "        for k in key_list_to_remove:\n",
    "            labelled[labelled == k] = 0\n",
    "    \n",
    "    return labelled\n",
    "\n",
    "#From scan2data > image_segmentation > trim_raw_metadata\n",
    "#test provided values and change if needed\n",
    "def bottomside_metadata_trimming(connected_meta, opened_meta,\n",
    "                                 h_window = 100, w_window = 700, starting_y = 0, starting_x = 15, step_size = 10, trim_if_small = 10):\n",
    "    \"\"\"Sliding window method to locate and trim bottomside metadata\"\"\"\n",
    "    \n",
    "    def sliding_window(image, starting_y, starting_x, h_window, w_window, step_size):\n",
    "        \"\"\"da sliding window\"\"\"\n",
    "        h_img, w_img = np.shape(image)\n",
    "        for y in range(starting_y, h_img - h_window, step_size):\n",
    "            for x in range(starting_x, w_img - w_window, step_size):\n",
    "                yield y,x,image[y:y + h_window, x:x + w_window]\n",
    "    \n",
    "    h_raw,w_raw = opened_meta.shape\n",
    "    \n",
    "    if h_window + step_size  >= h_raw:\n",
    "        h_window = h_raw -trim_if_small\n",
    "    if w_window + step_size>= w_raw:\n",
    "        w_window = w_raw -trim_if_small\n",
    "    \n",
    "    s = sliding_window(connected_meta, starting_y, starting_x, h_window, w_window, step_size)\n",
    "    \n",
    "    max_window = connected_meta[starting_y:h_window+starting_y,\n",
    "                 starting_x:w_window+starting_x ]\n",
    "    max_mean = np.mean(max_window)\n",
    "    y_max= starting_y\n",
    "    x_max = starting_x\n",
    "    \n",
    "    for y, x, window in s:\n",
    "        tmp = window\n",
    "        mean = np.mean(tmp)\n",
    "        if mean > max_mean:\n",
    "            max_window = tmp\n",
    "            max_mean  = mean\n",
    "            y_max = y\n",
    "            x_max = x\n",
    "\n",
    "    trimmed_metadata =  opened_meta[y_max:y_max + h_window, x_max:x_max + w_window]\n",
    "\n",
    "    return trimmed_metadata\n",
    "\n",
    "#From scan2data > image_segmentation > trim_raw_metadata\n",
    "def trimming_metadata(raw_metadata,type_metadata, opening_kernal_size = (3,3), median_kernal_size = 5):\n",
    "    \"\"\"\"Trim the rectangle containing metadata to smallest workable area.\"\"\"\n",
    "\n",
    "    try:\n",
    "        #Filtering to reduce noise\n",
    "        median_filtered_meta = cv2.medianBlur(raw_metadata, median_kernal_size)\n",
    "        \n",
    "        #Opening operation: Eroision + Dilation\n",
    "        kernal_opening = np.ones(opening_kernal_size, dtype = np.uint8)\n",
    "        opened_meta = cv2.morphologyEx(median_filtered_meta, cv2.MORPH_OPEN, kernal_opening)\n",
    "        \n",
    "\n",
    "        # Binarization\n",
    "        _, metadata_binary = cv2.threshold(opened_meta, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        #Run connected components algorithm\n",
    "        connected_meta = metadata_location(metadata_binary)\n",
    "\n",
    "        trimmed_metadata = bottomside_metadata_trimming(connected_meta, metadata_binary)\n",
    "        #bottomside_metadata_trimming function is from same location\n",
    "\n",
    "        #Checking\n",
    "        #cv2.imshow(\"test\", trimmed_metadata)\n",
    "        #cv2.waitKey(0)\n",
    "        return (trimmed_metadata)\n",
    "    except:\n",
    "        return (\"oh no\")\n",
    "    \n",
    "\n",
    "##From scan2data > metadata_translation > leftside_metdata_grid_mapping\n",
    "def indices_highest_bin(list_coords, nbins = 500, peak_threshold = 0.2, distance_bwtn_peaks = 30 ):\n",
    "    \"\"\"\" returns indices of most common values using binning\n",
    "    list_coords: (np.arrray)\"\"\"\n",
    "\n",
    "    mean_coords = np.mean(list_coords)\n",
    "    std_coords = np.std(list_coords)\n",
    "    no_outlier_coords = list_coords[np.abs(list_coords - mean_coords) < 3 * std_coords]\n",
    "\n",
    "    #Binning\n",
    "    counts, bin_edges = np.histogram(no_outlier_coords, bins=nbins)\n",
    "\n",
    "    #Detect peaks\n",
    "    counts_norm = (counts - np.min(counts)) / (np.map(counts) - np.min(counts))\n",
    "    select_peaks = find_peaks(counts_norm, distance = distance_bwtn_peaks, promience = peak_threshold)    \n",
    "\n",
    "    return select_peaks, bin_edges, counts\n",
    "\n",
    "#From scan2data > metadata_translation> leftside_metadata_grid_mapping \n",
    "def extract_centroids(cut_metadata, file_name):\n",
    "    \"\"\"Takes in cut metadata and extracts centroids\n",
    "    \n",
    "    cut_metadata: np.array\"\"\"\n",
    "\n",
    "    try:\n",
    "        min_pixels = 50\n",
    "        max_pixels = 2000\n",
    "\n",
    "        _, __, stats, centroids = cv2.connectedComponentsWithStats(cut_metadata) \n",
    "        area_centroids = stats[:,-1]\n",
    "        #print (\"STATS\", stats)\n",
    "        #print (\"AREA\", area_centroids)\n",
    "\n",
    "        centroids_metadata = centroids[np.logical_and(area_centroids > min_pixels, area_centroids < max_pixels),:]\n",
    "        #^ consider adjusting min and max range\n",
    "\n",
    "        zip_centroids = list(zip(centroids_metadata))\n",
    "        col_centroids = list(zip_centroids[0])\n",
    "        row_centroids = list(zip_centroids[1])\n",
    "\n",
    "        #Round to nearest integer\n",
    "        col_centroids = (map(round, col_centroids))\n",
    "        print (\"COL/X CENTROIDS\", col_centroids)\n",
    "        row_centroids = (map(round, row_centroids))\n",
    "        print (\"ROW/Y CENTROIDS\", row_centroids)\n",
    "\n",
    "        return (col_centroids, row_centroids)\n",
    "    except: \n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "#From scan2data > image_segmentation > extract_ionogram_from_scan\n",
    "#For segment_metadata\n",
    "def limits_ionogram(raw_img, row_or_col, starting_index_col = 15):\n",
    "\n",
    "    mean_values = np.mean(raw_img, row_or_col)\n",
    "\n",
    "    #normalize mean\n",
    "    norm_mean = (mean_values - np.min(mean_values)) / np.max(mean_values)\n",
    "    thresh = np.mean(norm_mean)\n",
    "\n",
    "    if row_or_col == 0:\n",
    "        #Protect against scans that includes cuts from another ionogram\n",
    "        limits = [i for i, mean in enumerate(norm_mean) if mean > thresh and i > starting_index_col]\n",
    "    else:\n",
    "        limits = [i for i, mean in enumerate(norm_mean) if mean > thresh]\n",
    "\n",
    "    return limits[0], limits[-1]\n",
    "\n",
    "\n",
    "#From scan2data > image_segmentation > extract_ionogram_from_scan\n",
    "# For segment_metadata\n",
    "def extract_ionogram(raw_img_array):\n",
    "    \"\"\"\"this function is here for now to get limits of ionogram. \n",
    "    can later be changed to include ionogram graph\"\"\"\n",
    "    try:\n",
    "\n",
    "    #Extract coordinate delimiting the graph\n",
    "        x_left, x_right = limits_ionogram(raw_img_array, 0)\n",
    "        y_upper, y_lower = limits_ionogram(raw_img_array, 1)\n",
    "\n",
    "        limits = [x_left, x_right, y_upper, y_lower]\n",
    "        #ionogram = raw_img_array[y_upper:y_lower,x_left_:x_right]\n",
    "        return (limits)\n",
    "    except:\n",
    "        return (np.nan)\n",
    "\n",
    "\n",
    "#From scan2data > image_segmentation > extract_metadata_from_scan\n",
    "def extract_metadata (raw_img, limits_iono):\n",
    "    \"\"\"Extract metadata from raw scanned image and return coordinates delimiting its limits\"\"\"\n",
    "\n",
    "    #Limits for ionogram\n",
    "    x_left_lim = limits_iono[0][0]\n",
    "    x_right_lim = limits_iono[0][1]\n",
    "    y_upper_lim = limits_iono[0][2] \n",
    "    y_lower_lim = limits_iono[0][3]\n",
    "\n",
    "    #Extract retangular block below** ionogram\n",
    "    rect_left = raw_img[:,0:x_left_lim]\n",
    "    rect_right = raw_img[:,x_right_lim::]\n",
    "    rect_top = raw_img[0:y_upper_lim, :]\n",
    "    rect_bottom = raw_img[y_lower_lim::,:]\n",
    "\n",
    "    #Assumption: the location of the metadata will correspond to rectangle with the highest area\n",
    "    rect_list = [rect_left, rect_right, rect_top, rect_bottom]\n",
    "    rect_areas = [rect.shape[0] * rect.shape[1] for rect in rect_list]\n",
    "    dict_mapping_meta = {0:'left', 1:\"right\", 2:\"top\", 3:'bottom'}\n",
    "\n",
    "    type_metadata_idx = np.argmax(rect_areas)\n",
    "    raw_metadata = rect_list[type_metadata_idx]\n",
    "    type_metadata = dict_mapping_meta[type_metadata_idx]\n",
    "\n",
    "    \n",
    "    return (type_metadata, raw_metadata)\n",
    "\n",
    "#From scan2data > metadata_translation > translate_leftside_metadata\n",
    "def map_coord_to_metadata(list_cat_coord,list_num_coord,dict_mapping_cat, dict_mapping_num):\n",
    "    \"\"\"Map coordinate of metadata centroids to information\n",
    "    \n",
    "    :param list_cat_coord: list of metadata positions to map to categories   \n",
    "    :type list_cat_coord: list\n",
    "    :param list_num_coord: list of metadata positions to map to numbers \n",
    "    :type list_num_coord: list\n",
    "    :param dict_mapping_cat: dictionary used to map coordinate positions to categories\n",
    "    :type dict_mapping_cat: dict\n",
    "    :param dict_mapping_num: dictionary used to map coordinate positions to numbers\n",
    "    :type dict_mapping_num: dict\n",
    "    :returns: dict_metadata\n",
    "    :rtype: dict\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        list_coord = zip(list_cat_coord,list_num_coord)\n",
    "        coord_mapping_cat = dict_mapping_cat.keys()\n",
    "        coord_mapping_num = dict_mapping_num.keys()\n",
    "        \n",
    "        dict_metadata={}\n",
    "        for cat_coord, num_coord in list_coord:\n",
    "            cat_key = min(coord_mapping_cat, key=lambda x:abs(x-cat_coord))\n",
    "            num_key = min(coord_mapping_num, key=lambda x:abs(x-num_coord))\n",
    "            \n",
    "            cat = dict_mapping_cat[cat_key]\n",
    "            num = dict_mapping_num[num_key]\n",
    "            \n",
    "            # TODO: improve for many num\n",
    "            if cat in dict_metadata:\n",
    "                dict_metadata[cat].append(num)\n",
    "            else:\n",
    "                dict_metadata[cat] = [num]\n",
    "        \n",
    "        return dict_metadata\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "# From scan2data > metadata_translation > leftside_metadata_grid_mapping\n",
    "def get_leftside_metadata_grid_mapping(list_x_digit,list_y_digit,dir_name,\n",
    "                      difference_ratio=0.75,use_defaults=True):\n",
    "    \n",
    "    \"\"\"Determines and returns the the mapping between coordinate values on a metadata image \n",
    "    and metadata labels in a subdirectory, for metadata of types dot and digits, as well as returns \n",
    "    the histogram used to generate each mapping\n",
    "    \n",
    "    \"\"\"\n",
    "    # Dictionary of dictionaries that map labels to coordinate point in metadata\n",
    "    all_labels = [LABELS_CAT_DOT, LABELS_NUM, LABELS_CAT_DIGIT, LABELS_NUM]\n",
    "    all_dict_mapping = {}\n",
    "    all_dict_hist = {}\n",
    "    # Different protocols depending on the type of dictionary mappings\n",
    "    for i, list_coord in enumerate([list_x_digit,list_y_digit]):\n",
    "        type_dict = LABELS_DICT[i]\n",
    "        labels = all_labels[i]\n",
    "        try:\n",
    "            if 'cat' in type_dict:\n",
    "                if type_dict == 'dict_cat_digit':\n",
    "                    if any([dir_dot in dir_name for dir_dot in LIST_DIRECTORY_DOTS]):\n",
    "                        mean_dist_default,first_peak_default,last_peak_default=DEFAULT_DICT_CAT_DIGIT_F\n",
    "                    else:\n",
    "                        mean_dist_default,first_peak_default,last_peak_default = DEFAULT_DICT_CAT_DIGIT\n",
    "            \n",
    "                elif type_dict == 'dict_cat_dot':\n",
    "                    mean_dist_default,first_peak_default,last_peak_default=DEFAULT_DICT_CAT_DOT\n",
    "                try:\n",
    "                    idx_peaks,bin_edges,counts = indices_highest_bin(list_coord)\n",
    "                    peaks = bin_edges[np.array(idx_peaks)] #coordinate values on a metadata image probably corresponding to metadata\n",
    "                    \n",
    "                    n_labels = len(labels)\n",
    "                    first_peak = peaks[0]\n",
    "                    last_peak = peaks[-1]\n",
    "\n",
    "                    if use_defaults and abs(last_peak -last_peak_default)  > difference_ratio*mean_dist_default:\n",
    "                        last_peak = last_peak_default\n",
    "                    if use_defaults and abs(first_peak -first_peak_default)  > difference_ratio*mean_dist_default:\n",
    "                        first_peak = first_peak_default\n",
    "                        \n",
    "                    mean_dist_btw_peaks = (last_peak - first_peak)/(n_labels -1)\n",
    "                    list_peaks = [int(round(first_peak + i* mean_dist_btw_peaks)) for i in range(0,n_labels )]\n",
    "                    \n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] = (idx_peaks,bin_edges,counts)\n",
    "                \n",
    "\n",
    "                except:\n",
    "                    last_peak = last_peak_default\n",
    "                    first_peak = first_peak_default\n",
    "                    mean_dist_btw_peaks = mean_dist_default\n",
    "                    list_peaks = [int(round(first_peak + i* mean_dist_btw_peaks)) for i in range(0,n_labels )]\n",
    "                    \n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] = {}\n",
    "                \n",
    "            elif 'num' in type_dict:\n",
    "                if  type_dict == 'dict_num_digit':\n",
    "                    if any([dir_dot in dir_name for dir_dot in LIST_DIRECTORY_DOTS]):\n",
    "                        mean_dist_default,peak_0_default,dist_btw_peaks = DEFAULT_DICT_NUM_DIGIT_F\n",
    "                    else:\n",
    "                        mean_dist_default,peak_0_default,dist_btw_peaks = DEFAULT_DICT_NUM_DIGIT\n",
    "                elif type_dict == 'dict_num_dot':\n",
    "                    mean_dist_default,peak_0_default,dist_btw_peaks= DEFAULT_DICT_NUM_DOT\n",
    "\n",
    "                    \n",
    "                try:\n",
    "                    idx_peaks,bin_edges,counts = indices_highest_bin(list_coord,peak_prominence_threshold=0.3,nbins=100,distance_between_peaks=dist_btw_peaks)\n",
    "                \n",
    "                    peaks = bin_edges[np.array(idx_peaks)]                \n",
    "                    peak_0 = peaks[0]\n",
    "                    if use_defaults and abs(peak_0 -peak_0_default)  > difference_ratio*mean_dist_default:\n",
    "                        peak_0 = peak_0_default\n",
    "                \n",
    "                    # only first three peaks are deemed relevant\n",
    "                    if len(peaks) < 3:\n",
    "                        max_idx = 2\n",
    "                    else:\n",
    "                        max_idx = 3\n",
    "                \n",
    "                    mean_dist_btw_peaks = np.mean([peaks[i+1]-peaks[i] for i in range(0,max_idx)])\n",
    "                    if use_defaults and abs(mean_dist_btw_peaks - mean_dist_default)  > difference_ratio*dist_btw_peaks:\n",
    "                        mean_dist_btw_peaks = mean_dist_default\n",
    "                    list_peaks = [int(round(peak_0 + i* mean_dist_btw_peaks)) for i in range(0,len(labels))]\n",
    "                \n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] = (idx_peaks,bin_edges,counts)\n",
    "                except:\n",
    "                    peak_0 = peak_0_default\n",
    "                    mean_dist_btw_peaks = mean_dist_default\n",
    "                    list_peaks = [int(round(peak_0 + i* mean_dist_btw_peaks)) for i in range(0,len(labels))]\n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] =  {}\n",
    "        except:\n",
    "            all_dict_mapping[type_dict] ={}\n",
    "            all_dict_hist[type_dict] =  {}\n",
    "\n",
    "    print (\"All dict mapping\", all_dict_mapping)\n",
    "    print (\"All histograms\", all_dict_hist)\n",
    "    return all_dict_mapping, all_dict_hist\n",
    "\n",
    "#From scan2data> image_segmentation > segment_images_in_subdir.py\n",
    "#some variables here are not necessary and can be removed, ie. height, width...\n",
    "def segment_metadata(subdir_location, regex_img, min_bottom_height = 25, cutoff_width=300, cutoff_height=150):\n",
    "    \"\"\"Should only segment metadata. Can be adjusted to include ionogram. \"\"\"\n",
    "    regex_raw_image =  SD_PATH + (\"/*\")\n",
    "    #print (\"the raw images path is:\", regex_raw_image)\n",
    "    list_images = glob.glob(regex_raw_image)\n",
    "    \n",
    "    #Dataframe is processing\n",
    "    df_img = pd.DataFrame(data = {\"file_name\": list_images})\n",
    "    #Read each image in a 2D UTF-8 grayscale array\n",
    "    df_img[\"raw\"] = df_img['file_name'].map(lambda file_name: cv2.imread(file_name, 0))\n",
    "\n",
    "\n",
    "    # Extract ionogram and coordinates delimiting its limits\n",
    "    df_img['limits']= list(zip(df_img['raw'].map(lambda raw_img: extract_ionogram(raw_img)))) \n",
    "    \n",
    "    # Record the files whose ionogram extraction was not successful\n",
    "    df_loss_ion_extraction, loss_ion_extraction = record_loss(df_img,'image_segmentation.extract_ionogram_from_scan.extract_ionogram',subdir_location)\n",
    "    df_img = df_img[~loss_ion_extraction]\n",
    "    #df_img['height'],df_img['width'] = list(zip(df_img['ionogram'].map(lambda array_pixels: array_pixels.shape)))\n",
    "    \n",
    "    #Raw metadata\n",
    "    df_tmp = (df_img.apply(lambda row: extract_metadata(row['raw'], row['limits']), axis = 1, result_type = 'expand'))\n",
    "    df_img = df_img.assign(metadata_type = df_tmp[0])\n",
    "    df_img = df_img.assign(raw_metadata = df_tmp[1])\n",
    "    #extract_metadata is function from extract_metadata_from_scan\n",
    "    \n",
    "    # There should be no metadata on left and top, especially after flipping\n",
    "    outlier_metadata_location = np.any([df_img['metadata_type'] == 'right',df_img['metadata_type']=='top', df_img['metadata_type'] == 'left'], axis=0)\n",
    "    df_outlier_metadata_location ,_ =  record_loss(df_img,'image_segmentation.segment_images_in_subdir.segment_images: metadata not on left or bottom',subdir_location,\n",
    "                                         ['file_name','metadata_type'],outlier_metadata_location )\n",
    "    \n",
    "    if not df_outlier_metadata_location.empty:\n",
    "        df_outlier_metadata_location['details'] = df_outlier_metadata_location.apply(lambda row: str(row['metadata_type']),1)\n",
    "        df_outlier_metadata_location = df_outlier_metadata_location[['file_name','func_name','subdir_name','details']]\n",
    "    else:\n",
    "        df_outlier_metadata_location = df_outlier_metadata_location[['file_name','func_name','subdir_name']]\n",
    "    \n",
    "    # Remove loss from detected metadata not being on the left or bottom\n",
    "    df_img = df_img[~outlier_metadata_location]\n",
    "\n",
    "    #Trimmed metadata\n",
    "    #df_img['trimmed_metadata'] = list(zip(*df_img.apply(lambda row: trimming_metadata(row['raw_metadata'], row['metadata_type']), axis = 1, result_type = 'expand')))\n",
    "    df_trim_tmp = (df_img.apply(lambda row: trimming_metadata(row[\"raw_metadata\"], row['metadata_type']), axis = 1))\n",
    "    df_img = df_img.assign(trimmed_metadata = df_trim_tmp)\n",
    "    df_loss_trim, loss_trim = record_loss(df_img, 'image_segmentation.trim_raw_metadata.ntrimming_metadata', subdir_location)\n",
    "    #trimming_metadata is a function from trim_raw_metadata\n",
    "    #record_loss is a function from helper_functions\n",
    "    \n",
    "    df_img = df_img[~loss_trim]\n",
    "\n",
    "    # Check if metadata too small\n",
    "    #df_map_tmp = df_img['trimmed_metadata'].map(lambda array_pixels: np.shape(array_pixels), axis = 1)\n",
    "    df_map_tmp = df_img.apply(lambda row: np.shape(row['trimmed_metadata']), axis = 1, result_type = 'expand')\n",
    "    df_img = df_img.assign(meta_height = df_map_tmp[0])\n",
    "    df_img = df_img.assign(meta_width = df_map_tmp[1])\n",
    "\n",
    "    outlier_size_metadata = (np.logical_and(df_img['metadata_type'] == 'bottom', df_img['meta_height'] < min_bottom_height))    \n",
    "        \n",
    "    df_outlier_metadata_size, _ = record_loss(df_img,'image_segmentation.segment_images_in_subdir.segment_images: metadata size outlier',subdir_location,\n",
    "                                           ['file_name','metadata_type','meta_height','meta_width'],outlier_size_metadata)\n",
    "\n",
    "    if not df_outlier_metadata_size.empty:\n",
    "        df_outlier_metadata_size['details'] = df_outlier_metadata_size.apply(lambda row: row['metadata_type'] + '_height: ' + \\\n",
    "                                                    str(row['meta_height'])+',width: ' + str(row['meta_width']),1)\n",
    "        df_outlier_metadata_size = df_outlier_metadata_size[['file_name','func_name','subdir_name','details']]\n",
    "        \n",
    "    else:\n",
    "        df_outlier_metadata_size = df_outlier_metadata_size[['file_name','func_name','subdir_name']]\n",
    "    \n",
    "    # Remove files whose metadata too small\n",
    "    df_img = df_img[~outlier_size_metadata]\n",
    "    \n",
    "    # Dataframe recording loss from programming errors\n",
    "    df_loss = pd.concat([df_loss_ion_extraction, df_loss_trim])\n",
    "    \n",
    "    # Dataframe recording loss from various filters i.e. metadata too small, ionogram too small/big\n",
    "    df_outlier = pd.concat([df_outlier_metadata_location, df_outlier_metadata_size])\n",
    "\n",
    "    return df_img,  df_loss, df_outlier\n",
    "# function can also return df_loss, df_outlier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dict mapping {'dict_cat_dot': {}, 'dict_num_dot': {}}\n",
      "All histograms {'dict_cat_dot': {}, 'dict_num_dot': {}}\n",
      "DICT MAPPING {'dict_cat_dot': {}, 'dict_num_dot': {}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'dict_cat_digit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\spunchiwickrama\\Documents\\Projects\\ISIS_I\\Alouette_extract\\ISIS Metadata Gridding.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     df_all_loss \u001b[39m=\u001b[39m df_all_loss\u001b[39m.\u001b[39mappend(df_loss_coord)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m df_processed, df_all_loss, df_outlier\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mprint\u001b[39m (process_subdir(SD_PATH, \u001b[39m'\u001b[39;49m\u001b[39m/*\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mG:/spunchiwickrama/ISIS_I/output\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "\u001b[1;32mc:\\Users\\spunchiwickrama\\Documents\\Projects\\ISIS_I\\Alouette_extract\\ISIS Metadata Gridding.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m#Translate metadata on bottom\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m df_img_bottom \u001b[39m=\u001b[39m df_img[df_img[\u001b[39m'\u001b[39m\u001b[39mmetadata_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbottom\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m df_img, df_loss_meta, dict_mapping, dict_hist \u001b[39m=\u001b[39m get_bottomside_metadata(df_img, subdir_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m#get_bottomside_metadata is another function\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m df_all_loss \u001b[39m=\u001b[39m df_loss\n",
      "\u001b[1;32mc:\\Users\\spunchiwickrama\\Documents\\Projects\\ISIS_I\\Alouette_extract\\ISIS Metadata Gridding.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mDICT MAPPING\u001b[39m\u001b[39m\"\u001b[39m, dict_mapping)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Determine the value of metadata based on the mappings\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m df_dict_meta \u001b[39m=\u001b[39m df_img\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m row: map_coord_to_metadata(row[\u001b[39m'\u001b[39;49m\u001b[39mx_centroids\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39;49m\u001b[39my_centroids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m                                                                   dict_mapping[\u001b[39m'\u001b[39;49m\u001b[39mdict_cat_digit\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                                                                   dict_mapping[\u001b[39m'\u001b[39;49m\u001b[39mdict_num_digit\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m df_img \u001b[39m=\u001b[39m df_img\u001b[39m.\u001b[39massign(dict_metadata \u001b[39m=\u001b[39m df_dict_meta)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m##FOR TESTING:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m#df_img.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/output.csv\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\spunchiwickrama\\.conda\\envs\\pythonv39\\lib\\site-packages\\pandas\\core\\frame.py:10037\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m  10025\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10027\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m  10028\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m  10029\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10035\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m  10036\u001b[0m )\n\u001b[1;32m> 10037\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\spunchiwickrama\\.conda\\envs\\pythonv39\\lib\\site-packages\\pandas\\core\\apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    835\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 837\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\spunchiwickrama\\.conda\\envs\\pythonv39\\lib\\site-packages\\pandas\\core\\apply.py:963\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 963\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    965\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    966\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\spunchiwickrama\\.conda\\envs\\pythonv39\\lib\\site-packages\\pandas\\core\\apply.py:979\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    977\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    978\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 979\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(v, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n\u001b[0;32m    980\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    981\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    982\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    983\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\spunchiwickrama\\Documents\\Projects\\ISIS_I\\Alouette_extract\\ISIS Metadata Gridding.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mDICT MAPPING\u001b[39m\u001b[39m\"\u001b[39m, dict_mapping)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Determine the value of metadata based on the mappings\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m df_dict_meta \u001b[39m=\u001b[39m df_img\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: map_coord_to_metadata(row[\u001b[39m'\u001b[39m\u001b[39mx_centroids\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39my_centroids\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m                                                                   dict_mapping[\u001b[39m'\u001b[39;49m\u001b[39mdict_cat_digit\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                                                                   dict_mapping[\u001b[39m'\u001b[39m\u001b[39mdict_num_digit\u001b[39m\u001b[39m'\u001b[39m]), axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m df_img \u001b[39m=\u001b[39m df_img\u001b[39m.\u001b[39massign(dict_metadata \u001b[39m=\u001b[39m df_dict_meta)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m##FOR TESTING:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spunchiwickrama/Documents/Projects/ISIS_I/Alouette_extract/ISIS%20Metadata%20Gridding.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m#df_img.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/output.csv\")\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'dict_cat_digit'"
     ]
    }
   ],
   "source": [
    "#From scan2data > metadata_translation > translate_leftside_metadata.py\n",
    "def get_bottomside_metadata (df_img, subdir_location, kernal_size =(1, 1)):\n",
    "    \"\"\"Reads the metadata (finally)\"\"\"\n",
    "\n",
    "    kernel_dilation = np.ones(kernal_size, np.uint8)\n",
    "\n",
    "    #df_dilate_tmp = df_img.apply(lambda trimmed_meta: cv2.dilate(trimmed_meta, kernel_dilation))\n",
    "    df_dilate_tmp = df_img['trimmed_metadata'].map(lambda trimmed_meta: cv2.dilate(trimmed_meta, kernel_dilation))\n",
    "    df_img = df_img.assign(dilated_metadata = df_dilate_tmp)\n",
    "\n",
    "    #df_img['x_centroids'], df_img['y_centroids'], df_img['is_dot'] = zip(*df_img.apply(lambda row: extract_centroids(row['dilated_metadata'], row['file_name']), 1))\n",
    "    df_cent_tmp = df_img.apply(lambda row: extract_centroids(row['dilated_metadata'], row['file_name']), axis = 1, result_type = 'expand')\n",
    "    print (\"DF CENT TEMP\")\n",
    "    print (df_cent_tmp)\n",
    "    df_img = df_img.assign(x_centroids = df_cent_tmp[0])\n",
    "    df_img = df_img.assign(y_centroids = df_cent_tmp[1])\n",
    "\n",
    "    df_loss_centroids_extraction, loss_centroids_extraction = record_loss(df_img,'metadata_translation.determine_metadata_grid_mapping.extract_centroids_',subdir_location)\n",
    "    # extract_centroids and record_loss are two other functions \n",
    "\n",
    "    # Remove files where the extraction didn't work\n",
    "    df_img = df_img[~loss_centroids_extraction]\n",
    "    # ^removes them from the main dataframe\n",
    "\n",
    "    #df_num_subset = df_img[np.invert(np.array(df_img['is_dot']))]\n",
    "\n",
    "    list_x_digit = list(df_img['x_centroids'].tolist())\n",
    "    list_y_digit = list(df_img['y_centroids'].tolist())\n",
    "    dict_mapping, dict_hist = get_leftside_metadata_grid_mapping(list_x_digit, list_y_digit, subdir_location)\n",
    "    print (\"DICT MAPPING\", dict_mapping)\n",
    "\n",
    "    # Determine the value of metadata based on the mappings\n",
    "    df_dict_meta = df_img.apply(lambda row: map_coord_to_metadata(row['x_centroids'], row['y_centroids'],\n",
    "                                                                      dict_mapping['dict_cat_digit'],\n",
    "                                                                      dict_mapping['dict_num_digit']), axis = 1)\n",
    "    df_img = df_img.assign(dict_metadata = df_dict_meta)\n",
    "\n",
    "    ##FOR TESTING:\n",
    "    #df_img.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/output.csv\")\n",
    "    \n",
    "    df_loss_mapping, loss_mapping = record_loss(df_img,\n",
    "                                                'metadata_translation.translate_leftside_metadata.map_coord_to_metadata',\n",
    "                                                subdir_location)\n",
    "    df_img = df_img[~loss_mapping]\n",
    "\n",
    "    df_loss = pd.concat([df_loss_centroids_extraction, df_loss_mapping])\n",
    "\n",
    "    return df_img, df_loss, dict_mapping, dict_hist\n",
    "\n",
    "#From process_directory.py\n",
    "def process_subdir(subdir_path, regex_images, output_folder_if_pickle,\n",
    "                   to_pickle=True):\n",
    "    \"\"\"Transform raw scanned images in subdir into information\"\"\"\n",
    "\n",
    "    #Run segment_images on subdir\n",
    "    df_img, df_loss, df_outlier = segment_metadata(subdir_path, regex_images)\n",
    "\n",
    "    #Translate metadata on bottom\n",
    "    df_img_bottom = df_img[df_img['metadata_type'] == 'bottom']\n",
    "\n",
    "    df_img, df_loss_meta, dict_mapping, dict_hist = get_bottomside_metadata(df_img, subdir_path)\n",
    "    #get_bottomside_metadata is another function\n",
    "    df_all_loss = df_loss\n",
    "\n",
    "    #pickle \n",
    "    if to_pickle:\n",
    "        start, subdir_name = ntpath.split(subdir_path[:-1])\n",
    "        start, dir_name = ntpath.split(start)\n",
    "        df_processed.to_pickle(os.pardir + '/pickle/' + str(dir_name) + '_' + str(subdir_name) + '.pkl')\n",
    "\n",
    "    df_all_loss = df_all_loss.append(df_loss_coord)\n",
    "\n",
    "    return df_processed, df_all_loss, df_outlier\n",
    "\n",
    "print (process_subdir(SD_PATH, '/*', \"G:/spunchiwickrama/ISIS_I/output\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonV31013",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
