{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISIS Gridding for Metadata (in-progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a random image\n",
    "Generates and displays a random image from subdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_ran_subdir (subdir_path):\n",
    "    \"\"\"\" Generates a random subdirectory\n",
    "    Requires: \n",
    "    subdir_path: name of the path for the subdirectory\n",
    "     L:/DATA/ISIS/ISIS_101300030772/b*/B1* \"\"\"\n",
    "\n",
    "    all_subs = glob.glob(subdir_path) #creates a list of all subdirects \n",
    "    selected_sub = all_subs[random.randint(0, len(all_subs)-1)] #picks a random one from list\n",
    "    return (selected_sub)\n",
    "\n",
    "def gen_ran_img (subdir_path, img):\n",
    "    \"\"\"\" Generates a random image \"\"\"\n",
    "\n",
    "    all_img = glob.glob(subdir_path + img) #creates list of all images\n",
    "\n",
    "    selected_img = all_img[random.randint(0, len(all_img) - 1)]\n",
    "    return (selected_img)\n",
    "\n",
    "img_path = ((gen_ran_img(gen_ran_subdir(\"L:/DATA/ISIS/ISIS_101300030772/b*/B1*\"),\"/*\")))\n",
    "\n",
    "# Display\n",
    "print (img_path)\n",
    "img = mpimg.imread(img_path)\n",
    "cv2.imshow(\"image\", img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Crop image\n",
    "# Need to crop bottom ~20%\n",
    "\n",
    "height, width = img.shape[0:2]\n",
    "\n",
    "y, x = img.shape[0], img.shape[1]\n",
    "h = int(y*0.85)\n",
    "\n",
    "crop_img = img[h:y, 0:x]\n",
    "cv2.imshow(\"cropped image\", crop_img)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From helper_functions.py\n",
    "def record_loss (df, function_name, subdir_location, columns_to_extract=['file_name'], loss_extraction=[]):\n",
    "    \"\"\"Create dataframe that records loss.\"\"\"\n",
    "\n",
    "    if len(loss_extraction) == 0:\n",
    "        #should return NA if there's an error\n",
    "        loss_extraction = df.isna().any(axis = 1)\n",
    "\n",
    "    df_loss_extraction = df[loss_extraction].copy()\n",
    "    df_loss_extraction = df_loss_extraction[columns_to_extract]\n",
    "    df_loss_extraction[\"func_name\"] = function_name\n",
    "    df_loss_extraction[\"subdir_name\"] = subdir_location\n",
    "\n",
    "    return df_loss_extraction, loss_extraction\n",
    "\n",
    "\n",
    "# location?\n",
    "def metadata_location(meta, min_count = 50, max_count = 1000):\n",
    "    \"\"\"\"Use connected component algorithm to find the location of the metadata\"\"\"\n",
    "    \n",
    "    #run algorithm on metadata section\n",
    "    _, labelled = cv2.connectedComponents(meta)\n",
    "\n",
    "    #Dictionary of label:counts\n",
    "    unique, counts = np.unique(labelled, return_counts = True)\n",
    "    dict_components = dict(zip(unique, counts))\n",
    "\n",
    "    #Remove outliers // Remove pixels not part of metadata\n",
    "    dict_subset = {}\n",
    "    dict_outlier = {}\n",
    "    for k,v, in dict_components.items():\n",
    "        if v > min_count and v < max_count:\n",
    "            dict_subset[k] = v\n",
    "        else:\n",
    "            dict_outlier[k] = v\n",
    "    \n",
    "    key_list_to_remove = list(dict_outlier.keys())\n",
    "    if len(key_list_to_remove) != 0:\n",
    "        for k in key_list_to_remove:\n",
    "            labelled[labelled == k] = 0\n",
    "    \n",
    "    return labelled\n",
    "\n",
    "#From scan2data > image_segmentation > trim_raw_metadata\n",
    "#test provided values and change if needed\n",
    "def bottomside_metadata_trimming(connected_meta, opened_meta,\n",
    "                                 h_window = 100, w_window = 700, starting_y = 0, starting_x = 15, step_size = 10, trim_if_small = 10):\n",
    "    \"\"\"Sliding window method to locate and trim bottomside metadata\"\"\"\n",
    "    \n",
    "    def sliding_window(image, starting_y, starting_x, h_window, w_window, step_size):\n",
    "        \"\"\"da sliding window\"\"\"\n",
    "        h_img, w_img = image.shape\n",
    "        for y in range(starting_y, h_img - h_window, step_size):\n",
    "            for x in range(starting_x, w_ing - w_window, step_size):\n",
    "                yield y,x,image[y:y + h_window, x:x + w_window]\n",
    "    \n",
    "    h_raw,w_raw = opened_meta.shape\n",
    "    \n",
    "    if h_window + step_size  >= h_raw:\n",
    "        h_window = h_raw -trim_if_small\n",
    "    if w_window + step_size>= w_raw:\n",
    "        w_window = w_raw -trim_if_small\n",
    "    \n",
    "    s = sliding_window(connected_meta,starting_y,starting_x,h_window,w_window,step_size)\n",
    "\n",
    "    max_window = connected_meta[starting_y:h_window+starting_y,\n",
    "                 starting_x:w_window+starting_x ]\n",
    "    max_mean = np.mean(max_window)\n",
    "    y_max= starting_y\n",
    "    x_max = starting_x\n",
    "    for y,x,window in s:\n",
    "        tmp = window\n",
    "        mean = np.mean(tmp)\n",
    "        if mean > max_mean:\n",
    "            max_window = tmp\n",
    "            max_mean  = mean\n",
    "            y_max= y\n",
    "            x_max =x\n",
    "\n",
    "    trimmed_metadata =  opened_meta[y_max:y_max+h_window,x_max:x_max+w_window]\n",
    "\n",
    "    return trimmed_metadata\n",
    "\n",
    "#From scan2data > image_segmentation > trim_raw_metadata\n",
    "def trimming_metadata(raw_metadata, opening_kernal_size = (3,3), median_kernal_size = 5):\n",
    "    \"\"\"\"Trim the rectangle containing metadata to smallest workable area.\"\"\"\n",
    "\n",
    "    try:\n",
    "        #Filtering to reduce noise\n",
    "        median_filtered_meta = cv2.medianBlur(raw_metadata, median_kernal_size)\n",
    "        \n",
    "        #Opening operation: Eroision + Dilation\n",
    "        kernal_opening = np.ones(opening_kernal_size, dtype = np.uint8)\n",
    "        opened_meta = cv2.morphologyEx(binr, cv2.MORPH_OPEN, kernel_opening, iterations = 1)\n",
    "\n",
    "        # Binarization\n",
    "        _, metadata_binary = cv2.threshold(opened_meta, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        #Run connected components algorithm\n",
    "        connected_meta = connected_components_metadata_location(metadata_binary)\n",
    "\n",
    "        trimmed_metadata = bottomside_metadata_trimming(connected_meta, metadata_binary)\n",
    "        #function from somewhere else\n",
    "\n",
    "        #Checking\n",
    "        cv2.imshow(\"test\", trimmed_metadata)\n",
    "        cv2.waitKey(0)\n",
    "        return (trimmed_metadata)\n",
    "    except:\n",
    "        return (np.nan)\n",
    "    \n",
    "\n",
    "#location?\n",
    "#function is possible not needed -- can potentially remove\n",
    "def indices_highest_bin(list_coords):\n",
    "    \"\"\"\" returns indices of most common values using binning\n",
    "    list_coords: (np.arrray)\"\"\"\n",
    "\n",
    "    nbins = 50\n",
    "    peak_threshold = 0.2\n",
    "    distance_bwtn_peaks = 30\n",
    "\n",
    "    mean_coords = np.mean(list_coords)\n",
    "    std_coords = np.std(list_coords)\n",
    "    no_outlier_coords = list_coords[np.abs(list_coords - mean_coords) < 3 * std_coords]\n",
    "\n",
    "    #Binning\n",
    "    counts, bin_edges = np.histogram(no_outlier_coords, bins=nbins)\n",
    "\n",
    "    #Detect peaks\n",
    "    counts_norm = (counts - np.min(counts)) / (np.map(counts) - np.min(counts))\n",
    "    select_peaks = find_peaks(counts_norm, distance = distance_bwtn_peaks, promience = peak_threshold)    \n",
    "\n",
    "    return select_peaks, bin_edges, counts\n",
    "\n",
    "#location? \n",
    "def extract_centroids(cut_metadata):\n",
    "    \"\"\"Takes in cut metadata and extracts centroids\n",
    "    \n",
    "    cut_metadata: np.array\"\"\"\n",
    "\n",
    "    min_pixels = 50\n",
    "    max_pixels = 1000\n",
    "\n",
    "    _, __, stats, centroids = cv2.connectedComponentsWithStats(cut_metadata, 8) \n",
    "    area_centroids = stats[:,-1]\n",
    "\n",
    "    centroids_metadata = centroids[np.logical_and(area_centroids > min_pixels, area_centroids < max_pixels)]\n",
    "    col_centroids, row_centroids = zip(*centroids_metadata)\n",
    "\n",
    "    #Round to nearest integer\n",
    "    col_centroids = list(map(round, col_centroids))\n",
    "    row_centroids = list(map(round, row_centroids))\n",
    "\n",
    "    return col_centroids, row_centroids\n",
    "\n",
    "#location?\n",
    "#From scan2data > image_segmentation > extract_metadata_from_scan\n",
    "def extract_metadata (raw_img, limits_iono):\n",
    "    \"\"\"Extract metadata from raw scanned image and return coordinates delimiting its limits\"\"\"\n",
    "    \n",
    "    #Limits for ionogram\n",
    "    x_left_lim, x_right_lim, y_upper_lim, y_lower_lim = limits_iono\n",
    "\n",
    "    #Extract retangular block below** ionogram\n",
    "    rect_left = raw_img[:,0:x_left_lim]\n",
    "    rect_right = raw_img[:,x_right_lim::]\n",
    "    rect_top = raw_img[0:y_upper_lim, :]\n",
    "    rect_bottom = raw_img[y_lower_lim::,:]\n",
    "\n",
    "    #Assumption: the location of the metadata will correspond to rectangle with the highest area\n",
    "    rect_list = [rect_left, rect_right, rect_top, rect_bottom]\n",
    "    rect_areas = [rect.shape[0] * rect_shape[1] for rect in rect_list]\n",
    "    dict_mapping_meta = {0:'left', 1:\"right\", 2:\"top\", 3:'bottom'}\n",
    "\n",
    "    type_metadata_idx = np.argmax(rect_areas)\n",
    "    raw_metadata = rect_list[type_metadata_idx]\n",
    "    type_metadata = dict_mapping_meta[type_metadata_idx]\n",
    "\n",
    "    return (type_metadata, raw_metadata)\n",
    "\n",
    "\n",
    "#From scan2data> image_segmentation > segment_images_in_subdir.py\n",
    "#some variables here are not necessary and can be removed, ie. height, width...\n",
    "def segment_metadata(subdir_location, regex_img, min_bottom_height = 25, cutoff_width=300, cutoff_height=150):\n",
    "    \"\"\"Should only segment metadata. Can be adjusted to include ionogram. \"\"\"\n",
    "    regex_raw_image = subdir_location + regex_img\n",
    "    list_images = glob.glob(regex_raw_image)\n",
    "    \n",
    "    #Dataframe is processing\n",
    "    df_img = pd.DataFrame(data = {\"file_name\": list_images})\n",
    "    \n",
    "    #Read each image in a 2D UTF-8 grayscale array\n",
    "    df_img[\"raw\"] = df_img['file_name'].map(lambda file_name: cv2.imread(file_name, 0))\n",
    "\n",
    "    # Extract ionogram and coordinates delimiting its limits\n",
    "    #df_img['limits'],df_img['ionogram'] = zip(*df_img['raw'].map(lambda raw_img: extract_ionogram(raw_img)))\n",
    "\n",
    "    # Record the files whose ionogram extraction was not successful\n",
    "    #df_loss_ion_extraction, loss_ion_extraction = record_loss(df_img,'image_segmentation.extract_ionogram_from_scan.extract_ionogram',subdir_location)\n",
    "    \n",
    "    #Raw metadata\n",
    "    df_img['metadata_type'], df_img['raw_metadata'] = zip(*df_img.apply(lambda row: extract_metadata(row['raw'], row['limits']), 1))\n",
    "    #extract_metadata is function from extract_metadata_from_scan\n",
    "\n",
    "    # There should be no metadata on left and top, especially after flipping\n",
    "    outlier_metadata_location = np.any([df_img['metadata_type'] == 'right',df_img['metadata_type']=='top'],axis=0)\n",
    "    df_outlier_metadata_location ,_ =  record_loss(df_img,'image_segmentation.segment_images_in_subdir.segment_images: metadata not on left or bottom',subdir_location,\n",
    "                                         ['file_name','metadata_type'],outlier_metadata_location )\n",
    "    \n",
    "    if not df_outlier_metadata_location.empty:\n",
    "        df_outlier_metadata_location['details'] = df_outlier_metadata_location.apply(lambda row: str(row['metadata_type']),1)\n",
    "        df_outlier_metadata_location = df_outlier_metadata_location[['file_name','func_name','subdir_name','details']]\n",
    "    else:\n",
    "        df_outlier_metadata_location = df_outlier_metadata_location[['file_name','func_name','subdir_name']]\n",
    "    \n",
    "    # Remove loss from detected metadata not being on the left or bottom\n",
    "    df_img = df_img[~outlier_metadata_location]\n",
    "\n",
    "    #Trimmed metadata\n",
    "    df_img['trimmed_metadata'] = df_img.apply(lambda row: trimming_metadata(row['raw_metadata'], row['metadata_type']), 1)\n",
    "    df_loss_trim, lost_trim = record_loss(df.img, 'image_segmentation.trim_raw_metadata.ntrimming_metadata', subdir_location)\n",
    "    #trimming_metadata is a function from trim_raw_metadata\n",
    "    #record_loss is a function from helper_functions\n",
    "\n",
    "    df_img = df_img[~loss_trim]\n",
    "\n",
    "    # Check if metadata too small\n",
    "    df_img['meta_height'],df_img['meta_width'] = zip(*df_img['trimmed_metadata'].map(lambda array_pixels: array_pixels.shape))\n",
    "    outlier_size_metadata = np.logical_or(np.logical_and(df_img['metadata_type'] == 'left', \n",
    "                                                      df_img['meta_width'] < min_leftside_meta_width),\n",
    "                                       np.logical_and(df_img['metadata_type'] == 'bottom', \n",
    "                                                      df_img['meta_height'] < min_bottomside_meta_height))\n",
    "        \n",
    "    df_outlier_metadata_size, _ = record_loss(df_img,'image_segmentation.segment_images_in_subdir.segment_images: metadata size outlier',subdir_location,\n",
    "                                           ['file_name','metadata_type','meta_height','meta_width'],outlier_size_metadata)\n",
    "\n",
    "    if not df_outlier_metadata_size.empty:\n",
    "        df_outlier_metadata_size['details'] = df_outlier_metadata_size.apply(lambda row: row['metadata_type'] + '_height: ' + \\\n",
    "                                                    str(row['meta_height'])+',width: ' + str(row['meta_width']),1)\n",
    "        df_outlier_metadata_size = df_outlier_metadata_size[['file_name','func_name','subdir_name','details']]\n",
    "        \n",
    "    else:\n",
    "        df_outlier_metadata_size = df_outlier_metadata_size[['file_name','func_name','subdir_name']]\n",
    "    \n",
    "    # Remove files whose metadata too small\n",
    "    df_img = df_img[~outlier_size_metadata]\n",
    "    \n",
    "    \n",
    "    # Dataframe recording loss from programming errors\n",
    "    df_loss = pd.concat([df_loss_ion_extraction,df_loss_trim])\n",
    "    \n",
    "    # Dataframe recording loss from various filters i.e. metadata too small, ionogram too small/big\n",
    "    df_outlier = pd.concat([df_outlier_ionogram,df_outlier_metadata_location,df_outlier_metadata_size])\n",
    "\n",
    "    return df_img,  df_loss, df_outlier\n",
    "# function can also return df_loss, df_outlier\n",
    "\n",
    "#From scan2data > metadata_translation > translate_leftside_metadata.py\n",
    "def get_bottomside_metadata (df_img, subdir_location, kernal_size =(1, 1)):\n",
    "    \"\"\"Reads the metadata (finally)\"\"\"\n",
    "\n",
    "    kernel_dilation = np.ones(kernal_size, np.uint8)\n",
    "\n",
    "    df_img['dilated_metadata'] = df_img['trimmed_metadata'].map(\n",
    "        lambda trimmed_meta: cv2.dilate(trimmed_meta, kernel_dilation))\n",
    "    df_img['x_centroids'], df_img['y_centroids'], df_img['is_dot'] = zip(\n",
    "        *df_img.apply(lambda row: extract_centroids(row['dilated_metadata'], row['file_name']), 1))\n",
    "    df_loss_centroids_extraction, loss_centroids_extraction = record_loss(df_img,\n",
    "                                                                          'metadata_translation.determine_metadata_grid_mapping.extract_centroids_',\n",
    "                                                                          subdir_location)\n",
    "    # extract_centroids and record_loss are two other functions \n",
    "\n",
    "    # Remove files where the extraction didn't work\n",
    "    df_img = df_img[~loss_centroids_extraction]\n",
    "    # ^removes them from the main dataframe\n",
    "    df_num_subset = df_img[np.invert(np.array(df_img['is_data']))]\n",
    "\n",
    "    list_x_digit = list(chain(*df_num_subset['x_centroids'].tolist()))\n",
    "    list_y_digit = list(chain(*df_num_subset['y_centroids'].tolist()))\n",
    "    dict_mapping, dict_hist = get_leftside_metadata_grid_mapping(list_x_dot, list_y_dot, list_x_digit, list_y_digit,\n",
    "                                                                 subdir_location)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SOME GOOD CODE FOR ONCE \n",
    "\n",
    "#From process_directory.py\n",
    "def process_subdir(subdir_path, regex_images, output_folder_if_pickle,\n",
    "                   to_pickle=True):\n",
    "    \"\"\"Transform raw scanned images in subdir into information\"\"\"\n",
    "\n",
    "    #Run segment_images on subdir\n",
    "    df_img, df_loss, df_outlier = segment_images(subdir_path, regex_images)\n",
    "\n",
    "    #Translate metadata on bottom\n",
    "    df_img_bottom = df_img[df_img['metadata_type'] == 'bottom']\n",
    "\n",
    "    df_img, df_loss_meta, dict_mapping, dict_hist = get_bottonside_metadata(df_img, subdir_path)\n",
    "    #get_bottomside_metadata is another function\n",
    "    df_all_loss = df_loss\n",
    "\n",
    "    #pickle \n",
    "    if to_pickle:\n",
    "        start, subdir_name = ntpath.split(subdir_path[:-1])\n",
    "        start, dir_name = ntpath.split(start)\n",
    "        df_processed.to_pickle(os.pardir + '/pickle/' + str(dir_name) + '_' + str(subdir_name) + '.pkl')\n",
    "\n",
    "    df_all_loss = df_all_loss.append(df_loss_coord)\n",
    "\n",
    "    return df_processed, df_all_loss, df_outlier\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
