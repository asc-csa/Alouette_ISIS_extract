{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51d8647",
   "metadata": {},
   "source": [
    "# Determine BATCH II Scan Errors\n",
    "\n",
    "#### Updated: June 3, 2023 by Ashley Ferreira"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d2457",
   "metadata": {},
   "source": [
    "Note that all of my code so far uses the old way of naming where box = roll, which we now know is false, the code has just yet to be updated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdbf052",
   "metadata": {},
   "source": [
    "#### Setup \n",
    "You will likely need to pip install line_profiler, tensorflow, and keras_ocr as they do not come by default with anaconda, uncomment the cells below to do this if needed. Then run the cells to import the libraries adn set some of the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7070530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to download non-standard libraries\n",
    "#! pip install tensorflow --user\n",
    "#! pip install keras_ocr --user\n",
    "#! pip install line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd394ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your network username to have correct paths\n",
    "username = 'aferreira'\n",
    "\n",
    "# enter True to use GPU, False for CPU\n",
    "gpu_use = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777138ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from threading import Thread\n",
    "#from PIL import Image\n",
    "#from numba import jit, cuda, njit\n",
    "\n",
    "# replace this with your own library path for --user pip installs\n",
    "sys.path.append('C:/Users/' + username + '/AppData/Roaming/Python/Python38/Scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu_use:\n",
    "    sys.path.insert(0, 'u:/temp/' + username + '/python/envs/tf210/lib/site-packages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17754f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_ocr\n",
    "print(tf.__version__) # for gpu use should be version 2.10.*\n",
    "print(tf.config.list_physical_devices('GPU')) # for GPU use this should show something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd26f0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for C:\\Users\\aferreira\\.keras-ocr\\craft_mlt_25k.h5\n",
      "Looking for C:\\Users\\aferreira\\.keras-ocr\\crnn_kurapan.h5\n",
      "number of CPU cores available: 6\n"
     ]
    }
   ],
   "source": [
    "pipeline = keras_ocr.pipeline.Pipeline()\n",
    "%load_ext line_profiler\n",
    "print('number of CPU cores available:', os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b83481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "batchDir = 'L:/DATA/Alouette_I/BATCH_II_raw/'\n",
    "save_dir = 'U:/Downloads/test_runs/' \n",
    "outFile = save_dir + 'notebook20_outputs_v16.csv'\n",
    "\n",
    "# make the directory to save into  \n",
    "# if it doesn't already exist\n",
    "if not(os.path.exists(save_dir)):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# set default saving settings\n",
    "# (not sure if code works anymore if these are changed)\n",
    "append2outFile = True \n",
    "saveImages = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c8933c",
   "metadata": {},
   "source": [
    "#### Inititalize functions\n",
    "The main processing for this code uses two functions, read_all_rolls() which loops over all the batch 2 raw data ionogram images and saves the outputs from read_image() to a CSV file. This second function, read_image() reads in one image a time, whos path is passed it it by read_all_rolls(), and outputs the height and width along with the estimated digit count of the metadata.\n",
    "\n",
    "Playing around with downsizing the image and found that 2-4 factors of downsizing in likely sweet spot where time taken for inference is significantly reduced but also image is somewhat legible still. Could alternatively decide on set length of one axis and then just scale the other accordingly or have different downscales for large and small images. Image library can likely allow for better interpolation but is not easily compatible with keras_ocr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e47542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path, plotting=False, just_digits=False, down_factor=2):\n",
    "    '''\n",
    "    This function reads in one image a time and outputs the height \n",
    "    and width along with the estimated digit count of the metadata.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "        image_path (str): path to the image\n",
    "\n",
    "        plotting (bool, optional): True for a verbose display mode to\n",
    "                                   illustrate the analysis in detail, \n",
    "                                   False otherwise\n",
    "\n",
    "        just_digits (bool, optional): if True only count characters that are \n",
    "                                      integers, False to count any characters\n",
    "\n",
    "        down_factor (int, optional): factor by which to integer divide height\n",
    "                                     and width to scale down size of image\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        digit_count (int): estimated number of integers in the ionogram\n",
    "                          metadata (right now, only looks for numbers along\n",
    "                          bottom 20% of the image, usually only 15 expected)\n",
    "                          ~~~~ done for cropped image ~~~~\n",
    "    \n",
    "        height (int): number of pixels along y-axis of original image\n",
    "        \n",
    "        width (int): number of pixels along x-axis of origional image\n",
    "\n",
    "        says_isis (bool): True if 'isis' independant of capitalization is \n",
    "                          present within the detected text, False otherwise\n",
    "                          ~~~~ done for origional image ~~~~\n",
    "    '''\n",
    "    try: \n",
    "\n",
    "        # read in image using keras_ocr\n",
    "        image = keras_ocr.tools.read(image_path) \n",
    "\n",
    "        # extract height and width of image in pixels \n",
    "        height, width = image.shape[0], image.shape[1]\n",
    "\n",
    "        # cut image to just include bottom 20% of pixels\n",
    "        cropped_height = height-height//5\n",
    "        #cropped_image = [image[cropped_height:height,:]]\n",
    "\n",
    "        down_size = (width//down_factor, height//down_factor)\n",
    "        image = cv2.resize(image, down_size)\n",
    "\n",
    "        # create predictions for location and value of characters\n",
    "        # on the cropped image, will output (word, box) tuples\n",
    "        prediction = pipeline.recognize([image])[0]\n",
    "\n",
    "        # if no characters are found move on\n",
    "        if prediction == [[]]:\n",
    "            digit_count = 0\n",
    "\n",
    "        # if characters are found look at the predictions\n",
    "        else:\n",
    "            if plotting == True:\n",
    "                # plot the predictied box and tuples\n",
    "                keras_ocr.tools.drawAnnotations(image=image, predictions=prediction)\n",
    "                plt.show()\n",
    "\n",
    "            # loop over predicted (word, box) tuples and count number of digit characters\n",
    "            digit_count = 0\n",
    "            says_isis = False\n",
    "            for p in prediction:\n",
    "\n",
    "                # select word and box part of the tuple\n",
    "                value, box = p[0], p[1]\n",
    "\n",
    "                # check for 'isis' of any capitalization in image\n",
    "                if 'isis' in value.lower(): # may want to check 1, I, 5 variations on this\n",
    "                    says_isis = True\n",
    "                    print('found potential ISIS text')\n",
    "                \n",
    "                # if word is composed of just integers then \n",
    "                # count how many and incriment digit_count\n",
    "                if just_digits == False or (just_digits == True and value.isdigit()):\n",
    "\n",
    "                    # check that box is within the cropped height\n",
    "                    in_bounds = True\n",
    "                    for b in box:\n",
    "                        if b[1] < cropped_height:\n",
    "                            in_bounds = False\n",
    "                            break\n",
    "                            \n",
    "                    if in_bounds:\n",
    "                        digit_count += len(value)\n",
    "\n",
    "        print('digits count:', digit_count)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('ERR:', e)\n",
    "        digit_count, height, width, says_isis = 'ERR', 'ERR', 'ERR', 'ERR'\n",
    "\n",
    "    return digit_count, height, width, says_isis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72f90719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_rolls(outFile=outFile, append2outFile=True, batchDir=batchDir, plotting=False, max_images=None, save_each=100):\n",
    "   '''\n",
    "   This function loops over all images nested within batchDir\n",
    "   and saves the outputs from read_image() to a CSV file.\n",
    "\n",
    "   Parameters:\n",
    "\n",
    "      outFile (str, optional): path to CSV file where results from this \n",
    "                               function can be stored \n",
    "\n",
    "      append2outFile (bool, optional): if True will append to data in outFile \n",
    "                                       (if any exists), otherwise overwrites\n",
    "\n",
    "      batchDir (str, optional): path to directory of entire batch \n",
    "                                of ionogram scan images to analyze\n",
    "\n",
    "      plotting (bool, optional): just passes directly to read_image()\n",
    "\n",
    "      max_images (int, optional): maximum number of images used to iterate over\n",
    "\n",
    "      save_each (int, optional): save results to CSV after this number of images\n",
    "\n",
    "   Returns:\n",
    "\n",
    "      None\n",
    "\n",
    "   '''\n",
    "   # check if there is already data in the output file \n",
    "   if os.path.exists(outFile) and os.path.getsize(outFile)!=0:\n",
    "      found = False\n",
    "      header = False \n",
    "\n",
    "      df = pd.read_csv(outFile)\n",
    "      last_entry = batchDir + df['roll'].iloc[-1] + '/' + df['subdir'].iloc[-1] + '/' + df['image'].iloc[-1]\n",
    "      del df \n",
    "\n",
    "      # garbage collector\n",
    "      collected = gc.collect()\n",
    "      print(\"Garbage collector: collected\",\n",
    "               \"%d objects.\" % collected)\n",
    "\n",
    "   else: \n",
    "      found = True\n",
    "      header = True\n",
    "      last_entry = ''\n",
    "\n",
    "   # initialize lists to save values to in loop\n",
    "   rolls, subdirs, images = [], [], []\n",
    "   heights, widths, digit_counts = [], [], []\n",
    "   says_isis_lst = []\n",
    "\n",
    "   images_saved = 0\n",
    "   \n",
    "   # loop over all rolls in the batch 2 raw data directory\n",
    "   raw_contents = os.listdir(batchDir)\n",
    "   for roll in raw_contents:\n",
    "\n",
    "      # loop over all subdirectories within the roll\n",
    "      roll_contents = os.listdir(batchDir + roll) \n",
    "      for subdir in roll_contents:\n",
    "         \n",
    "         # loop over all images in the subdirectory\n",
    "         subdir_contents = os.listdir(batchDir + roll + '/' + subdir) \n",
    "         for image in subdir_contents:\n",
    "\n",
    "            # save full path of image\n",
    "            image_path = batchDir + roll + '/' + subdir + '/' + image\n",
    "\n",
    "            # skip over image if already analyzed in CSV\n",
    "            if found == False and last_entry == image_path:\n",
    "               found = True\n",
    "\n",
    "            if found == True:\n",
    "               images_saved += 1\n",
    "\n",
    "               if max_images != None and images_saved > max_images:\n",
    "                  sys.exit()\n",
    "\n",
    "               # save id of image\n",
    "               rolls.append(roll)\n",
    "               subdirs.append(subdir)\n",
    "               images.append(image)\n",
    "\n",
    "               # send to read_image to get aspect ratio, digit count, and isis text\n",
    "               num_of_digits, h, w, says_isis = read_image(image_path)\n",
    "\n",
    "               # save values\n",
    "               digit_counts.append(num_of_digits)\n",
    "               heights.append(h)\n",
    "               widths.append(w)\n",
    "               says_isis_lst.append(says_isis)              \n",
    "\n",
    "               # save to csv after a set number of images (perhaps best to make propto max images)\n",
    "               if images_saved % save_each == 0:\n",
    "\n",
    "                  # initialize dataframe and save results to csv\n",
    "                  # (redoing this each interation to not loose information)\n",
    "                  df_mapping_results = pd.DataFrame()\n",
    "\n",
    "                  df_mapping_results['roll'] = rolls\n",
    "                  df_mapping_results['subdir'] = subdirs\n",
    "                  df_mapping_results['image'] = images\n",
    "                  df_mapping_results['digit_count'] = digit_counts\n",
    "                  df_mapping_results['height'] = heights\n",
    "                  df_mapping_results['width'] = widths\n",
    "                  df_mapping_results['says_isis'] = says_isis_lst\n",
    "\n",
    "                  # mode = 'a' means it will append to existing data within the file\n",
    "                  if append2outFile == True:\n",
    "                     mode = 'a' \n",
    "\n",
    "                     # wipe lists now that they have been saved\n",
    "                     rolls, subdirs, images = [], [], []\n",
    "                     heights, widths, digit_counts = [], [], []\n",
    "                     says_isis_lst = []\n",
    "                     \n",
    "                  else: \n",
    "                     # this overwrites existing file\n",
    "                     mode = 'w'\n",
    "                     header = True\n",
    "\n",
    "                  df_mapping_results.to_csv(outFile, mode=mode, index=False, header=header)\n",
    "                  del df_mapping_results\n",
    "\n",
    "                  collected = gc.collect()\n",
    "                  print(\"Garbage collector: collected\",\n",
    "                           \"%d objects.\" % collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61437050",
   "metadata": {},
   "source": [
    "#### Running the functions\n",
    "Below, I run the read_all_rolls() function for the batch 2 raw data directory and it saves the results as it processes.\n",
    "\n",
    "Profilers are run on the functions as well and on VDI the read_all_rolls() call below spends 99.1% of the processing time on the num_of_digits, h, w, says_isis = read_image(image_path) line, 0.1% of the time saving the CSV, and 0.8% of the time doing the forced garbage cleaning. For the read_image() function first call 97.4% of the processing time is spent on the prediction = pipeline.recognize([image])[0] line and 2.5% of the time is spent on the image = keras_ocr.tools.read(image_path) line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe21dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "digits count: 0\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 241ms/step\n",
      "digits count: 0\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 345ms/step\n",
      "digits count: 0\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 342ms/step\n",
      "digits count: 0\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "digits count: 0\n",
      "Garbage collector: collected 8585 objects.\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "digits count: 0\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 321ms/step\n",
      "digits count: 0\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 304ms/step\n",
      "digits count: 0\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 352ms/step\n",
      "digits count: 0\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 229ms/step\n",
      "digits count: 0\n",
      "Garbage collector: collected 1449 objects.\n",
      "*** SystemExit exception caught in code being profiled."
     ]
    }
   ],
   "source": [
    "# profile the read_all_rolls() function\n",
    "# run it for 10 only to get results\n",
    "%lprun -f read_all_rolls read_all_rolls(max_images=10, save_each=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35226dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "digits count: 0\n"
     ]
    }
   ],
   "source": [
    "# (just one iteration here)\n",
    "image_path = batchDir + 'R014207709' + '/' + '145' + '/' + '1.png'\n",
    "%lprun -f read_image read_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77e04ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 351ms/step\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "digits count: 0\n"
     ]
    }
   ],
   "source": [
    "# (just one iteration here)\n",
    "image_path = batchDir + 'R014207709' + '/' + '145' + '/' + '10.png'\n",
    "%lprun -f read_image read_image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a385931c",
   "metadata": {},
   "source": [
    "#### Multiprocessing & Multithreading Implimentations\n",
    "Testing to see if either of these are a worthwhile approach. Multiprocessing and multithreading were able to work and do a better job at maximizing CPU and having good memory usage but after a while CPU usage still goes down to only ~30% utlization and this significantly slows down performance. \n",
    "\n",
    "I don't think true multithreading will ever work well with python but its useful here as there does seem to be some time the CPU takes a dip in utilization like when retreieving an image from the ethernet. \n",
    "\n",
    "I've tried a few different ways to do the multithreading/processing, one was setting it up to designate each thread to a set roll (can be seen in earlier version of this notebook) but that means its a long time before they join up so below I've implimented the multithreading/processing where the different threads are assigned to subdirectories within the processing loop. This means an alternate version of read_all_rolls() is created below, however read_image() can stay the same as already defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fb350fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subdir_analysis(roll, subdir, outFile=outFile, batchDir=batchDir, save_each=100):\n",
    "   '''\n",
    "   to support read_all_rolls_multithread() in multithreading\n",
    "   '''\n",
    "   # initialize values\n",
    "   rolls, subdirs, images = [], [], []\n",
    "   heights, widths, digit_counts = [], [], []\n",
    "   says_isis_lst, images_saved, mode = [], 0, 'a'\n",
    "\n",
    "   # loop over all images in the subdirectory\n",
    "   subdir_contents = os.listdir(batchDir + roll + '/' + subdir) \n",
    "   total_images = len(subdir_contents)\n",
    "   for image in subdir_contents:\n",
    "\n",
    "      # save full path of image\n",
    "      image_path = batchDir + roll + '/' + subdir + '/' + image\n",
    "      images_saved += 1\n",
    "      print(images_saved, image_path)\n",
    "\n",
    "      # save id of image\n",
    "      rolls.append(roll)\n",
    "      subdirs.append(subdir)\n",
    "      images.append(image)\n",
    "\n",
    "      # send to read_image to get aspect ratio, digit count, and isis text\n",
    "      num_of_digits, h, w, says_isis = read_image(image_path)\n",
    "\n",
    "      # save values\n",
    "      digit_counts.append(num_of_digits)\n",
    "      heights.append(h)\n",
    "      widths.append(w)\n",
    "      says_isis_lst.append(says_isis)              \n",
    "\n",
    "      # save to csv after a set number of images or if last image\n",
    "      if images_saved % save_each == 0 or images_saved == total_images:\n",
    "\n",
    "         # initialize dataframe and save results to csv\n",
    "         df_mapping_results = pd.DataFrame()\n",
    "\n",
    "         df_mapping_results['roll'] = rolls\n",
    "         df_mapping_results['subdir'] = subdirs\n",
    "         df_mapping_results['image'] = images\n",
    "         df_mapping_results['digit_count'] = digit_counts\n",
    "         df_mapping_results['height'] = heights\n",
    "         df_mapping_results['width'] = widths\n",
    "         df_mapping_results['says_isis'] = says_isis_lst\n",
    "\n",
    "         # wipe lists now that they have been saved\n",
    "         rolls, subdirs, images = [], [], []\n",
    "         heights, widths, digit_counts = [], [], []\n",
    "         says_isis_lst = []\n",
    "            \n",
    "         df_mapping_results.to_csv(outFile, mode=mode, index=False, header=False)\n",
    "         del df_mapping_results\n",
    "\n",
    "         collected = gc.collect()\n",
    "         print(\"Garbage collector: collected\",\n",
    "                  \"%d objects.\" % collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f40e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_rolls_multithread(thread_count=6):\n",
    "   '''\n",
    "   A more bare bones version of read_all_rolls() taylored to support multithreading\n",
    "   (reference read_all_rolls() for more info)\n",
    "   --> does not yet support re-runs with no overwriting like the origional\n",
    "   '''\n",
    "   # inialize header for the output file\n",
    "   header = {'roll':[], 'subdir':[], 'image':[], 'digit_count':[],  \n",
    "                  'height':[], 'width':[], 'says_isis':[]}\n",
    "   df_header = pd.DataFrame(data=header)\n",
    "   df_header.to_csv(outFile, mode='w', index=False, header=True)\n",
    "\n",
    "   subdir_threads = []\n",
    "\n",
    "   # loop over all rolls in the batch 2 raw data directory\n",
    "   raw_contents = os.listdir(batchDir)\n",
    "   for roll in raw_contents:\n",
    "\n",
    "      # loop over all subdirectories within the roll\n",
    "      roll_contents = os.listdir(batchDir + roll) \n",
    "      for subdir in roll_contents:\n",
    "\n",
    "         # setup a set of subdirectories to run\n",
    "         # (if running for all rolls need boundary control here)\n",
    "         if len(subdir_threads) < thread_count:\n",
    "            subdir_threads.append(subdir)\n",
    "\n",
    "         else:\n",
    "            # create threads (replace 'Thread' with 'Process' for multiprocessing)\n",
    "            threads = [Thread(target=subdir_analysis, args=[roll, subdir]) \n",
    "                        for subdir in subdir_threads]\n",
    "\n",
    "            # start the threads\n",
    "            for thread in threads:\n",
    "               thread.start()\n",
    "\n",
    "            # wait for completion\n",
    "            for thread in threads:\n",
    "               thread.join()\n",
    "\n",
    "            print('#### All threads done, beginning a new set ####')\n",
    "            \n",
    "            # clear list and append first one\n",
    "            subdir_threads = []\n",
    "            subdir_threads.append(subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f27a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/145/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/148/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/151/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/149/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/150/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/153/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/147/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/146/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-104-21/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/152/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/154/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-108-16/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-103-21/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-105-21/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-106-16/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-107-16/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-111-50/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-109-06/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-112-50/1.png\n",
      "1 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-113-50/1.png\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 10s 10s/step\n",
      "1/1 [==============================] - 13s 13s/step\n",
      "1/1 [==============================] - 13s 13s/step\n",
      "1/1 [==============================] - 13s 13s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-109-06/10.png\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-105-21/10.png\n",
      "1/1 [==============================] - 14s 14s/step\n",
      "1/1 [==============================] - 16s 16s/step\n",
      "1/1 [==============================] - 16s 16s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/148/10.png\n",
      "1/1 [==============================] - 10s 10s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/145/10.png\n",
      "1/1 [==============================] - 19s 19s/step\n",
      "1/1 [==============================] - 21s 21s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 23s 23s/step\n",
      "1/1 [==============================] - 23s 23s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/146/10.png\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-107-16/10.png\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-103-21/10.png\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 11s 11s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-104-21/10.png\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "digits count: 0\n",
      "3 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-105-21/100.png\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "digits count: 0\n",
      "3 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-109-06/100.png\n",
      "1/1 [==============================] - 31s 31s/step\n",
      "1/1 [==============================] - 12s 12s/step\n",
      "1/1 [==============================] - 34s 34s/step\n",
      "1/1 [==============================] - 27s 27s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/151/10.png\n",
      "1/1 [==============================] - 17s 17s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/152/10.png\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/153/10.png\n",
      "1/1 [==============================] - 12s 12s/step\n",
      "1/1 [==============================] - 37s 37s/step\n",
      "digits count: 0\n",
      "3 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-103-21/100.png\n",
      "1/1 [==============================] - 41s 41s/step\n",
      "1/1 [==============================] - 42s 42s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/147/10.png\n",
      "1/1 [==============================] - 47s 47s/step\n",
      "1/1 [==============================] - 23s 23s/step\n",
      "1/1 [==============================] - 23s 23s/step\n",
      "1/1 [==============================] - 26s 26s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-111-50/10.png\n",
      "1/1 [==============================] - 22s 22s/step\n",
      "digits count: 0\n",
      "3 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-104-21/100.png\n",
      "1/1 [==============================] - 19s 19s/step\n",
      "1/1 [==============================] - 23s 23s/step\n",
      "digits count: 0\n",
      "4 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-105-21/101.png\n",
      "1/1 [==============================] - 27s 27s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/154/10.png\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/149/10.png\n",
      "1/1 [==============================] - 16s 16s/step\n",
      "1/1 [==============================] - 37s 37s/step\n",
      "digits count: 0\n",
      "3 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/148/11.png\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/150/10.png\n",
      "1/1 [==============================] - 20s 20s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-108-16/10.png\n",
      "1/1 [==============================] - 27s 27s/step\n",
      "digits count: 0\n",
      "4 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-109-06/101.png\n",
      "1/1 [==============================] - 22s 22s/step\n",
      "1/1 [==============================] - 12s 12s/step\n",
      "1/1 [==============================] - 21s 21s/step\n",
      "1/1 [==============================] - 16s 16s/step\n",
      "1/1 [==============================] - 30s 30s/step\n",
      "digits count: 0\n",
      "3 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/145/11.png\n",
      "digits count: 0\n",
      "3 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/147/11.png\n",
      "1/1 [==============================] - 34s 34s/step\n",
      "digits count: 0\n",
      "4 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-103-21/101.png\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-113-50/10.png\n",
      "digits count: 0\n",
      "3 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/152/100.png\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "1/1 [==============================] - 18s 18s/step\n",
      "digits count: 0\n",
      "3 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/154/100.png\n",
      "1/1 [==============================] - 19s 19s/step\n",
      "1/1 [==============================] - 19s 19s/step\n",
      "digits count: 0\n",
      "3 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-107-16/100.png\n",
      "digits count: 0\n",
      "4 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-104-21/101.png\n",
      "1/1 [==============================] - 33s 33s/step\n",
      "1/1 [==============================] - 20s 20s/step\n",
      "digits count: 0\n",
      "3 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/151/11.png\n",
      "1/1 [==============================] - 24s 24s/step\n",
      "1/1 [==============================] - 22s 22s/step\n",
      "digits count: 0\n",
      "3 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/146/11.png\n",
      "1/1 [==============================] - 21s 21s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "digits count: 0\n",
      "3 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/153/11.png\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "digits count: 0\n",
      "3 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-111-50/100.png\n",
      "1/1 [==============================] - 19s 19s/step\n",
      "digits count: 0\n",
      "5 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-109-06/102.png\n",
      "1/1 [==============================] - 38s 38s/step\n",
      "digits count: 0\n",
      "2 L:/DATA/Alouette_I/BATCH_II_raw/R014207709/C-106-16/10.png\n"
     ]
    }
   ],
   "source": [
    "read_all_rolls_multithread(thread_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b8b4f1",
   "metadata": {},
   "source": [
    "#### EasyOCR Implimentation\n",
    "We are pretty limitted with just running these models on CPU but this should be faster, lets see how well it does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6d09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e92df5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "text_reader = easyocr.Reader(['en']) # load model into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7727c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_easyOCR(image_path, down_factor=1):\n",
    "    '''\n",
    "    '''\n",
    "    try: \n",
    "        # read in with cv2 \n",
    "        image_png = cv2.imread(image_path, 0)\n",
    "        height, width = image_png.shape\n",
    "\n",
    "        # downsize \n",
    "        down_size = (width//down_factor, height//down_factor)\n",
    "        image_png = cv2.resize(image_png, down_size)\n",
    "\n",
    "        # save as jpeg (overwrite)\n",
    "        image_name = image_path.split('/')[-1]\n",
    "        temp_image_path = save_dir + image_name.replace('.png', '.jpg') # just want the name part\n",
    "        cv2.imwrite(temp_image_path, image_png) # need to delete these if actually scaling this\n",
    "\n",
    "        # do reading \n",
    "        results = text_reader.readtext(temp_image_path)\n",
    "        for (bbox, text, prob) in results:\n",
    "            print(text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('ERR:', e)\n",
    "        digit_count, height, width, says_isis = 'ERR', 'ERR', 'ERR', 'ERR'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d6394",
   "metadata": {},
   "source": [
    "Does not give good results for reading in ISIS 1 metadata image but not so bad for ionogram film annotation. Overall found worse perfomance and not as much of a speed difference as I expected while just on CPU. Batching and GPU acceleration should help but perhaps not enough to make redoing this with EasyOCR worthwhile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65500653",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = batchDir + 'R014207709' + '/' + 'C-111-50' + '/' + '9.png' \n",
    "#image_path = batchDir + 'R014207709' + '/' + '145' + '/' + '1.png'\n",
    "#image_path = batchDir + 'R014207709' + '/' + 'C-109-06' + '/' + '13.png'\n",
    "read_image_easyOCR(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39e8fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC 6S\n",
      "'Sc\n",
      "CSSX Sc\n",
      "SC &S\n",
      "S\n",
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 46.7089 s\n",
      "File: <ipython-input-16-c490ec352688>\n",
      "Function: read_image_easyOCR at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def read_image_easyOCR(image_path, down_factor=1):\n",
      "     2                                               '''\n",
      "     3                                               '''\n",
      "     4         1         10.0     10.0      0.0      try: \n",
      "     5                                                   # read in with cv2 \n",
      "     6         1   55643461.0 55643461.0     11.9          image_png = cv2.imread(image_path, 0)\n",
      "     7         1         79.0     79.0      0.0          height, width = image_png.shape\n",
      "     8                                           \n",
      "     9                                                   # downsize \n",
      "    10         1         22.0     22.0      0.0          down_size = (width//down_factor, height//down_factor)\n",
      "    11         1      36075.0  36075.0      0.0          image_png = cv2.resize(image_png, down_size)\n",
      "    12                                           \n",
      "    13                                                   # save as jpeg (overwrite)\n",
      "    14         1         84.0     84.0      0.0          image_name = image_path.split('/')[-1]\n",
      "    15         1         54.0     54.0      0.0          temp_image_path = save_dir + image_name.replace('.png', '.jpg') # just want the name part\n",
      "    16         1  115864692.0 115864692.0     24.8          cv2.imwrite(temp_image_path, image_png) # crash if dir not already existing?\n",
      "    17                                           \n",
      "    18                                                   # do reading \n",
      "    19         1  295498131.0 295498131.0     63.3          results = text_reader.readtext(temp_image_path)\n",
      "    20         5        106.0     21.2      0.0          for (bbox, text, prob) in results:\n",
      "    21         5      46572.0   9314.4      0.0              print(text)\n",
      "    22                                           \n",
      "    23                                               except Exception as e:\n",
      "    24                                                   print('ERR:', e)\n",
      "    25                                                   digit_count, height, width, says_isis = 'ERR', 'ERR', 'ERR', 'ERR'"
     ]
    }
   ],
   "source": [
    "# results from profiler on local\n",
    "%lprun -f read_image_easyOCR read_image_easyOCR(image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf210",
   "language": "python",
   "name": "tf210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
