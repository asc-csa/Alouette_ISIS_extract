{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISIS Gridding for Metadata\n",
    "By: Salini Punchiwickrama\n",
    "\n",
    "This notebook takes the gridding code developed during the Alouette project to now read the metadata for the ISIS ionograms. \n",
    "All the code can be found on either the scan2data file on GitHub under the ISIS-working branch or under the Summer of 2022 file on Livelink in the Alouette folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import ntpath\n",
    "from scipy.signal import find_peaks\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates a random sub-directory and image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_ran_subdir (subdir_path):\n",
    "    \"\"\"\" Generates a random subdirectory\n",
    "    Requires: \n",
    "    subdir_path: name of the path for the subdirectory\n",
    "     L:/DATA/ISIS/ISIS_101300030772/b*/B1* \"\"\"\n",
    "\n",
    "    all_subs = glob.glob(subdir_path) #creates a list of all subdirects \n",
    "    selected_sub = all_subs[random.randint(0, len(all_subs)-1)] #picks a random one from list\n",
    "    return (selected_sub)\n",
    "\n",
    "def gen_ran_img (subdir_path, img):\n",
    "    \"\"\"\" Generates a random image \"\"\"\n",
    "\n",
    "    all_img = glob.glob(subdir_path + img) #creates list of all images\n",
    "\n",
    "    selected_img = all_img[random.randint(0, len(all_img) - 1)]\n",
    "    return (selected_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subdir path: C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/Test-Images\n",
      "Random image in subdir path C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/Test-Images\\Image0024.png\n"
     ]
    }
   ],
   "source": [
    "#SD_Path = sub-directory path\n",
    "#Test folder contains first 30 images from L:/DATA/ISIS/ISIS_101300030772/b3_R014207773/B1-35-31 ISIS B D-1059\n",
    "SD_PATH = os.path.realpath(\"Test-Images\")\n",
    "\n",
    "#Image path\n",
    "I_PATH = gen_ran_img(SD_PATH, '/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DICTIONARIES\n",
    "# Labelling of coordinates\n",
    "LABELS_NUM = [1,2,4,8]\n",
    "LABELS_CAT_DOT = ['day_1','day_2','day_3','hour_1','hour_2','minute_1','minute_2','second_1', 'second_2','station_code']\n",
    "#LABELS_CAT_DIGIT = ['satellite_number','year','day_1','day_2','day_3','hour_1','hour_2','minute_1','minute_2',\n",
    "                    #'second_1', 'second_2', 'station_number_1','station_number_2']\n",
    "LABELS_CAT_NUM = ['Operating Mode 1','Operating Mode 2','Station Number 1', 'Station Number 2', 'Year 1', 'Year 2', 'Day 1', 'Day 2', 'Day 3',\n",
    "                    'Hour 1', 'Hour 2', 'Min 1', 'Min 2', 'Sec 1', 'Sec 2']\n",
    "LABELS_DICT = ['dict_cat_digit','dict_num_digit','dict_cat_dot','dict_num_dot',]\n",
    "\n",
    "#Defaults for dictionary mappings of coordinates to labels\n",
    "DEFAULT_DICT_CAT_DIGIT = (53,21,661) #mean_dist_default,first_peak_default,last_peak_default\n",
    "DEFAULT_DICT_NUM_DIGIT = (47,41,20) #mean_dist_default,first_peak_default,dist_btw_peaks for peak detection\n",
    "\n",
    "#DEFAULT_DICT_CAT_DIGIT_F = (43,23,540) #mean_dist_default,first_peak_default,last_peak_default for those in LIST_DIRECTORY_DOTS \n",
    "#DEFAULT_DICT_NUM_DIGIT_F = (40,37,20) #mean_dist_default,first_peak_default,dist_btw_peaks for peak detection for those in LIST_DIRECTORY_DOTS \n",
    "\n",
    "#DEFAULT_DICT_CAT_DOT = (59,20,549)##mean_dist_default,first_peak_default,last_peak_default\n",
    "#DEFAULT_DICT_NUM_DOT = (15,32,10) #mean_dist_default,first_peak_default,dist_btw_peaks for peak detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From helper_functions.py\n",
    "def record_loss (df, function_name, subdir_location, columns_to_extract = None, loss_extraction = None):\n",
    "    \"\"\"Create dataframe that records loss.\"\"\"\n",
    "\n",
    "    if columns_to_extract is None:\n",
    "        columns_to_extract = ['file_name']\n",
    "    if loss_extraction is None:\n",
    "        loss_extraction = []\n",
    "    if len(loss_extraction) == 0:\n",
    "        # function should return NA if there an error\n",
    "        loss_extraction = df.isna().any(axis=1)\n",
    "    # Record the files whose extraction was not successful\n",
    "    df_loss_extraction = df[loss_extraction].copy()\n",
    "    df_loss_extraction = df_loss_extraction[columns_to_extract]\n",
    "    df_loss_extraction['func_name'] = function_name\n",
    "    df_loss_extraction[ 'subdir_name'] = subdir_location\n",
    "\n",
    "    return df_loss_extraction, loss_extraction\n",
    "\n",
    "\n",
    "# From scan2data > image_segmentation > trim_raw_metadata > connected_components_metadata_location\n",
    "def metadata_location(meta, min_count = 50, max_count = 1000):\n",
    "    \"\"\"\"Use connected component algorithm to find the location of the metadata\"\"\"\n",
    "    \n",
    "    #run algorithm on metadata section\n",
    "    _, labelled = cv2.connectedComponents(meta)\n",
    "\n",
    "    #Dictionary of label:counts\n",
    "    unique, counts = np.unique(labelled, return_counts = True)\n",
    "    dict_components = dict(zip(unique, counts))\n",
    "\n",
    "    #Remove outliers // Remove pixels not part of metadata\n",
    "    dict_subset = {}\n",
    "    dict_outlier = {}\n",
    "    for k,v, in dict_components.items():\n",
    "        if v > min_count and v < max_count:\n",
    "            dict_subset[k] = v\n",
    "        else:\n",
    "            dict_outlier[k] = v\n",
    "    \n",
    "    if key_list_to_remove := list(dict_outlier.keys()):\n",
    "        for k in key_list_to_remove:\n",
    "            labelled[labelled == k] = 0\n",
    "    \n",
    "    return labelled\n",
    "\n",
    "#From scan2data > image_segmentation > trim_raw_metadata\n",
    "#test provided values and change if needed\n",
    "def bottomside_metadata_trimming(connected_meta, opened_meta,\n",
    "                                 h_window = 100, w_window = 700, starting_y = 0, starting_x = 15, step_size = 10, trim_if_small = 10):\n",
    "    \"\"\"Sliding window method to locate and trim bottomside metadata\"\"\"\n",
    "    \n",
    "    def sliding_window(image, starting_y, starting_x, h_window, w_window, step_size):\n",
    "        \"\"\"sliding window generator object\"\"\"\n",
    "        h_img, w_img = np.shape(image)\n",
    "        for y in range(starting_y, h_img - h_window, step_size):\n",
    "            for x in range(starting_x, w_img - w_window, step_size):\n",
    "                yield y,x,image[y:y + h_window, x:x + w_window]\n",
    "    \n",
    "    h_raw, w_raw = np.shape(opened_meta)\n",
    "    \n",
    "    if h_window + step_size  >= h_raw:\n",
    "        h_window = h_raw -trim_if_small\n",
    "    if w_window + step_size>= w_raw:\n",
    "        w_window = w_raw -trim_if_small\n",
    "    \n",
    "    s = sliding_window(connected_meta, starting_y, starting_x, h_window, w_window, step_size)\n",
    "    \n",
    "    max_window = connected_meta[starting_y:h_window+starting_y,\n",
    "                 starting_x:w_window+starting_x ]\n",
    "    max_mean = np.mean(max_window)\n",
    "    y_max= starting_y\n",
    "    x_max = starting_x\n",
    "    \n",
    "    for y, x, window in s:\n",
    "        tmp = window\n",
    "        mean = np.mean(tmp)\n",
    "        if mean > max_mean:\n",
    "            max_window = tmp\n",
    "            max_mean  = mean\n",
    "            y_max = y\n",
    "            x_max = x\n",
    "\n",
    "    trimmed_metadata =  opened_meta[y_max:y_max + h_window, x_max:x_max + w_window]\n",
    "\n",
    "    return trimmed_metadata\n",
    "\n",
    "#From scan2data > image_segmentation > trim_raw_metadata\n",
    "def trimming_metadata(raw_metadata,type_metadata, opening_kernal_size = (3,3), median_kernal_size = 5):\n",
    "    \"\"\"\"Trim the rectangle containing metadata to smallest workable area.\"\"\"\n",
    "\n",
    "    try:\n",
    "        #Filtering to reduce noise\n",
    "        median_filtered_meta = cv2.medianBlur(raw_metadata, median_kernal_size)\n",
    "        \n",
    "        #Opening operation: Eroision + Dilation\n",
    "        kernal_opening = np.ones(opening_kernal_size, dtype = np.uint8)\n",
    "        opened_meta = cv2.morphologyEx(median_filtered_meta, cv2.MORPH_OPEN, kernal_opening)\n",
    "        \n",
    "\n",
    "        # Binarization\n",
    "        _, metadata_binary = cv2.threshold(opened_meta, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        #Run connected components algorithm\n",
    "        connected_meta = metadata_location(metadata_binary)\n",
    "\n",
    "        trimmed_metadata = bottomside_metadata_trimming(connected_meta, metadata_binary)\n",
    "        #bottomside_metadata_trimming function is from same location\n",
    "\n",
    "        #Checking\n",
    "        #cv2.imshow(\"test\", trimmed_metadata)\n",
    "        #cv2.waitKey(0)\n",
    "        return (trimmed_metadata)\n",
    "    except:\n",
    "        return (np.nan)\n",
    "    \n",
    "\n",
    "##From scan2data > metadata_translation > leftside_metdata_grid_mapping\n",
    "def indices_highest_bin(list_coords, nbins = 500, peak_threshold = 0.2, distance_bwtn_peaks = 30 ):\n",
    "    \"\"\"\" returns indices of most common values using binning\n",
    "    list_coords: (np.arrray)\"\"\"\n",
    "\n",
    "    arr_coord= np.array(list_coords)\n",
    "\n",
    "    mean_coords = np.mean(arr_coord)\n",
    "    std_coords = np.std(arr_coord)\n",
    "    no_outlier_coords = arr_coord[np.abs(arr_coord - mean_coords) < 3 * std_coords]\n",
    "\n",
    "    #Binning\n",
    "    counts, bin_edges = np.histogram(no_outlier_coords, bins=nbins)\n",
    "\n",
    "    #Detect peaks\n",
    "    counts_norm = (counts - np.min(counts)) / (np.map(counts) - np.min(counts))\n",
    "    select_peaks = find_peaks(counts_norm, distance = distance_bwtn_peaks, promience = peak_threshold)    \n",
    "\n",
    "    return select_peaks, bin_edges, counts\n",
    "\n",
    "#From scan2data > metadata_translation> leftside_metadata_grid_mapping \n",
    "def extract_centroids(cut_metadata, file_name, min_pixels = 50, max_pixels = 1000, max_area_dot = 120):\n",
    "    \"\"\"Takes in cut metadata and extracts centroids\n",
    "    \n",
    "    cut_metadata: np.array\"\"\"\n",
    "\n",
    "    try:\n",
    "        _, __, stats, centroids = cv2.connectedComponentsWithStats(cut_metadata) \n",
    "        area_centroids = stats[:,-1]\n",
    "\n",
    "        centroids_metadata = centroids[np.logical_and(area_centroids > min_pixels, area_centroids < max_pixels),:]\n",
    "        #^ consider adjusting min and max range\n",
    "\n",
    "        zip_centroids = list(zip(*centroids_metadata))\n",
    "        #print (\"ZIPPED CENTROIDS\", zip_centroids)\n",
    "        col_centroids = list((zip_centroids[0]))\n",
    "        #print (\"col\", col_centroids)\n",
    "        #for i in zip_centroids:\n",
    "            #print (i)\n",
    "        row_centroids = list((zip_centroids[1]))\n",
    "\n",
    "        #Determine dot type (temp)\n",
    "        #area_centroids = area_centroids[np.logical_and(area_centroids > min_num_pixels, area_centroids < max_number_pixels)]\n",
    "        #median_area = np.median(area_centroids)\n",
    "        #The line below is commented to prevent giving the dot items manually\n",
    "        #if any([dir_dot in file_name for dir_dot in LIST_DIRECTORY_DOTS]) and median_area < max_area_dot:\n",
    "        #is_dot = median_area < max_area_dot\n",
    "\n",
    "        return col_centroids, row_centroids\n",
    "    \n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "#From scan2data > image_segmentation > extract_ionogram_from_scan\n",
    "#For segment_metadata\n",
    "def limits_ionogram(raw_img, row_or_col, starting_index_col = 15):\n",
    "\n",
    "    mean_values = np.mean(raw_img, row_or_col)\n",
    "\n",
    "    #normalize mean\n",
    "    norm_mean = (mean_values - np.min(mean_values)) / np.max(mean_values)\n",
    "    thresh = np.mean(norm_mean)\n",
    "\n",
    "    if row_or_col == 0:\n",
    "        #Protect against scans that includes cuts from another ionogram\n",
    "        limits = [i for i, mean in enumerate(norm_mean) if mean > thresh and i > starting_index_col]\n",
    "    else:\n",
    "        limits = [i for i, mean in enumerate(norm_mean) if mean > thresh]\n",
    "\n",
    "    return limits[0], limits[-1]\n",
    "\n",
    "\n",
    "#From scan2data > image_segmentation > extract_ionogram_from_scan\n",
    "# For segment_metadata\n",
    "def extract_ionogram(raw_img_array):\n",
    "    \"\"\"\"this function is here for now to get limits of ionogram. \n",
    "    can later be changed to include ionogram graph\"\"\"\n",
    "    try:\n",
    "\n",
    "    #Extract coordinate delimiting the graph\n",
    "        x_left, x_right = limits_ionogram(raw_img_array, 0)\n",
    "        y_upper, y_lower = limits_ionogram(raw_img_array, 1)\n",
    "\n",
    "        limits = [x_left, x_right, y_upper, y_lower]\n",
    "        #ionogram = raw_img_array[y_upper:y_lower,x_left_:x_right]\n",
    "\n",
    "        #For checking the metadata part of image\n",
    "        imgMetadataPart = raw_img_array[y_upper:y_lower, 15:(x_left - 1)]\n",
    "        #cv2.imshow(\"test Metadata\", imgMetadataPart)\n",
    "        #cv2.waitKey(0)\n",
    "\n",
    "        return (limits)\n",
    "    except:\n",
    "        return (np.nan)\n",
    "\n",
    "\n",
    "#From scan2data > image_segmentation > extract_metadata_from_scan\n",
    "def extract_metadata (raw_img, limits_iono):\n",
    "    \"\"\"Extract metadata from raw scanned image and return coordinates delimiting its limits\"\"\"\n",
    "\n",
    "    #Limits for ionogram\n",
    "    #print (limits_iono)\n",
    "    x_left_lim = limits_iono[0][0]\n",
    "    x_right_lim = limits_iono[0][1]\n",
    "    y_upper_lim = limits_iono[0][2] \n",
    "    y_lower_lim = limits_iono[0][3]\n",
    "\n",
    "    #Extract retangular block below** ionogram\n",
    "    rect_left = raw_img[:,0:x_left_lim]\n",
    "    rect_right = raw_img[:,x_right_lim::]\n",
    "    rect_top = raw_img[0:y_upper_lim, :]\n",
    "    rect_bottom = raw_img[y_lower_lim:: ,:]\n",
    "\n",
    "    #Assumption: the location of the metadata will correspond to rectangle with the highest area\n",
    "    rect_list = [rect_left, rect_right, rect_top, rect_bottom]\n",
    "    rect_areas = [rect.shape[0] * rect.shape[1] for rect in rect_list]\n",
    "    dict_mapping_meta = {0:'left', 1:\"right\", 2:\"top\", 3:'bottom'}\n",
    "\n",
    "    type_metadata_idx = np.argmax(rect_areas)\n",
    "    raw_metadata = rect_list[type_metadata_idx]\n",
    "    type_metadata = dict_mapping_meta[type_metadata_idx]\n",
    "\n",
    "    \n",
    "    return (type_metadata, raw_metadata)\n",
    "\n",
    "#From scan2data > metadata_translation > translate_leftside_metadata\n",
    "def map_coord_to_metadata(list_cat_coord,list_num_coord,dict_mapping_cat, dict_mapping_num):\n",
    "    \"\"\"Map coordinate of metadata centroids to information\n",
    "    \n",
    "    :param list_cat_coord: list of metadata positions to map to categories   \n",
    "    :type list_cat_coord: list\n",
    "    :param list_num_coord: list of metadata positions to map to numbers \n",
    "    :type list_num_coord: list\n",
    "    :param dict_mapping_cat: dictionary used to map coordinate positions to categories\n",
    "    :type dict_mapping_cat: dict\n",
    "    :param dict_mapping_num: dictionary used to map coordinate positions to numbers\n",
    "    :type dict_mapping_num: dict\n",
    "    :returns: dict_metadata\n",
    "    :rtype: dict\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    list_coord = list(zip(list_cat_coord,list_num_coord))\n",
    "    coord_mapping_cat = dict_mapping_cat.keys()\n",
    "    coord_mapping_num = dict_mapping_num.keys()\n",
    "        \n",
    "    dict_metadata={}\n",
    "    for cat_coord, num_coord in list_coord:\n",
    "        cat_key = min(coord_mapping_cat, key=lambda x:abs(x-cat_coord))\n",
    "        num_key = min(coord_mapping_num, key=lambda x:abs(x-num_coord))\n",
    "            \n",
    "        cat = dict_mapping_cat[cat_key]\n",
    "        num = dict_mapping_num[num_key]\n",
    "            \n",
    "            # tODO: improve for many num\n",
    "        if cat in dict_metadata:\n",
    "            dict_metadata[cat].append(num)\n",
    "        else:\n",
    "            dict_metadata[cat] = [num]\n",
    "    #print (dict_metadata)\n",
    "    return dict_metadata\n",
    "    #except:\n",
    "        #return np.nan\n",
    "    \n",
    "# From scan2data > metadata_translation > leftside_metadata_grid_mapping\n",
    "def get_leftside_metadata_grid_mapping(list_x_digit,list_y_digit,dir_name,\n",
    "                      difference_ratio=0.75,use_defaults=True):\n",
    "    \n",
    "    \"\"\"Determines and returns the the mapping between coordinate values on a metadata image \n",
    "    and metadata labels in a subdirectory, for metadata of types dot and digits, as well as returns \n",
    "    the histogram used to generate each mapping\n",
    "    \n",
    "    \"\"\"\n",
    "    # Dictionary of dictionaries that map labels to coordinate point in metadata\n",
    "    all_labels = [LABELS_CAT_NUM, LABELS_NUM]\n",
    "    all_dict_mapping = {}\n",
    "    all_dict_hist = {}\n",
    "    # Different protocols depending on the type of dictionary mappings\n",
    "    for i, list_coord in (enumerate([list_x_digit,list_y_digit])):\n",
    "        type_dict = LABELS_DICT[i]\n",
    "        labels = all_labels[i]\n",
    "        try:\n",
    "            if 'cat' in type_dict:\n",
    "                if type_dict == 'dict_cat_digit':\n",
    "                    #if any([dir_dot in dir_name for dir_dot in LIST_DIRECTORY_DOTS]):\n",
    "                        #mean_dist_default,first_peak_default,last_peak_default=DEFAULT_DICT_CAT_DIGIT_F\n",
    "                    #else:\n",
    "                    mean_dist_default,first_peak_default,last_peak_default = DEFAULT_DICT_CAT_DIGIT\n",
    "            \n",
    "                elif type_dict == 'dict_cat_dot':\n",
    "                    mean_dist_default,first_peak_default,last_peak_default = DEFAULT_DICT_CAT_DOT\n",
    "                try:\n",
    "                    idx_peaks,bin_edges,counts = indices_highest_bin(list_coord)\n",
    "                    peaks = bin_edges[np.array(idx_peaks)] #coordinate values on a metadata image probably corresponding to metadata\n",
    "                    \n",
    "                    n_labels = len(labels)\n",
    "                    first_peak = peaks[0]\n",
    "                    last_peak = peaks[-1]\n",
    "\n",
    "                    if use_defaults and abs(last_peak -last_peak_default)  > difference_ratio*mean_dist_default:\n",
    "                        last_peak = last_peak_default\n",
    "                    if use_defaults and abs(first_peak -first_peak_default)  > difference_ratio*mean_dist_default:\n",
    "                        first_peak = first_peak_default\n",
    "                    \n",
    "                    mean_dist_btw_peaks = (last_peak - first_peak)/(n_labels -1)\n",
    "                    list_peaks = [int(round(first_peak + i* mean_dist_btw_peaks)) for i in range(0,n_labels )]\n",
    "                    \n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] = (idx_peaks,bin_edges,counts)\n",
    "                \n",
    "\n",
    "                except:\n",
    "                    last_peak = last_peak_default\n",
    "                    first_peak = first_peak_default\n",
    "                    mean_dist_btw_peaks = mean_dist_default\n",
    "                    n_labels = len(labels)\n",
    "                    list_peaks = [int(round(first_peak + i* mean_dist_btw_peaks)) for i in range(0, n_labels)]\n",
    "                    \n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] = {}\n",
    "                \n",
    "            elif 'num' in type_dict:\n",
    "                if  type_dict == 'dict_num_digit':\n",
    "                    #if any([dir_dot in dir_name for dir_dot in LIST_DIRECTORY_DOTS]):\n",
    "                        #mean_dist_default,peak_0_default,dist_btw_peaks = DEFAULT_DICT_NUM_DIGIT_F\n",
    "                    #else:\n",
    "                    mean_dist_default,peak_0_default,dist_btw_peaks = DEFAULT_DICT_NUM_DIGIT\n",
    "                elif type_dict == 'dict_num_dot':\n",
    "                    mean_dist_default,peak_0_default,dist_btw_peaks= DEFAULT_DICT_NUM_DOT\n",
    "\n",
    "                    \n",
    "                try:\n",
    "                    idx_peaks,bin_edges,counts = indices_highest_bin(list_coord,peak_prominence_threshold=0.3,nbins=100,distance_between_peaks=dist_btw_peaks)\n",
    "                \n",
    "                    peaks = bin_edges[np.array(idx_peaks)]                \n",
    "                    peak_0 = peaks[0]\n",
    "                    if use_defaults and abs(peak_0 -peak_0_default)  > difference_ratio*mean_dist_default:\n",
    "                        peak_0 = peak_0_default\n",
    "                \n",
    "                    # only first three peaks are deemed relevant\n",
    "                    if len(peaks) < 3:\n",
    "                        max_idx = 2\n",
    "                    else:\n",
    "                        max_idx = 3\n",
    "                \n",
    "                    mean_dist_btw_peaks = np.mean([peaks[i+1]-peaks[i] for i in range(0,max_idx)])\n",
    "                    if use_defaults and abs(mean_dist_btw_peaks - mean_dist_default)  > difference_ratio*dist_btw_peaks:\n",
    "                        mean_dist_btw_peaks = mean_dist_default\n",
    "                    list_peaks = [int(round(peak_0 + i* mean_dist_btw_peaks)) for i in range(0,len(labels))]\n",
    "                \n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] = (idx_peaks,bin_edges,counts)\n",
    "                except:\n",
    "                    peak_0 = peak_0_default\n",
    "                    mean_dist_btw_peaks = mean_dist_default\n",
    "                    list_peaks = [int(round(peak_0 + i* mean_dist_btw_peaks)) for i in range(0,len(labels))]\n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] =  {}\n",
    "        except:\n",
    "            all_dict_mapping[type_dict] ={}\n",
    "            all_dict_hist[type_dict] =  {}\n",
    "\n",
    "    return all_dict_mapping, all_dict_hist\n",
    "\n",
    "#From scan2data> image_segmentation > segment_images_in_subdir.py\n",
    "#some variables here are not necessary and can be removed, ie. height, width...\n",
    "def segment_metadata(subdir_location, regex_img, min_bottom_height = 25, cutoff_width=300, cutoff_height=150):\n",
    "    \"\"\"Should only segment metadata. Can be adjusted to include ionogram. \"\"\"\n",
    "    regex_raw_image =  SD_PATH + (\"/*\")\n",
    "    #print (\"the raw images path is:\", regex_raw_image)\n",
    "    list_images = glob.glob(regex_raw_image)\n",
    "    \n",
    "    #Dataframe is processing\n",
    "    df_img = pd.DataFrame(data = {\"file_name\": list_images})\n",
    "    #Read each image in a 2D UTF-8 grayscale array\n",
    "    df_img[\"raw\"] = df_img['file_name'].map(lambda file_name: cv2.imread(file_name, 0))\n",
    "\n",
    "    # Extract ionogram and coordinates delimiting its limits\n",
    "    df_img['limits']= list(zip(df_img['raw'].map(lambda raw_img: extract_ionogram(raw_img)))) \n",
    "    # Record the files whose ionogram extraction was not successful\n",
    "    df_loss_ion_extraction, loss_ion_extraction = record_loss(df_img,'image_segmentation.extract_ionogram_from_scan.extract_ionogram',subdir_location)\n",
    "    df_img = df_img[~loss_ion_extraction]\n",
    "    #df_img['height'],df_img['width'] = list(zip(df_img['ionogram'].map(lambda array_pixels: array_pixels.shape)))\n",
    "    \n",
    "    #Raw metadata\n",
    "    df_tmp = (df_img.apply(lambda row: extract_metadata(row['raw'], row['limits']), axis = 1, result_type = 'expand'))\n",
    "    df_img = df_img.assign(metadata_type = df_tmp[0])\n",
    "    df_img = df_img.assign(raw_metadata = df_tmp[1])\n",
    "    #extract_metadata is function from extract_metadata_from_scan\n",
    "    \n",
    "    # There should be no metadata on left and top, especially after flipping\n",
    "    outlier_metadata_location = np.any([df_img['metadata_type'] == 'right',df_img['metadata_type']=='top', df_img['metadata_type'] == 'left'], axis=0)\n",
    "    df_outlier_metadata_location ,_ =  record_loss(df_img,'image_segmentation.segment_images_in_subdir.segment_images: metadata not on left or bottom',subdir_location,\n",
    "                                         ['file_name','metadata_type'],outlier_metadata_location )\n",
    "    \n",
    "    if not df_outlier_metadata_location.empty:\n",
    "        df_outlier_metadata_location['details'] = df_outlier_metadata_location.apply(lambda row: str(row['metadata_type']),1)\n",
    "        df_outlier_metadata_location = df_outlier_metadata_location[['file_name','func_name','subdir_name','details']]\n",
    "    else:\n",
    "        df_outlier_metadata_location = df_outlier_metadata_location[['file_name','func_name','subdir_name']]\n",
    "    \n",
    "    # Remove loss from detected metadata not being on the left or bottom\n",
    "    df_img = df_img[~outlier_metadata_location]\n",
    "\n",
    "    #Trimmed metadata\n",
    "    #df_img['trimmed_metadata'] = list(zip(*df_img.apply(lambda row: trimming_metadata(row['raw_metadata'], row['metadata_type']), axis = 1, result_type = 'expand')))\n",
    "    df_trim_tmp = (df_img.apply(lambda row: trimming_metadata(row[\"raw_metadata\"], row['metadata_type']), axis = 1))\n",
    "    df_img = df_img.assign(trimmed_metadata = df_trim_tmp)\n",
    "    df_loss_trim, loss_trim = record_loss(df_img, 'image_segmentation.trim_raw_metadata.ntrimming_metadata', subdir_location)\n",
    "    #trimming_metadata is a function from trim_raw_metadata\n",
    "    #record_loss is a function from helper_functions\n",
    "    \n",
    "    df_img = df_img[~loss_trim]\n",
    "\n",
    "    # Check if metadata too small\n",
    "    #df_map_tmp = df_img['trimmed_metadata'].map(lambda array_pixels: np.shape(array_pixels), axis = 1)\n",
    "    df_map_tmp = df_img.apply(lambda row: np.shape(row['trimmed_metadata']), axis = 1, result_type = 'expand')\n",
    "    print (df_map_tmp)\n",
    "    df_img = df_img.assign(meta_height = df_map_tmp[0])\n",
    "    df_img = df_img.assign(meta_width = df_map_tmp[1])\n",
    "\n",
    "    outlier_size_metadata = (np.logical_and(df_img['metadata_type'] == 'bottom', df_img['meta_height'] < min_bottom_height))    \n",
    "        \n",
    "    df_outlier_metadata_size, _ = record_loss(df_img,'image_segmentation.segment_images_in_subdir.segment_images: metadata size outlier',subdir_location,\n",
    "                                           ['file_name','metadata_type','meta_height','meta_width'],outlier_size_metadata)\n",
    "\n",
    "    if not df_outlier_metadata_size.empty:\n",
    "        df_outlier_metadata_size['details'] = df_outlier_metadata_size.apply(lambda row: row['metadata_type'] + '_height: ' + \\\n",
    "                                                    str(row['meta_height'])+',width: ' + str(row['meta_width']),1)\n",
    "        df_outlier_metadata_size = df_outlier_metadata_size[['file_name','func_name','subdir_name','details']]\n",
    "        \n",
    "    else:\n",
    "        df_outlier_metadata_size = df_outlier_metadata_size[['file_name','func_name','subdir_name']]\n",
    "    \n",
    "    # Remove files whose metadata too small\n",
    "    df_img = df_img[~outlier_size_metadata]\n",
    "    \n",
    "    # Dataframe recording loss from programming errors\n",
    "    df_loss = pd.concat([df_loss_ion_extraction, df_loss_trim])\n",
    "    \n",
    "    # Dataframe recording loss from various filters i.e. metadata too small, ionogram too small/big\n",
    "    df_outlier = pd.concat([df_outlier_metadata_location, df_outlier_metadata_size])\n",
    "\n",
    "    return df_img,  df_loss, df_outlier\n",
    "# function can also return df_loss, df_outlier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1\n",
      "0   43  700\n",
      "1   44  700\n",
      "2   44  700\n",
      "3   43  700\n",
      "4   44  700\n",
      "5   44  700\n",
      "6   44  700\n",
      "7   44  700\n",
      "8   44  700\n",
      "9   44  700\n",
      "10  43  700\n",
      "11  43  700\n",
      "12  44  700\n",
      "13  42  322\n",
      "17  44  700\n",
      "18  45  700\n",
      "19  45  700\n",
      "20  44  700\n",
      "21  45  700\n",
      "22  44  700\n",
      "23  44  700\n",
      "24  44  700\n",
      "25  45  700\n",
      "26  45  700\n",
      "27  45  700\n",
      "28  45  700\n",
      "29  44  700\n",
      "(                                            file_name  \\\n",
      "0   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "1   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "2   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "4   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "5   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "6   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "7   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "9   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "13  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "17  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "18  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "19  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "21  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "25  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "26  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "27  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "28  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "\n",
      "                                                  raw                  limits  \\\n",
      "0   [[121, 111, 110, 108, 99, 83, 85, 80, 76, 66, ...   ([16, 989, 18, 344],)   \n",
      "1   [[1, 4, 3, 3, 5, 5, 3, 10, 7, 4, 13, 8, 3, 2, ...  ([16, 1095, 18, 343],)   \n",
      "2   [[2, 1, 1, 3, 32, 2, 1, 1, 1, 0, 2, 1, 2, 3, 6...  ([16, 1116, 18, 343],)   \n",
      "4   [[0, 1, 1, 2, 6, 0, 2, 2, 0, 1, 2, 4, 4, 2, 7,...  ([16, 1116, 18, 343],)   \n",
      "5   [[2, 1, 5, 1, 6, 1, 1, 2, 4, 0, 6, 1, 4, 1, 4,...  ([16, 1108, 17, 343],)   \n",
      "6   [[2, 2, 4, 0, 4, 2, 5, 0, 1, 3, 4, 1, 0, 2, 2,...  ([16, 1107, 17, 343],)   \n",
      "7   [[1, 3, 1, 12, 3, 4, 7, 5, 3, 2, 2, 2, 3, 1, 1...  ([16, 1106, 17, 343],)   \n",
      "9   [[1, 4, 1, 1, 1, 5, 2, 0, 5, 16, 2, 1, 5, 4, 0...  ([16, 1108, 17, 343],)   \n",
      "13  [[2, 2, 4, 1, 3, 0, 3, 2, 2, 2, 3, 6, 5, 10, 4...   ([23, 336, 16, 343],)   \n",
      "17  [[21, 13, 9, 8, 13, 6, 10, 11, 5, 12, 7, 12, 1...  ([16, 1645, 16, 341],)   \n",
      "18  [[0, 2, 0, 0, 1, 6, 2, 1, 1, 0, 2, 2, 1, 0, 0,...  ([17, 1105, 15, 340],)   \n",
      "19  [[1, 0, 2, 1, 1, 3, 0, 2, 1, 0, 1, 1, 2, 5, 13...  ([16, 1106, 15, 340],)   \n",
      "21  [[3, 0, 2, 0, 0, 1, 0, 1, 1, 1, 1, 1, 2, 1, 2,...  ([16, 1106, 15, 340],)   \n",
      "25  [[1, 0, 6, 1, 0, 4, 1, 2, 2, 3, 3, 0, 1, 2, 1,...  ([16, 1111, 14, 340],)   \n",
      "26  [[2, 0, 3, 1, 10, 0, 0, 1, 2, 1, 4, 1, 1, 0, 2...  ([16, 1110, 14, 340],)   \n",
      "27  [[1, 4, 3, 0, 1, 0, 1, 1, 0, 2, 4, 2, 0, 6, 16...  ([16, 1111, 14, 340],)   \n",
      "28  [[3, 0, 1, 2, 1, 2, 4, 0, 1, 0, 1, 1, 1, 0, 0,...  ([16, 1111, 14, 340],)   \n",
      "\n",
      "   metadata_type                                       raw_metadata  \\\n",
      "0         bottom  [[223, 219, 215, 215, 212, 215, 213, 210, 212,...   \n",
      "1         bottom  [[218, 198, 204, 207, 221, 206, 203, 205, 214,...   \n",
      "2         bottom  [[205, 208, 214, 220, 205, 200, 207, 215, 48, ...   \n",
      "4         bottom  [[218, 221, 198, 201, 215, 222, 211, 207, 220,...   \n",
      "5         bottom  [[195, 221, 215, 205, 210, 216, 218, 207, 206,...   \n",
      "6         bottom  [[192, 184, 217, 203, 206, 211, 225, 219, 207,...   \n",
      "7         bottom  [[42, 134, 207, 208, 141, 169, 189, 201, 215, ...   \n",
      "9         bottom  [[216, 66, 28, 74, 216, 108, 104, 162, 209, 20...   \n",
      "13        bottom  [[5, 10, 13, 8, 7, 19, 12, 15, 18, 22, 24, 25,...   \n",
      "17        bottom  [[238, 231, 182, 210, 235, 235, 235, 231, 232,...   \n",
      "18        bottom  [[205, 199, 18, 34, 108, 212, 218, 206, 115, 6...   \n",
      "19        bottom  [[210, 200, 120, 26, 51, 168, 209, 207, 217, 8...   \n",
      "21        bottom  [[206, 201, 174, 34, 36, 146, 201, 203, 112, 1...   \n",
      "25        bottom  [[193, 206, 220, 211, 198, 202, 200, 52, 52, 1...   \n",
      "26        bottom  [[201, 197, 208, 47, 39, 111, 207, 220, 206, 1...   \n",
      "27        bottom  [[35, 58, 156, 206, 208, 222, 200, 191, 209, 2...   \n",
      "28        bottom  [[208, 205, 220, 214, 208, 208, 219, 225, 212,...   \n",
      "\n",
      "                                     trimmed_metadata  meta_height  \\\n",
      "0   [[255, 255, 255, 255, 255, 255, 255, 255, 255,...           43   \n",
      "1   [[255, 255, 255, 255, 255, 255, 255, 255, 255,...           44   \n",
      "2   [[0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 255...           44   \n",
      "4   [[255, 255, 255, 255, 255, 255, 255, 255, 255,...           44   \n",
      "5   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...           44   \n",
      "6   [[0, 0, 0, 0, 0, 255, 255, 255, 255, 255, 255,...           44   \n",
      "7   [[255, 255, 255, 255, 255, 255, 255, 255, 255,...           44   \n",
      "9   [[255, 255, 255, 255, 255, 255, 255, 255, 255,...           44   \n",
      "13  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...           42   \n",
      "17  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...           44   \n",
      "18  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...           45   \n",
      "19  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...           45   \n",
      "21  [[0, 0, 255, 255, 255, 255, 255, 255, 255, 255...           45   \n",
      "25  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...           45   \n",
      "26  [[0, 0, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0...           45   \n",
      "27  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...           45   \n",
      "28  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...           45   \n",
      "\n",
      "    meta_width                                   dilated_metadata  \\\n",
      "0          700  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
      "1          700  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
      "2          700  [[0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 255...   \n",
      "4          700  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
      "5          700  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
      "6          700  [[0, 0, 0, 0, 0, 255, 255, 255, 255, 255, 255,...   \n",
      "7          700  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
      "9          700  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
      "13         322  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
      "17         700  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
      "18         700  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
      "19         700  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
      "21         700  [[0, 0, 255, 255, 255, 255, 255, 255, 255, 255...   \n",
      "25         700  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
      "26         700  [[0, 0, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0...   \n",
      "27         700  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
      "28         700  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
      "\n",
      "                                          x_centroids  \\\n",
      "0             [15.334975369458128, 164.0722891566265]   \n",
      "1                   [23.0, 71.0, 114.0, 591.0, 667.0]   \n",
      "2   [29.5, 77.5, 120.0, 161.0, 448.5, 503.0, 597.0...   \n",
      "4            [16.5, 58.0, 102.0, 141.0, 592.0, 663.5]   \n",
      "5                                      [578.5, 659.5]   \n",
      "6                         [20.5, 119.5, 605.0, 670.5]   \n",
      "7                   [16.0, 58.0, 101.5, 589.5, 664.0]   \n",
      "9            [17.0, 59.0, 103.0, 143.0, 593.0, 665.0]   \n",
      "13                                            [198.0]   \n",
      "17           [20.995192307692307, 186.72222222222223]   \n",
      "18                       [105.5, 140.0, 176.5, 663.5]   \n",
      "19    [16.0, 57.5, 100.5, 142.0, 178.5, 214.0, 663.5]   \n",
      "21  [399.5, 450.5, 492.0, 533.5, 570.5, 605.5, 644.0]   \n",
      "25                 [40.5, 476.0, 513.5, 549.5, 679.0]   \n",
      "26                [142.0, 179.5, 215.5, 598.0, 665.0]   \n",
      "27                               [63.5, 564.0, 600.5]   \n",
      "28                                     [574.5, 674.0]   \n",
      "\n",
      "                                 y_centroids  \\\n",
      "0   [2.4088669950738915, 1.0843373493975903]   \n",
      "1                  [0.5, 0.5, 0.5, 0.5, 0.5]   \n",
      "2   [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]   \n",
      "4             [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]   \n",
      "5                                 [0.5, 0.5]   \n",
      "6                       [0.5, 0.5, 0.5, 0.5]   \n",
      "7                  [0.5, 0.5, 0.5, 0.5, 0.5]   \n",
      "9             [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]   \n",
      "13                                     [0.5]   \n",
      "17  [1.7355769230769231, 0.6666666666666666]   \n",
      "18                      [0.5, 0.5, 0.5, 0.5]   \n",
      "19       [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]   \n",
      "21       [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]   \n",
      "25                 [0.5, 0.5, 0.5, 0.5, 0.5]   \n",
      "26                 [0.5, 0.5, 0.5, 0.5, 0.5]   \n",
      "27                           [0.5, 0.5, 0.5]   \n",
      "28                                [0.5, 0.5]   \n",
      "\n",
      "                                        dict_metadata  \n",
      "0   {'Operating Mode 1': [1], 'Station Number 2': ...  \n",
      "1   {'Operating Mode 1': [1], 'Operating Mode 2': ...  \n",
      "2   {'Operating Mode 1': [1], 'Operating Mode 2': ...  \n",
      "4   {'Operating Mode 1': [1], 'Operating Mode 2': ...  \n",
      "5                        {'Min 1': [1], 'Min 2': [1]}  \n",
      "6   {'Operating Mode 1': [1], 'Station Number 1': ...  \n",
      "7   {'Operating Mode 1': [1], 'Operating Mode 2': ...  \n",
      "9   {'Operating Mode 1': [1], 'Operating Mode 2': ...  \n",
      "13                          {'Station Number 2': [1]}  \n",
      "17  {'Operating Mode 1': [1], 'Station Number 2': ...  \n",
      "18  {'Station Number 1': [1, 1], 'Station Number 2...  \n",
      "19  {'Operating Mode 1': [1], 'Operating Mode 2': ...  \n",
      "21  {'Day 2': [1], 'Day 3': [1], 'Hour 1': [1], 'H...  \n",
      "25  {'Operating Mode 1': [1], 'Hour 1': [1, 1], 'H...  \n",
      "26  {'Station Number 1': [1], 'Station Number 2': ...  \n",
      "27  {'Operating Mode 2': [1], 'Hour 2': [1], 'Min ...  \n",
      "28                      {'Hour 2': [1], 'Min 2': [1]}  ,                                            file_name  \\\n",
      "0  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "1  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "2  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "3  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "4  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "5  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "6  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "7  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "8  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "9  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "\n",
      "                                           func_name  \\\n",
      "0  metadata_translation.determine_metadata_grid_m...   \n",
      "1  metadata_translation.determine_metadata_grid_m...   \n",
      "2  metadata_translation.determine_metadata_grid_m...   \n",
      "3  metadata_translation.determine_metadata_grid_m...   \n",
      "4  metadata_translation.determine_metadata_grid_m...   \n",
      "5  metadata_translation.determine_metadata_grid_m...   \n",
      "6  metadata_translation.determine_metadata_grid_m...   \n",
      "7  metadata_translation.determine_metadata_grid_m...   \n",
      "8  metadata_translation.determine_metadata_grid_m...   \n",
      "9  metadata_translation.determine_metadata_grid_m...   \n",
      "\n",
      "                                         subdir_name  \n",
      "0  C:/Users/spunchiwickrama/Documents/Projects/IS...  \n",
      "1  C:/Users/spunchiwickrama/Documents/Projects/IS...  \n",
      "2  C:/Users/spunchiwickrama/Documents/Projects/IS...  \n",
      "3  C:/Users/spunchiwickrama/Documents/Projects/IS...  \n",
      "4  C:/Users/spunchiwickrama/Documents/Projects/IS...  \n",
      "5  C:/Users/spunchiwickrama/Documents/Projects/IS...  \n",
      "6  C:/Users/spunchiwickrama/Documents/Projects/IS...  \n",
      "7  C:/Users/spunchiwickrama/Documents/Projects/IS...  \n",
      "8  C:/Users/spunchiwickrama/Documents/Projects/IS...  \n",
      "9  C:/Users/spunchiwickrama/Documents/Projects/IS...  ,                                             file_name  \\\n",
      "14  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "15  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "16  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "\n",
      "                                            func_name  \\\n",
      "14  image_segmentation.segment_images_in_subdir.se...   \n",
      "15  image_segmentation.segment_images_in_subdir.se...   \n",
      "16  image_segmentation.segment_images_in_subdir.se...   \n",
      "\n",
      "                                          subdir_name details  \n",
      "14  C:/Users/spunchiwickrama/Documents/Projects/IS...     top  \n",
      "15  C:/Users/spunchiwickrama/Documents/Projects/IS...     top  \n",
      "16  C:/Users/spunchiwickrama/Documents/Projects/IS...   right  )\n"
     ]
    }
   ],
   "source": [
    "#From scan2data > metadata_translation > translate_leftside_metadata.py\n",
    "def get_bottomside_metadata (df_img, subdir_location, kernal_size =(1, 1)):\n",
    "    \"\"\"Reads the metadata\"\"\"\n",
    "\n",
    "    kernel_dilation = np.ones(kernal_size, np.uint8)\n",
    "\n",
    "    #df_dilate_tmp = df_img.apply(lambda trimmed_meta: cv2.dilate(trimmed_meta, kernel_dilation))\n",
    "    df_dilate_tmp = df_img['trimmed_metadata'].map(lambda trimmed_meta: cv2.dilate(trimmed_meta, kernel_dilation))\n",
    "    df_img = df_img.assign(dilated_metadata = df_dilate_tmp)\n",
    "\n",
    "\n",
    "    #df_img['x_centroids'], df_img['y_centroids'], df_img['is_dot'] = zip(*df_img.apply(lambda row: extract_centroids(row['dilated_metadata'], row['file_name']), 1))\n",
    "    df_cent_tmp = df_img.apply(lambda row: extract_centroids(row['dilated_metadata'], row['file_name']), axis = 1, result_type = 'expand')\n",
    "    #df_cent_tmp.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/output-cent-tmp.csv\")\n",
    "    df_img = df_img.assign(x_centroids = df_cent_tmp[0])\n",
    "    df_img = df_img.assign(y_centroids = df_cent_tmp[1])\n",
    "\n",
    "    df_loss_centroids_extraction, loss_centroids_extraction = record_loss(df_img,'metadata_translation.determine_metadata_grid_mapping.extract_centroids_',subdir_location)\n",
    "    # extract_centroids and record_loss are two other functions \n",
    "\n",
    "    # Remove files where the extraction didn't work\n",
    "    df_img = df_img[~loss_centroids_extraction]\n",
    "    # ^removes them from the main dataframe\n",
    "    \n",
    "    #df_num_subset = df_img[np.invert(np.array(df_img['is_dot']))]\n",
    "    list_x_digit, list_y_digit = [0], [0]\n",
    "\n",
    "    list_x_digit = list(chain(*df_img['x_centroids'].tolist()))\n",
    "    list_y_digit = list(chain(*df_img['y_centroids'].tolist()))\n",
    "    dict_mapping, dict_hist = get_leftside_metadata_grid_mapping(list_x_digit, list_y_digit, subdir_location)\n",
    "\n",
    "\n",
    "    # Determine the value of metadata based on the mappings\n",
    "    #print (dict_mapping)\n",
    "    df_dict_meta = df_img.apply(lambda row: map_coord_to_metadata(row['x_centroids'], row['y_centroids'],\n",
    "                                                                      dict_mapping['dict_cat_digit'],\n",
    "                                                                      dict_mapping['dict_num_digit']), axis = 1)\n",
    "    df_img = df_img.assign(dict_metadata = df_dict_meta)\n",
    "    #df_img.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/01-mapping-to-output.csv\")\n",
    "    \n",
    "    df_loss_mapping, loss_mapping = record_loss(df_img,'map_coord_to_metadata', subdir_location)\n",
    "    df_img = df_img[~loss_mapping]\n",
    "    \n",
    "    \n",
    "    df_loss = pd.concat([df_loss_centroids_extraction, df_loss_mapping],ignore_index=True)\n",
    "\n",
    "    return df_img, df_loss, dict_mapping, dict_hist\n",
    "\n",
    "#From process_directory.py\n",
    "def process_subdir(subdir_path, regex_images):\n",
    "    \"\"\"Transform raw scanned images in subdir into information\"\"\"\n",
    "\n",
    "    #Run segment_images on subdir\n",
    "    df_img, df_loss, df_outlier = segment_metadata(subdir_path, regex_images)\n",
    "    #df_img.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/segment-output.csv\")\n",
    "\n",
    "    #Translate metadata on bottom\n",
    "    #df_img_bottom =  df_img.loc[df_img['metadata_type'] == 'bottom']\n",
    "    df_img_bottom, df_loss_meta_bottom, _, __ = get_bottomside_metadata(df_img, subdir_path) #from metadata_translation.translate_bottomside_metadata\n",
    "    df_img_bottom.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/bottomside-output.csv\")\n",
    "    df_loss = pd.concat([df_loss_meta_bottom], ignore_index=True)\n",
    "\n",
    "    df_processed = df_img_bottom\n",
    "    #df_loss = pd.concat([df_loss_coord_bottom], ignore_index=True)\n",
    "    return df_processed, df_loss, df_outlier\n",
    "\n",
    "print (process_subdir(SD_PATH, '/*'))\n",
    "\n",
    "# Currently not being used\n",
    "def process_df_bottomside_metadata(df_processed, subdir_name, source_dir):\n",
    "\n",
    "    df_final_data = df_processed[['file_name', 'dict_metadata']]\n",
    "    df_final_data['subdir_name'] = subdir_name\n",
    "    labels = ['Operating Mode 1','Operating Mode 2','Station Number 1', 'Station Number 2', 'Year 1', 'Year 2', 'Day 1', 'Day 2', 'Day 3'\n",
    "                    'Hour 1', 'Hour 2', 'Min 1', 'Min 2', 'Sec 1', 'Sec 2']\n",
    "\n",
    "    for label in labels:\n",
    "        df_final_data[label] = df_final_data['dict_metadata'].map(\n",
    "            lambda dict_meta: sum(dict_meta[label]) if label in dict_meta.keys() else 0)\n",
    "\n",
    "    del df_final_data['dict_metadata']\n",
    "\n",
    "    #df_final_data['year'] = df_final_data['year'] + 1900\n",
    "    #df_final_data['day'] = df_final_data['day_1'].astype(str) + df_final_data['day_2'].astype(str) + df_final_data['day_3'].astype(str) \n",
    "    #df_final_data['hour'] = df_final_data['hour_1'].astype(str) + df_final_data['hour_2'].astype(str) \n",
    "    #df_final_data['minute'] = df_final_data['minute_1'].astype(str) + df_final_data['minute_2'].astype(str) \n",
    "    #df_final_data['second'] = df_final_data['second_1'].astype(str) + df_final_data['second_2'].astype(str) \n",
    "    #df_final_data['station_number'] = df_final_data['station_number_1'].astype(str) + df_final_data['station_number_2'].astype(str) \n",
    "    #df_final_data['day'] = df_final_data['day'].astype(int)\n",
    "    #df_final_data['hour'] = df_final_data['hour'].astype(int)\n",
    "    #df_final_data['minute'] = df_final_data['minute'].astype(int)\n",
    "    #df_final_data['second'] = df_final_data['second'].astype(int)\n",
    "    #df_final_data['station_number'] = df_final_data['station_number'].astype(int)\n",
    "\n",
    "    df_final_data['Operating_Mode'] = df_final_data[\"Operating Mode 1\"].astype(str) + df_final_data[\"Operating Mode 2\"].astype(str) \n",
    "    df_final_data[\"Station_Number\"] = df_final_data['Station Number 1'].astype(str) + df_final_data['Station Number 2'].astype(str)\n",
    "    df_final_data['Year'] = (df_final_data['Year 1'].astype(str) + df_final_data['Year 2'].astype(str)).astype(int) + 1900\n",
    "    df_final_data[\"Day\"] = (df_final_data['Day 1'].astype(str) + df_final_data['Day 2'].astype(str) + df_final_data['Day 3'].astype(str)).astype(int)\n",
    "    df_final_data[\"Hour\"] = (df_final_data['Hour 1'].astype(str) + df_final_data['Hour 2'].astype(str)).astype(int)\n",
    "    df_final_data[\"Min\"] = (df_final_data['Min 1'].astype(str) + df_final_data['Min 2'].astype(str)).astype(int)\n",
    "    df_final_data[\"Sec\"] = (df_final_data['Sec 1'].astype(str) + df_final_data['Sec 2'].astype(str)).astype(int)\n",
    "\n",
    "\n",
    "    #if len(df_final_data) > 0:          \n",
    "        #code_list_of_station_after1965 = pd.read_csv(source_dir + 'Post_July_1_1965_Code_List_Station.csv')\n",
    "        #code_list_of_station_before1963 = pd.read_csv(source_dir + 'Pre_1963_Code_List_Station.csv')\n",
    "        #code_list_of_station_between1963_1964 = pd.read_csv(source_dir + '1963_1964.csv')\n",
    "        #df_result_after1965 = pd.merge(df_final_data.loc[df_final_data['year'] >= 1965], \n",
    "                                       #code_list_of_station_after1965, on='station_number')\n",
    "        #df_result_before1963 = pd.merge(df_final_data.loc[df_final_data['year'] <= 1963],\n",
    "                                        #code_list_of_station_before1963, on='station_number')\n",
    "        #df_result_mid1964 = pd.merge (df_final_data.loc[df_final_data['year'] == 1964],\n",
    "                                        #code_list_of_station_between1963_1964, on = 'station_number')\n",
    "        #df_final_result = pd.concat([df_result_before1963, df_result_mid1964, df_result_after1965]).reset_index(drop=True)\n",
    "        #df_final_result = df_result_before1963.append(df_result_after1965.append(df_result_mid1964, ignore_index=True)) #Why was pd.concat not used?        \n",
    "    #else:\n",
    "        #df_final_result = pd.DataFrame()\n",
    "    \n",
    "    '''if len(df_final_data['year']) != 0:  # and df_final_data['year'] >= 1965:\n",
    "        code_list_of_station = pd.read_csv(source_dir+'Post_July_1_1965_Code_List_Station.csv')\n",
    "    #else:\n",
    "    #    code_list_of_station = pd.read_csv(source_dir + 'Pre_July_1_1965_Code_List_Station.csv')\n",
    "    df_final_result = pd.merge(df_final_data,code_list_of_station, on='station_number')\n",
    "    code_list_of_station_after1965 = pd.read_csv(source_dir + 'Post_July_1_1965_Code_List_Station.csv')\n",
    "    code_list_of_station_before1963 = pd.read_csv(source_dir + 'Pre_1963_Code_List_Station.csv')\n",
    "    code_list_of_station_between1963_1964 = pd.read_csv(source_dir + '1963_1964.csv')\n",
    "    df_result_after1965 = pd.merge(df_final_data.loc[df_final_data['year'] >= 1965], code_list_of_station_after1965,\n",
    "                                   on='station_number')\n",
    "    df_result_before1963 = pd.merge(df_final_data.loc[df_final_data['year'] <= 1963],\n",
    "                                    code_list_of_station_before1963, on='station_number')\n",
    "    df_result_mid1964 = pd.merge (df_final_data.loc[df_final_data['year'] == 1964],\n",
    "                                    code_list_of_station_between1963_1964, on = 'station_number')\n",
    "    df_final_result = df_result_before1963.append(df_result_after1965.append(df_result_mid1964, ignore_index=True))'''\n",
    "    \n",
    "    return df_final_data\n",
    "\n",
    "source_dir = \"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/\"\n",
    "#print (process_df_bottomside_metadata(process_subdir(SD_PATH, '/*')[0], SD_PATH, source_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonV31013",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
