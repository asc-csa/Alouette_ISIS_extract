{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISIS METADATA GRIDDING - Test 02\n",
    "working with Roksana's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import ntpath\n",
    "from scipy.signal import find_peaks\n",
    "import traceback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subdir path: C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/Output-Test-Images\n"
     ]
    }
   ],
   "source": [
    "#Sub-directory path\n",
    "#SD_PATH = gen_ran_subdir(\"L:/DATA/ISIS/ISIS_101300030772/b*/B1*\")\n",
    "\n",
    "#Image path\n",
    "#I_PATH = gen_ran_img(SD_PATH, '/*')\n",
    "\n",
    "#For testing - only use same sub-directory (subdir)\n",
    "SD_PATH = os.path.realpath(\"Output-Test-Images\")\n",
    "#Folder containing 30 images for testing (very temporary)\n",
    "#I_PATH = gen_ran_img(SD_PATH, '/*')\n",
    "print (\"Subdir path:\", SD_PATH)\n",
    "#print (\"Random image in subdir path\", I_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DICTIONARIES\n",
    "# Labelling of coordinates\n",
    "LABELS_NUM = [1,2,3, 4, 5, 6, 7, 8, 9]\n",
    "#LABELS_NUM = [1,2, 4, 8]\n",
    "#LABELS_CAT_DOT = ['day_1','day_2','day_3','hour_1','hour_2','minute_1','minute_2','second_1', 'second_2','station_code']\n",
    "#LABELS_CAT_DIGIT = ['satellite_number','year','day_1','day_2','day_3','hour_1','hour_2','minute_1','minute_2',\n",
    "                    #'second_1', 'second_2', 'station_number_1','station_number_2']\n",
    "LABELS_CAT_NUM = ['Operating Mode 1','Operating Mode 2','Station Number 1', 'Station Number 2', 'Year 1', 'Year 2', 'Day 1', 'Day 2', 'Day 3',\n",
    "                    'Hour 1', 'Hour 2', 'Min 1', 'Min 2', 'Sec 1', 'Sec 2']\n",
    "LABELS_DICT = ['dict_cat_digit','dict_num_digit']#'dict_cat_dot','dict_num_dot',]\n",
    "\n",
    "#Defaults for dictionary mappings of coordinates to labels\n",
    "DEFAULT_DICT_CAT_DIGIT = (53,21,661) #mean_dist_default,first_peak_default,last_peak_default\n",
    "DEFAULT_DICT_NUM_DIGIT = (47,41,20) #mean_dist_default,first_peak_default,dist_btw_peaks for peak detection\n",
    "\n",
    "#DEFAULT_DICT_CAT_DIGIT_F = (43,23,540) #mean_dist_default,first_peak_default,last_peak_default for those in LIST_DIRECTORY_DOTS \n",
    "#DEFAULT_DICT_NUM_DIGIT_F = (40,37,20) #mean_dist_default,first_peak_default,dist_btw_peaks for peak detection for those in LIST_DIRECTORY_DOTS \n",
    "\n",
    "#DEFAULT_DICT_CAT_DOT = (59,20,549)##mean_dist_default,first_peak_default,last_peak_default\n",
    "#DEFAULT_DICT_NUM_DOT = (15,32,10) #mean_dist_default,first_peak_default,dist_btw_peaks for peak detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From helper_functions.py\n",
    "def record_loss (df, function_name, subdir_location, columns_to_extract = None, loss_extraction = None):\n",
    "    \"\"\"Create dataframe that records loss.\"\"\"\n",
    "\n",
    "    if columns_to_extract is None:\n",
    "        columns_to_extract = ['file_name']\n",
    "    if loss_extraction is None:\n",
    "        loss_extraction = []\n",
    "    if len(loss_extraction) == 0:\n",
    "        # function should return NA if there an error\n",
    "        loss_extraction = df.isna().any(axis=1)\n",
    "    # Record the files whose extraction was not successful\n",
    "    df_loss_extraction = df[loss_extraction].copy()\n",
    "    df_loss_extraction = df_loss_extraction[columns_to_extract]\n",
    "    df_loss_extraction['func_name'] = function_name\n",
    "    df_loss_extraction[ 'subdir_name'] = subdir_location\n",
    "\n",
    "    return df_loss_extraction, loss_extraction\n",
    "\n",
    "# From scan2data > image_segmentation > trim_raw_metadata > connected_components_metadata_location\n",
    "def metadata_location(meta, min_count = 20, max_count = 2000):\n",
    "    \"\"\"\"Use connected component algorithm to find the location of the metadata\n",
    "    :param meta: binarized UTF-8 2D array of values (0 or 1) array containing metadata\n",
    "    :type meta: class: `numpy.ndarray`\n",
    "    :param min_count: minimum number of pixels to be considered metadata dot/num, defaults to 50\n",
    "    :type min_count: int, optional\n",
    "    :param max_count: maximum number of pixels to be considered metadata dot/num, defaults to 1000\n",
    "    :type max_count: int, optional\n",
    "    :return: metadata labelled by the connected component algorithm ie UTF-8 2D array of values where each value correspond to belonging to a metadata group\n",
    "    :rtype: class: `numpy.ndarray`\n",
    "    \"\"\"\n",
    "    \n",
    "    #run algorithm on metadata section\n",
    "    blurred = cv2.GaussianBlur(meta, (3,3), 0)\n",
    "    _, labelled = cv2.connectedComponents(blurred)\n",
    "\n",
    "    #Dictionary of label:counts\n",
    "    unique, counts = np.unique(labelled, return_counts = True)\n",
    "    dict_components = dict(zip(unique, counts))\n",
    "\n",
    "    #Remove outliers // Remove pixels not part of metadata\n",
    "    dict_subset = {}\n",
    "    dict_outlier = {}\n",
    "    for k,v, in dict_components.items():\n",
    "        if v > min_count and v < max_count:\n",
    "            dict_subset[k] = v\n",
    "        else:\n",
    "            dict_outlier[k] = v\n",
    "    \n",
    "    if key_list_to_remove := list(dict_outlier.keys()):\n",
    "        for k in key_list_to_remove:\n",
    "            labelled[labelled == k] = 0\n",
    "\n",
    "    return labelled\n",
    "\n",
    "#From scan2data > image_segmentation > trim_raw_metadata\n",
    "#test provided values and change if needed\n",
    "def bottomside_metadata_trimming(connected_meta, opened_meta,\n",
    "                                 h_window = 100, w_window = 600, starting_y = 10, starting_x = 25, step_size = 20, trim_if_small = 25):\n",
    "    \"\"\"Sliding window method to locate and trim bottomside metadata\n",
    "    :param connected_meta: metadata labelled by the connected component algorithm ie UTF-8 2D array of values where each value correspond to belonging to a metadata group\n",
    "    :type connected_meta: class: `numpy.ndarray`\n",
    "    :param opened_meta:  UTF-8 2D array of values representing raw metadata after morphological operations including opening\n",
    "    :type opened_meta: nclass: `numpy.ndarray`\n",
    "    :param h_window: height of sliding window, defaults to 100\n",
    "    :type h_window: int, optional\n",
    "    :param w_window: width of sliding window, defaults to 700\n",
    "    :type w_window: int, optional\n",
    "    :param starting_y: by how many pixels from the top to start windowing process, defaults to 0\n",
    "    :type starting_y: int, optional\n",
    "    :param starting_x: by how many pixels from the left to start the windowing process, defaults to 15\n",
    "    :type starting_x: int, optional\n",
    "    :param step_size: by how much sliding window moves to the right and/or bottom, defaults to 10\n",
    "    :type step_size: int, optional\n",
    "    :param trim_if_small: by how many pixels to trim metadata's height or width if they are smaller than h_window or w_window,defaults to 11\n",
    "    :type trim_if_small: int, optional\n",
    "    :return: trimmed metadata i.e.  trimmed UTF-8 2D array of values containing metadata (window with highest mean area)\n",
    "    :rtype: class: `numpy.ndarray`\n",
    "    '''\"\"\"\n",
    "    \n",
    "    def sliding_window(image, starting_y, starting_x, h_window, w_window, step_size):\n",
    "        \"\"\"sliding window generator object\"\"\"\n",
    "        h_img, w_img = np.shape(image)\n",
    "        for y in range(starting_y, h_img - h_window, step_size):\n",
    "            for x in range(starting_x, w_img - w_window, step_size):\n",
    "                yield y,x,image[y:y + h_window, x:x + w_window]\n",
    "    \n",
    "    h_raw, w_raw = np.shape(opened_meta)\n",
    "    \n",
    "    if h_window + step_size  >= h_raw:\n",
    "        h_window = h_raw - trim_if_small\n",
    "    if w_window + step_size >= w_raw:\n",
    "        w_window = w_raw - trim_if_small\n",
    "    \n",
    "    s = sliding_window(connected_meta, starting_y, starting_x, h_window, w_window, step_size)\n",
    "    \n",
    "    max_window = connected_meta[starting_y:h_window+starting_y,\n",
    "                 starting_x:w_window+starting_x ]\n",
    "    max_mean = np.mean(max_window)\n",
    "    y_max= starting_y\n",
    "    x_max = starting_x\n",
    "    \n",
    "    for y, x, window in s:\n",
    "        tmp = window\n",
    "        mean = np.mean(tmp)\n",
    "        if mean > max_mean:\n",
    "            max_window = tmp\n",
    "            max_mean  = mean\n",
    "            y_max = y\n",
    "            x_max = x\n",
    "\n",
    "    trimmed_metadata =  opened_meta[y_max:y_max + h_window, x_max:x_max + w_window]\n",
    "    return trimmed_metadata\n",
    "\n",
    "#From scan2data > image_segmentation > trim_raw_metadata\n",
    "def trimming_metadata(raw_metadata, opening_kernal_size = (3,3), median_kernal_size = 5):\n",
    "    \"\"\"\"Trim the rectangle containing metadata to smallest workable area.\"\"\"\n",
    "\n",
    "    try:\n",
    "        #Filtering to reduce noise\n",
    "        #median_filtered_meta = cv2.medianBlur(raw_metadata, median_kernal_size)\n",
    "\n",
    "        #Opening operation: Eroision + Dilation\n",
    "        #kernal_opening = np.ones(opening_kernal_size, dtype = np.uint8)\n",
    "        #opened_meta = cv2.morphologyEx(raw_metadata, cv2.MORPH_OPEN, kernal_opening)\n",
    "\n",
    "        # Binarization\n",
    "        metadata_binary = cv2.threshold(raw_metadata, 127, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "        #Run connected components algorithm\n",
    "        connected_meta = metadata_location(metadata_binary)\n",
    "\n",
    "        trimmed_metadata = bottomside_metadata_trimming(connected_meta, metadata_binary)\n",
    "        #bottomside_metadata_trimming function is from same location\n",
    "\n",
    "        #Checking\n",
    "        #cv2.imshow(\"test\", trimmed_metadata)\n",
    "        #cv2.waitKey(0)\n",
    "        return (trimmed_metadata)\n",
    "    except:\n",
    "        return (np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From scan2data > metadata_translation> leftside_metadata_grid_mapping \n",
    "def extract_centroids(cut_metadata, file_name, min_pixels = 5, max_pixels = 3000, max_area_dot = 120):\n",
    "    \n",
    "    '''Extract the coordinates of the centroid of each metadata number using the connected component algorithm\n",
    "    \n",
    "    :param dilated_meta: trimmed metadata (output of image_segmentation.leftside_metadata_trimming) after a rotation and dilation morphological transformation (see translate_leftside_metadata.extract_leftside_metadata )\n",
    "    :type dilated_meta: class: `numpy.ndarray`\n",
    "    :param file_name: full path of starting raw image ex:G:/R014207929F/431/Image0399.png\n",
    "    :type file_name: str\n",
    "    :param min_num_pixels: minimum number of pixels to be considered metadata dot/num, defaults to 50\n",
    "    :type min_num_pixels: int, optional\n",
    "    :param max_number_pixels: maximum number of pixels to be considered metadata dot/num, defaults to 1000\n",
    "    :type max_number_pixels: int, optional\n",
    "    :param max_area_dot: maximum median area of a single metadata components to be considered a dot, defaults to 120\n",
    "    :type max_area_dot: int, optional\n",
    "    :returns: col_centroids,row_centroids,is_dot : list of col ('x') positions where metadata is detected,list of row ('y') positions where metadata is detected,whether the metadata is dots\n",
    "    :rtype: class: `list`,class: `list`, bool\n",
    "    :raises Exception: returns np.nan,np.nan,np.nan if there is an error\n",
    "    \n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        blurred = cv2.GaussianBlur(cut_metadata, (3,3), 0)\n",
    "        totalLabels, label_ids, stats, centroids = cv2.connectedComponentsWithStats(blurred)\n",
    "        area_centroids = stats[:,-1]\n",
    "\n",
    "        centroids_metadata = centroids[np.logical_and(area_centroids > min_pixels, area_centroids < max_pixels),:]\n",
    "        #^ consider adjusting min and max range\n",
    "\n",
    "        zip_centroids = list(zip(*centroids_metadata))\n",
    "        col_centroids = list((zip_centroids[0]))\n",
    "        row_centroids = list((zip_centroids[1]))\n",
    "\n",
    "        #Determine dot type (temp)\n",
    "        #area_centroids = area_centroids[np.logical_and(area_centroids > min_num_pixels, area_centroids < max_number_pixels)]\n",
    "        #median_area = np.median(area_centroids)\n",
    "        #The line below is commented to prevent giving the dot items manually\n",
    "        #if any([dir_dot in file_name for dir_dot in LIST_DIRECTORY_DOTS]) and median_area < max_area_dot:\n",
    "        #is_dot = median_area < max_area_dot\n",
    "\n",
    "        #### CHECK (TEMP) ####\n",
    "        #output = np.zeros(np.shape(cut_metadata), dtype = 'uint8')\n",
    "        #for i in range(1, totalLabels):\n",
    "            #area = stats[i, cv2.CC_STAT_AREA]\n",
    "            #if area > min_pixels and area < max_pixels:\n",
    "                #new_img = cut_metadata.copy()\n",
    "                #x1 = stats[i, cv2.CC_STAT_LEFT]\n",
    "                #y1 = stats[i, cv2.CC_STAT_TOP]\n",
    "                #h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "                #w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "\n",
    "                #pt1 = (x1, y1)\n",
    "                #pt2 = (x1 + w, y1 + h)\n",
    "                #(X, Y) = centroids[i]\n",
    "\n",
    "                #cv2.rectangle(new_img, pt1, pt2, (0, 0, 255), 3)\n",
    "                #cv2.circle(new_img, (int(X), int(Y)), 4, (0, 0, 255), -1)\n",
    "\n",
    "                #component = np.zeros(np.shape(cut_metadata), dtype= \"uint8\")\n",
    "                #componentMask = (label_ids == i).astype('uint8') *255\n",
    "\n",
    "                #component = cv2.bitwise_or(component, componentMask)\n",
    "                #output =cv2.bitwise_or(output, componentMask)\n",
    "\n",
    "                #cv2.imshow(\"numbers\", component)\n",
    "        #cv2.imshow(\"Mask\", output)\n",
    "        #cv2.waitKey(0)\n",
    "                #os.chdir(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/ISIS_Meeting\")\n",
    "                # Generate input and output file paths \n",
    "                #cv2.imwrite(file_name +\"Test\" + str(i) + \".png\", output)\n",
    "\n",
    "        return col_centroids, row_centroids\n",
    "    \n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "##From scan2data > metadata_translation > leftside_metdata_grid_mapping\n",
    "def indices_highest_bin(list_coords, nbins, peak_threshold, distance_bwtn_peaks):\n",
    "    \"\"\"\" returns indices of most common values using binning\n",
    "\n",
    "    :param list_coord: list of positions where metadata is detected\n",
    "    :type list_coord: class: `list`\n",
    "    :param nbins: number of bins used for binning operation, defaults to 500\n",
    "    :type nbins: int, optional\n",
    "    :param peak_prominence_threshold: the threshold to detect peaks that correspond to the peaks corresponding to the most common values, defaults to 0.2\n",
    "    :type peak_prominence_threshold: int, optional\n",
    "    :param distance_between_peaks: the minimum number of samples between subsequent peaks corresponding to the most common values, defaults to 30\n",
    "    :type distance_between_peaks: int, optional\n",
    "    :returns: select_peaks,bin_edges,counts i.e. array of the indices of  peaks corresponding to the most common values, array for the bin edges after calling np.histogram, array for counts of the number of elements in each bin after calling np.histogram  \n",
    "    :rtype: class: `numpy.ndarray`,class: `numpy.ndarray`,class: `numpy.ndarray`\n",
    "    \"\"\"\n",
    "    \n",
    "    arr_coord= np.array(list_coords)\n",
    "\n",
    "    mean_coords = np.mean(arr_coord)\n",
    "    std_coords = np.std(arr_coord)\n",
    "    no_outlier_coords = arr_coord[np.abs(arr_coord - mean_coords) < 3 * std_coords]\n",
    "\n",
    "    #Binning\n",
    "    counts, bin_edges = np.histogram(no_outlier_coords, bins=nbins)\n",
    "\n",
    "    #Detect peaks\n",
    "    counts_norm = (counts - np.min(counts)) / (np.max(counts) - np.min(counts)) #Normalization \n",
    "    select_peaks,_ = find_peaks(counts_norm, distance = distance_bwtn_peaks, prominence = peak_threshold) \n",
    "    #print (\"select peaks\", select_peaks)  \n",
    "    return select_peaks, bin_edges, counts\n",
    "\n",
    "# From scan2data > metadata_translation > leftside_metadata_grid_mapping\n",
    "def get_leftside_metadata_grid_mapping(list_x_digit,list_y_digit,dir_name,\n",
    "                      difference_ratio=0.5,use_defaults=True):\n",
    "    \n",
    "    \"\"\"Determines and returns the the mapping between coordinate values on a metadata image \n",
    "    and metadata labels in a subdirectory, for metadata of types dot and digits, as well as returns \n",
    "    the histogram used to generate each mapping\n",
    "    \n",
    "    \"\"\"\n",
    "    # Dictionary of dictionaries that map labels to coordinate point in metadata\n",
    "    all_labels = [LABELS_CAT_NUM, LABELS_NUM]\n",
    "    all_dict_mapping = {}\n",
    "    all_dict_hist = {}\n",
    "    # Different protocols depending on the type of dictionary mappings\n",
    "    for i, list_coord in (enumerate([list_x_digit,list_y_digit])):\n",
    "        type_dict = LABELS_DICT[i]\n",
    "        labels = all_labels[i]\n",
    "        try:\n",
    "            if 'cat' in type_dict:\n",
    "                if type_dict == 'dict_cat_digit':\n",
    "                    #if any([dir_dot in dir_name for dir_dot in LIST_DIRECTORY_DOTS]):\n",
    "                        #mean_dist_default,first_peak_default,last_peak_default=DEFAULT_DICT_CAT_DIGIT_F\n",
    "                    #else:\n",
    "                    mean_dist_default,first_peak_default,last_peak_default = DEFAULT_DICT_CAT_DIGIT #53, 21, 661\n",
    "            \n",
    "                #elif type_dict == 'dict_cat_dot':\n",
    "                    #mean_dist_default,first_peak_default,last_peak_default = DEFAULT_DICT_CAT_DOT\n",
    "                try:\n",
    "                    idx_peaks,bin_edges,counts = indices_highest_bin(list_coord, 500, 0.2, 30)\n",
    "                    peaks = bin_edges[np.array(idx_peaks)] #coordinate values on a metadata image probably corresponding to metadata\n",
    "                    \n",
    "                    n_labels = len(labels)\n",
    "                    first_peak = peaks[0]\n",
    "                    last_peak = peaks[-1]\n",
    "\n",
    "                    if use_defaults and abs(last_peak -last_peak_default)  > difference_ratio*mean_dist_default:\n",
    "                        last_peak = last_peak_default\n",
    "                    if use_defaults and abs(first_peak -first_peak_default)  > difference_ratio*mean_dist_default:\n",
    "                        first_peak = first_peak_default\n",
    "                    \n",
    "                    mean_dist_btw_peaks = (last_peak - first_peak)/(n_labels - 1)\n",
    "                    list_peaks = [int(round(first_peak + i* mean_dist_btw_peaks)) for i in range(0, n_labels)]\n",
    "                    \n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] = (idx_peaks,bin_edges,counts)\n",
    "                    print (\"[try]dict type\", type_dict)\n",
    "                \n",
    "\n",
    "                except:\n",
    "                    last_peak = last_peak_default\n",
    "                    first_peak = first_peak_default\n",
    "                    mean_dist_btw_peaks = mean_dist_default\n",
    "                    n_labels = len(labels)\n",
    "                    list_peaks = [int(round(first_peak + i* mean_dist_btw_peaks)) for i in range(0, n_labels)]\n",
    "                    \n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] = {}\n",
    "                    print (\"[except]dict type\", type_dict)\n",
    "                \n",
    "            elif 'num' in type_dict:\n",
    "                if  type_dict == 'dict_num_digit':\n",
    "                    #if any([dir_dot in dir_name for dir_dot in LIST_DIRECTORY_DOTS]):\n",
    "                        #mean_dist_default,peak_0_default,dist_btw_peaks = DEFAULT_DICT_NUM_DIGIT_F\n",
    "                    #else:\n",
    "                    mean_dist_default, peak_0_default, dist_btw_peaks = DEFAULT_DICT_NUM_DIGIT # 47, 41, 20\n",
    "                #elif type_dict == 'dict_num_dot':\n",
    "                    #mean_dist_default,peak_0_default,dist_btw_peaks= DEFAULT_DICT_NUM_DOT\n",
    "\n",
    "                    \n",
    "                try:\n",
    "                    idx_peaks, bin_edges, counts = indices_highest_bin(list_coord, 500, 0.2, dist_btw_peaks)\n",
    "                    arr_idx_peaks = np.array(idx_peaks)\n",
    "                    print (\"arr idx peaks\", arr_idx_peaks)\n",
    "                    print (\"bin edges\", bin_edges)\n",
    "                    peaks = bin_edges[arr_idx_peaks]\n",
    "                    peak_0 = peaks[0]\n",
    "                    if use_defaults and abs(peak_0 - peak_0_default)  > difference_ratio*mean_dist_default:\n",
    "                        peak_0 = peak_0_default\n",
    "                    #print (\"peak_0\", peak_0)\n",
    "                    # only first three peaks are deemed relevant\n",
    "                    if len(peaks) < 3:\n",
    "                        max_idx = 2\n",
    "                    else:\n",
    "                        max_idx = 3\n",
    "                    #print (\"max idx\", max_idx)\n",
    "                    \n",
    "                    #mean_dist_btw_peaks = np.mean([peaks[i+1]- peaks[i] for i in range(0, max_idx)])\n",
    "                    mean_dist_btw_peaks = np.mean([peaks[1]-peaks[0]])\n",
    "                    print (\"mean dist\", mean_dist_btw_peaks)\n",
    "                    if use_defaults and abs(mean_dist_btw_peaks - mean_dist_default)  > difference_ratio*dist_btw_peaks:\n",
    "                        mean_dist_btw_peaks = mean_dist_default\n",
    "\n",
    "                    list_peaks = [int(round(peak_0 + i* mean_dist_btw_peaks)) for i in range(len(labels))]\n",
    "                \n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] = (idx_peaks,bin_edges,counts)\n",
    "                    print (\"[try]dict type\", type_dict)\n",
    "                except:\n",
    "                    peak_0 = peak_0_default\n",
    "                    mean_dist_btw_peaks = mean_dist_default\n",
    "                    list_peaks = [int(round(peak_0 + i* mean_dist_btw_peaks)) for i in range(len(labels))]\n",
    "                    all_dict_mapping[type_dict] =dict(zip(list_peaks,labels))\n",
    "                    all_dict_hist[type_dict] =  {}\n",
    "                    print (\"[except]dict type\", type_dict)\n",
    "        except:\n",
    "            all_dict_mapping[type_dict] ={}\n",
    "            all_dict_hist[type_dict] =  {}\n",
    "    \n",
    "    return all_dict_mapping, all_dict_hist\n",
    "\n",
    "#From scan2data > metadata_translation > translate_leftside_metadata\n",
    "def map_coord_to_metadata(list_cat_coord,list_num_coord,dict_mapping_cat, dict_mapping_num):\n",
    "    \"\"\"Map coordinate of metadata centroids to information\n",
    "    \n",
    "    :param list_cat_coord: list of metadata positions to map to categories   \n",
    "    :type list_cat_coord: list\n",
    "    :param list_num_coord: list of metadata positions to map to numbers \n",
    "    :type list_num_coord: list\n",
    "    :param dict_mapping_cat: dictionary used to map coordinate positions to categories\n",
    "    :type dict_mapping_cat: dict\n",
    "    :param dict_mapping_num: dictionary used to map coordinate positions to numbers\n",
    "    :type dict_mapping_num: dict\n",
    "    :returns: dict_metadata\n",
    "    :rtype: dict\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    list_coord = list(zip(list_cat_coord,list_num_coord))\n",
    "    coord_mapping_cat = dict_mapping_cat.keys()\n",
    "    coord_mapping_num = dict_mapping_num.keys()\n",
    "        \n",
    "    dict_metadata={}\n",
    "    for cat_coord, num_coord in list_coord:\n",
    "        cat_key = min(coord_mapping_cat, key=lambda x:abs(x-cat_coord))\n",
    "        num_key = min(coord_mapping_num, key=lambda x:abs(x-num_coord))\n",
    "    \n",
    "        cat = dict_mapping_cat[cat_key]\n",
    "        num = dict_mapping_num[num_key]\n",
    "            \n",
    "        if cat in dict_metadata:\n",
    "            dict_metadata[cat].append(num)\n",
    "        else:\n",
    "            dict_metadata[cat] = [num]\n",
    "    return dict_metadata\n",
    "    #except:\n",
    "        #return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From scan2data> image_segmentation > segment_images_in_subdir.py\n",
    "#some variables here are not necessary and can be removed, ie. height, width...\n",
    "def segment_metadata(subdir_location, regex_img, min_bottom_height = 25, cutoff_width=300, cutoff_height=150):\n",
    "    \"\"\"Should only segment metadata. Can be adjusted to include ionogram. \"\"\"\n",
    "    regex_raw_image =  SD_PATH + (\"/*\")\n",
    "    #print (\"the raw images path is:\", regex_raw_image)\n",
    "    list_images = glob.glob(regex_raw_image)\n",
    "    \n",
    "    #Dataframe is processing\n",
    "    df_img = pd.DataFrame(data = {\"file_name\": list_images})\n",
    "    #Read each image in a 2D UTF-8 grayscale array\n",
    "    df_img[\"raw\"] = df_img['file_name'].map(lambda file_name: cv2.imread(file_name, 0))\n",
    "\n",
    "    # Extract ionogram and coordinates delimiting its limits\n",
    "    #df_img['limits']= list(zip(df_img['raw'].map(lambda raw_img: extract_ionogram(raw_img)))) \n",
    "    # Record the files whose ionogram extraction was not successful\n",
    "    #df_loss_ion_extraction, loss_ion_extraction = record_loss(df_img,'image_segmentation.extract_ionogram_from_scan.extract_ionogram',subdir_location)\n",
    "    #df_img = df_img[~loss_ion_extraction]\n",
    "    #df_img['height'],df_img['width'] = list(zip(df_img['ionogram'].map(lambda array_pixels: array_pixels.shape)))\n",
    "    \n",
    "    #Raw metadata\n",
    "    #df_tmp = (df_img.apply(lambda row: extract_metadata(row['raw'], row['limits']), axis = 1, result_type = 'expand'))\n",
    "    #df_img = df_img.assign(metadata_type = df_tmp[0])\n",
    "    #df_img = df_img.assign(raw_metadata = df_tmp[1])\n",
    "    #extract_metadata is function from extract_metadata_from_scan\n",
    "    \n",
    "    # There should be no metadata on left and top, especially after flipping\n",
    "    #outlier_metadata_location = np.any([df_img['metadata_type'] == 'right',df_img['metadata_type']=='top', df_img['metadata_type'] == 'left'], axis=0)\n",
    "    #df_outlier_metadata_location ,_ =  record_loss(df_img,'image_segmentation.segment_images_in_subdir.segment_images: metadata not on left or bottom',subdir_location,\n",
    "                                         #['file_name','metadata_type'],outlier_metadata_location )\n",
    "    \n",
    "    #if not df_outlier_metadata_location.empty:\n",
    "        #df_outlier_metadata_location['details'] = df_outlier_metadata_location.apply(lambda row: str(row['metadata_type']),1)\n",
    "        #df_outlier_metadata_location = df_outlier_metadata_location[['file_name','func_name','subdir_name','details']]\n",
    "    #else:\n",
    "        #df_outlier_metadata_location = df_outlier_metadata_location[['file_name','func_name','subdir_name']]\n",
    "    \n",
    "    # Remove loss from detected metadata not being on the left or bottom\n",
    "    #df_img = df_img[~outlier_metadata_location]\n",
    "\n",
    "    #Trimmed metadata\n",
    "    #df_img['trimmed_metadata'] = list(zip(*df_img.apply(lambda row: trimming_metadata(row['raw_metadata'], row['metadata_type']), axis = 1, result_type = 'expand')))\n",
    "    df_trim_tmp = (df_img.apply(lambda row: trimming_metadata(row[\"raw\"]), axis = 1))\n",
    "    df_img = df_img.assign(trimmed_metadata = df_trim_tmp)\n",
    "    df_loss_trim, loss_trim = record_loss(df_img, 'image_segmentation.trim_raw_metadata.trimming_metadata', subdir_location)\n",
    "    #trimming_metadata is a function from trim_raw_metadata\n",
    "    #record_loss is a function from helper_functions\n",
    "    \n",
    "    df_img = df_img[~loss_trim]\n",
    "    # Check if metadata too small\n",
    "    #df_map_tmp = df_img['trimmed_metadata'].map(lambda array_pixels: np.shape(array_pixels), axis = 1)\n",
    "    df_map_tmp = df_img.apply(lambda row: np.shape(row['trimmed_metadata']), axis = 1, result_type = 'expand')\n",
    "    df_img = df_img.assign(meta_height = df_map_tmp[0])\n",
    "    df_img = df_img.assign(meta_width = df_map_tmp[1])\n",
    "\n",
    "    outlier_size_metadata = (np.logical_and(df_img['meta_height'] < min_bottom_height, df_img['meta_height'] < min_bottom_height))    \n",
    "        \n",
    "    df_outlier_metadata_size, _ = record_loss(df_img,'image_segmentation.segment_images_in_subdir.segment_images: metadata size outlier',subdir_location,\n",
    "                                           ['file_name','meta_height','meta_width'], outlier_size_metadata)\n",
    "\n",
    "    if not df_outlier_metadata_size.empty:\n",
    "        df_outlier_metadata_size['details'] = df_outlier_metadata_size.apply(lambda row: str(row['meta_height']) + str(row['meta_width']), 1)\n",
    "        df_outlier_metadata_size = df_outlier_metadata_size[['file_name','func_name','subdir_name','details']]\n",
    "        \n",
    "    else:\n",
    "        df_outlier_metadata_size = df_outlier_metadata_size[['file_name','func_name','subdir_name']]\n",
    "    \n",
    "    # Remove files whose metadata too small\n",
    "    df_img = df_img[~outlier_size_metadata]\n",
    "    \n",
    "    # Dataframe recording loss from programming errors\n",
    "    df_loss = pd.concat([df_loss_trim])\n",
    "    \n",
    "    # Dataframe recording loss from various filters i.e. metadata too small, ionogram too small/big\n",
    "    df_outlier = pd.concat([df_outlier_metadata_size])\n",
    "\n",
    "    return df_img,  df_loss, df_outlier\n",
    "\n",
    "#From scan2data > metadata_translation > translate_leftside_metadata.py\n",
    "def get_bottomside_metadata (df_img, subdir_location, kernal_size =(1, 1)):\n",
    "    \"\"\"Reads the metadata\"\"\"\n",
    "\n",
    "    kernel_dilation = np.ones(kernal_size, np.uint8)\n",
    "\n",
    "    #df_dilate_tmp = df_img.apply(lambda trimmed_meta: cv2.dilate(trimmed_meta, kernel_dilation))\n",
    "    df_dilate_tmp = df_img['trimmed_metadata'].map(lambda trimmed_meta: cv2.dilate(trimmed_meta, kernel_dilation))\n",
    "    df_img = df_img.assign(dilated_metadata = df_dilate_tmp)\n",
    "\n",
    "\n",
    "    #df_img['x_centroids'], df_img['y_centroids'], df_img['is_dot'] = zip(*df_img.apply(lambda row: extract_centroids(row['dilated_metadata'], row['file_name']), 1))\n",
    "    df_cent_tmp = df_img.apply(lambda row: extract_centroids(row['dilated_metadata'], row['file_name']), axis = 1, result_type = 'expand')\n",
    "    #df_cent_tmp.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/output-cent-tmp.csv\")\n",
    "    df_img = df_img.assign(x_centroids = df_cent_tmp[0])\n",
    "    df_img = df_img.assign(y_centroids = df_cent_tmp[1])\n",
    "\n",
    "    df_loss_centroids_extraction, loss_centroids_extraction = record_loss(df_img,'metadata_translation.determine_metadata_grid_mapping.extract_centroids_',subdir_location)\n",
    "    # extract_centroids and record_loss are two other functions \n",
    "\n",
    "    # Remove files where the extraction didn't work\n",
    "    df_img = df_img[~loss_centroids_extraction]\n",
    "    # ^removes them from the main dataframe\n",
    "    \n",
    "    #df_num_subset = df_img[np.invert(np.array(df_img['is_dot']))]\n",
    "    list_x_digit, list_y_digit = [0], [0]\n",
    "    list_x_digit = list(chain(*df_img['x_centroids'].tolist()))\n",
    "    list_y_digit = list(chain(*df_img['y_centroids'].tolist()))\n",
    "    dict_mapping, dict_hist = get_leftside_metadata_grid_mapping(list_x_digit, list_y_digit, subdir_location)\n",
    "\n",
    "\n",
    "    # Determine the value of metadata based on the mappings\n",
    "    #print (dict_mapping)\n",
    "    df_dict_meta = df_img.apply(lambda row: map_coord_to_metadata(row['x_centroids'], row['y_centroids'],\n",
    "                                                                      dict_mapping['dict_cat_digit'],\n",
    "                                                                      dict_mapping['dict_num_digit']), axis = 1)\n",
    "    df_img = df_img.assign(dict_metadata = df_dict_meta)\n",
    "    #df_img.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/01-mapping-to-output.csv\")\n",
    "    \n",
    "    df_loss_mapping, loss_mapping = record_loss(df_img,'map_coord_to_metadata', subdir_location)\n",
    "    df_img = df_img[~loss_mapping]\n",
    "    \n",
    "    \n",
    "    df_loss = pd.concat([df_loss_centroids_extraction, df_loss_mapping],ignore_index=True)\n",
    "\n",
    "    return df_img, df_loss, dict_mapping, dict_hist\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From process_directory.py\n",
    "def process_subdir(subdir_path, regex_images):\n",
    "    \"\"\"Transform raw scanned images in subdir into information\"\"\"\n",
    "\n",
    "    #Run segment_images on subdir\n",
    "    df_img, df_loss, df_outlier = segment_metadata(subdir_path, regex_images)\n",
    "    #df_img.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/segment-output.csv\")\n",
    "\n",
    "    #Translate metadata on bottom\n",
    "    #df_img_bottom =  df_img.loc[df_img['metadata_type'] == 'bottom']\n",
    "    df_img_bottom, df_loss_meta_bottom, _, __ = get_bottomside_metadata(df_img, subdir_path) #from metadata_translation.translate_bottomside_metadata\n",
    "    df_loss = pd.concat([df_loss_meta_bottom], ignore_index=True)\n",
    "\n",
    "    df_processed = df_img_bottom\n",
    "    #df_loss = pd.concat([df_loss_coord_bottom], ignore_index=True)\n",
    "\n",
    "    df_processed.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/01-processed-output.csv\")\n",
    "    df_loss.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/02-loss-output.csv\")\n",
    "    df_outlier.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/03-outlier-output.csv\")\n",
    "    return df_processed, df_loss, df_outlier\n",
    "\n",
    "#print (process_subdir(SD_PATH, '/*'))\n",
    "def process_df_bottomside_metadata(df_processed, subdir_name, source_dir):\n",
    "\n",
    "    df_final_data = df_processed[['file_name', 'dict_metadata']]\n",
    "    df_final_data['subdir_name'] = subdir_name\n",
    "    labels = ['Operating Mode 1','Operating Mode 2','Station Number 1', 'Station Number 2', 'Year 1', 'Year 2', 'Day 1', 'Day 2', 'Day 3', 'Hour 1', 'Hour 2', 'Min 1', 'Min 2', 'Sec 1', 'Sec 2']\n",
    "\n",
    "    for label in labels:\n",
    "        df_final_data[label] = df_final_data['dict_metadata'].map(\n",
    "            lambda dict_meta: sum(dict_meta[label]) if label in dict_meta.keys() else 0)\n",
    "\n",
    "    del df_final_data['dict_metadata']\n",
    "\n",
    "    #df_final_data['year'] = df_final_data['year'] + 1900\n",
    "    #df_final_data['day'] = df_final_data['day_1'].astype(str) + df_final_data['day_2'].astype(str) + df_final_data['day_3'].astype(str) \n",
    "    #df_final_data['hour'] = df_final_data['hour_1'].astype(str) + df_final_data['hour_2'].astype(str) \n",
    "    #df_final_data['minute'] = df_final_data['minute_1'].astype(str) + df_final_data['minute_2'].astype(str) \n",
    "    #df_final_data['second'] = df_final_data['second_1'].astype(str) + df_final_data['second_2'].astype(str) \n",
    "    #df_final_data['station_number'] = df_final_data['station_number_1'].astype(str) + df_final_data['station_number_2'].astype(str) \n",
    "    #df_final_data['day'] = df_final_data['day'].astype(int)\n",
    "    #df_final_data['hour'] = df_final_data['hour'].astype(int)\n",
    "    #df_final_data['minute'] = df_final_data['minute'].astype(int)\n",
    "    df_final_data['Operating_Mode'] = df_final_data[\"Operating Mode 1\"].astype(str) + df_final_data[\"Operating Mode 2\"].astype(str) \n",
    "    df_final_data[\"Station_Number\"] = df_final_data['Station Number 1'].astype(str) + df_final_data['Station Number 2'].astype(str)\n",
    "    df_final_data['Year'] = (df_final_data['Year 1'].astype(str) + df_final_data['Year 2'].astype(str)).astype(int) + 1900\n",
    "    df_final_data[\"Day\"] = (df_final_data['Day 1'].astype(str) + df_final_data['Day 2'].astype(str) + df_final_data['Day 3'].astype(str)).astype(int)\n",
    "    df_final_data[\"Hour\"] = (df_final_data['Hour 1'].astype(str) + df_final_data['Hour 2'].astype(str)).astype(int)\n",
    "    df_final_data[\"Min\"] = (df_final_data['Min 1'].astype(str) + df_final_data['Min 2'].astype(str)).astype(int)\n",
    "    df_final_data[\"Sec\"] = (df_final_data['Sec 1'].astype(str) + df_final_data['Sec 2'].astype(str)).astype(int)\n",
    "\n",
    "\n",
    "    #if len(df_final_data) > 0:          \n",
    "        #code_list_of_station_after1965 = pd.read_csv(source_dir + 'Post_July_1_1965_Code_List_Station.csv')\n",
    "        #code_list_of_station_before1963 = pd.read_csv(source_dir + 'Pre_1963_Code_List_Station.csv')\n",
    "        #code_list_of_station_between1963_1964 = pd.read_csv(source_dir + '1963_1964.csv')\n",
    "        #df_result_after1965 = pd.merge(df_final_data.loc[df_final_data['year'] >= 1965], \n",
    "                                       #code_list_of_station_after1965, on='station_number')\n",
    "        #df_result_before1963 = pd.merge(df_final_data.loc[df_final_data['year'] <= 1963],\n",
    "                                        #code_list_of_station_before1963, on='station_number')\n",
    "        #df_result_mid1964 = pd.merge (df_final_data.loc[df_final_data['year'] == 1964],\n",
    "                                        #code_list_of_station_between1963_1964, on = 'station_number')\n",
    "        #df_final_result = pd.concat([df_result_before1963, df_result_mid1964, df_result_after1965]).reset_index(drop=True)\n",
    "        #df_final_result = df_result_before1963.append(df_result_after1965.append(df_result_mid1964, ignore_index=True)) #Why was pd.concat not used?        \n",
    "    #else:\n",
    "        #df_final_result = pd.DataFrame()\n",
    "    \n",
    "    df_final_data.to_csv(\"C:/Users/spunchiwickrama/Documents/Projects/ISIS_I/final_processed.csv\")\n",
    "    return df_final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[try]dict type dict_cat_digit\n",
      "arr idx peaks [166 190]\n",
      "bin edges [ 2.     2.042  2.084  2.126  2.168  2.21   2.252  2.294  2.336  2.378\n",
      "  2.42   2.462  2.504  2.546  2.588  2.63   2.672  2.714  2.756  2.798\n",
      "  2.84   2.882  2.924  2.966  3.008  3.05   3.092  3.134  3.176  3.218\n",
      "  3.26   3.302  3.344  3.386  3.428  3.47   3.512  3.554  3.596  3.638\n",
      "  3.68   3.722  3.764  3.806  3.848  3.89   3.932  3.974  4.016  4.058\n",
      "  4.1    4.142  4.184  4.226  4.268  4.31   4.352  4.394  4.436  4.478\n",
      "  4.52   4.562  4.604  4.646  4.688  4.73   4.772  4.814  4.856  4.898\n",
      "  4.94   4.982  5.024  5.066  5.108  5.15   5.192  5.234  5.276  5.318\n",
      "  5.36   5.402  5.444  5.486  5.528  5.57   5.612  5.654  5.696  5.738\n",
      "  5.78   5.822  5.864  5.906  5.948  5.99   6.032  6.074  6.116  6.158\n",
      "  6.2    6.242  6.284  6.326  6.368  6.41   6.452  6.494  6.536  6.578\n",
      "  6.62   6.662  6.704  6.746  6.788  6.83   6.872  6.914  6.956  6.998\n",
      "  7.04   7.082  7.124  7.166  7.208  7.25   7.292  7.334  7.376  7.418\n",
      "  7.46   7.502  7.544  7.586  7.628  7.67   7.712  7.754  7.796  7.838\n",
      "  7.88   7.922  7.964  8.006  8.048  8.09   8.132  8.174  8.216  8.258\n",
      "  8.3    8.342  8.384  8.426  8.468  8.51   8.552  8.594  8.636  8.678\n",
      "  8.72   8.762  8.804  8.846  8.888  8.93   8.972  9.014  9.056  9.098\n",
      "  9.14   9.182  9.224  9.266  9.308  9.35   9.392  9.434  9.476  9.518\n",
      "  9.56   9.602  9.644  9.686  9.728  9.77   9.812  9.854  9.896  9.938\n",
      "  9.98  10.022 10.064 10.106 10.148 10.19  10.232 10.274 10.316 10.358\n",
      " 10.4   10.442 10.484 10.526 10.568 10.61  10.652 10.694 10.736 10.778\n",
      " 10.82  10.862 10.904 10.946 10.988 11.03  11.072 11.114 11.156 11.198\n",
      " 11.24  11.282 11.324 11.366 11.408 11.45  11.492 11.534 11.576 11.618\n",
      " 11.66  11.702 11.744 11.786 11.828 11.87  11.912 11.954 11.996 12.038\n",
      " 12.08  12.122 12.164 12.206 12.248 12.29  12.332 12.374 12.416 12.458\n",
      " 12.5   12.542 12.584 12.626 12.668 12.71  12.752 12.794 12.836 12.878\n",
      " 12.92  12.962 13.004 13.046 13.088 13.13  13.172 13.214 13.256 13.298\n",
      " 13.34  13.382 13.424 13.466 13.508 13.55  13.592 13.634 13.676 13.718\n",
      " 13.76  13.802 13.844 13.886 13.928 13.97  14.012 14.054 14.096 14.138\n",
      " 14.18  14.222 14.264 14.306 14.348 14.39  14.432 14.474 14.516 14.558\n",
      " 14.6   14.642 14.684 14.726 14.768 14.81  14.852 14.894 14.936 14.978\n",
      " 15.02  15.062 15.104 15.146 15.188 15.23  15.272 15.314 15.356 15.398\n",
      " 15.44  15.482 15.524 15.566 15.608 15.65  15.692 15.734 15.776 15.818\n",
      " 15.86  15.902 15.944 15.986 16.028 16.07  16.112 16.154 16.196 16.238\n",
      " 16.28  16.322 16.364 16.406 16.448 16.49  16.532 16.574 16.616 16.658\n",
      " 16.7   16.742 16.784 16.826 16.868 16.91  16.952 16.994 17.036 17.078\n",
      " 17.12  17.162 17.204 17.246 17.288 17.33  17.372 17.414 17.456 17.498\n",
      " 17.54  17.582 17.624 17.666 17.708 17.75  17.792 17.834 17.876 17.918\n",
      " 17.96  18.002 18.044 18.086 18.128 18.17  18.212 18.254 18.296 18.338\n",
      " 18.38  18.422 18.464 18.506 18.548 18.59  18.632 18.674 18.716 18.758\n",
      " 18.8   18.842 18.884 18.926 18.968 19.01  19.052 19.094 19.136 19.178\n",
      " 19.22  19.262 19.304 19.346 19.388 19.43  19.472 19.514 19.556 19.598\n",
      " 19.64  19.682 19.724 19.766 19.808 19.85  19.892 19.934 19.976 20.018\n",
      " 20.06  20.102 20.144 20.186 20.228 20.27  20.312 20.354 20.396 20.438\n",
      " 20.48  20.522 20.564 20.606 20.648 20.69  20.732 20.774 20.816 20.858\n",
      " 20.9   20.942 20.984 21.026 21.068 21.11  21.152 21.194 21.236 21.278\n",
      " 21.32  21.362 21.404 21.446 21.488 21.53  21.572 21.614 21.656 21.698\n",
      " 21.74  21.782 21.824 21.866 21.908 21.95  21.992 22.034 22.076 22.118\n",
      " 22.16  22.202 22.244 22.286 22.328 22.37  22.412 22.454 22.496 22.538\n",
      " 22.58  22.622 22.664 22.706 22.748 22.79  22.832 22.874 22.916 22.958\n",
      " 23.   ]\n",
      "mean dist 1.0079999999999991\n",
      "[try]dict type dict_num_digit\n",
      "                                            file_name  \\\n",
      "0   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "1   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "2   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "3   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "4   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "5   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "6   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "7   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "8   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "9   C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "10  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "11  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "12  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "13  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "14  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "16  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "17  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "18  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "19  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "20  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "21  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "22  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "23  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "24  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "25  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "26  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "27  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "28  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "29  C:/Users/spunchiwickrama/Documents/Projects/IS...   \n",
      "\n",
      "                                          subdir_name  Operating Mode 1  \\\n",
      "0   C:/Users/spunchiwickrama/Documents/Projects/IS...                 2   \n",
      "1   C:/Users/spunchiwickrama/Documents/Projects/IS...                 0   \n",
      "2   C:/Users/spunchiwickrama/Documents/Projects/IS...                 0   \n",
      "3   C:/Users/spunchiwickrama/Documents/Projects/IS...                 0   \n",
      "4   C:/Users/spunchiwickrama/Documents/Projects/IS...                 0   \n",
      "5   C:/Users/spunchiwickrama/Documents/Projects/IS...                 0   \n",
      "6   C:/Users/spunchiwickrama/Documents/Projects/IS...                 1   \n",
      "7   C:/Users/spunchiwickrama/Documents/Projects/IS...                 0   \n",
      "8   C:/Users/spunchiwickrama/Documents/Projects/IS...                 0   \n",
      "9   C:/Users/spunchiwickrama/Documents/Projects/IS...                 0   \n",
      "10  C:/Users/spunchiwickrama/Documents/Projects/IS...                 0   \n",
      "11  C:/Users/spunchiwickrama/Documents/Projects/IS...                 0   \n",
      "12  C:/Users/spunchiwickrama/Documents/Projects/IS...                 3   \n",
      "13  C:/Users/spunchiwickrama/Documents/Projects/IS...                 0   \n",
      "14  C:/Users/spunchiwickrama/Documents/Projects/IS...                 1   \n",
      "16  C:/Users/spunchiwickrama/Documents/Projects/IS...                 0   \n",
      "17  C:/Users/spunchiwickrama/Documents/Projects/IS...                 1   \n",
      "18  C:/Users/spunchiwickrama/Documents/Projects/IS...                 2   \n",
      "19  C:/Users/spunchiwickrama/Documents/Projects/IS...                 4   \n",
      "20  C:/Users/spunchiwickrama/Documents/Projects/IS...                 2   \n",
      "21  C:/Users/spunchiwickrama/Documents/Projects/IS...                 2   \n",
      "22  C:/Users/spunchiwickrama/Documents/Projects/IS...                 1   \n",
      "23  C:/Users/spunchiwickrama/Documents/Projects/IS...                 3   \n",
      "24  C:/Users/spunchiwickrama/Documents/Projects/IS...                 2   \n",
      "25  C:/Users/spunchiwickrama/Documents/Projects/IS...                 2   \n",
      "26  C:/Users/spunchiwickrama/Documents/Projects/IS...                 3   \n",
      "27  C:/Users/spunchiwickrama/Documents/Projects/IS...                 5   \n",
      "28  C:/Users/spunchiwickrama/Documents/Projects/IS...                 2   \n",
      "29  C:/Users/spunchiwickrama/Documents/Projects/IS...                 3   \n",
      "\n",
      "    Operating Mode 2  Station Number 1  Station Number 2  Year 1  Year 2  \\\n",
      "0                  1                 1                 0       2       2   \n",
      "1                  0                 0                 0       1       4   \n",
      "2                  0                 0                 0       0       3   \n",
      "3                  0                 0                 0       1       3   \n",
      "4                  0                 0                 0       0       3   \n",
      "5                  0                 0                 0       3       2   \n",
      "6                  0                 0                 0       0       5   \n",
      "7                  0                 1                 0       0       2   \n",
      "8                  0                 0                 0       0       4   \n",
      "9                  0                 0                 0       0       2   \n",
      "10                 1                 0                 0       0       3   \n",
      "11                 0                 0                 0       0       2   \n",
      "12                 3                 3                 2       2       4   \n",
      "13                 0                 1                 0       1       0   \n",
      "14                 0                 0                 0       0       0   \n",
      "16                 4                 7                 1       5       4   \n",
      "17                 1                 2                 2       3       4   \n",
      "18                 2                 1                 2       4       4   \n",
      "19                 1                 1                 3       1       4   \n",
      "20                 1                 2                 5       2       4   \n",
      "21                 2                 1                 1       0       5   \n",
      "22                 0                 1                 1       3       4   \n",
      "23                 2                 2                 0       1       3   \n",
      "24                 0                 0                 1       2       4   \n",
      "25                 2                 1                 3       4       4   \n",
      "26                 0                 1                 2       1       4   \n",
      "27                 2                 0                 1       2       6   \n",
      "28                 0                 1                 0       2       4   \n",
      "29                 1                 1                 1       4       4   \n",
      "\n",
      "    Day 1  Day 2  ...  Min 2  Sec 1  Sec 2  Operating_Mode  Station_Number  \\\n",
      "0       4      2  ...      1      0      0              21              10   \n",
      "1       2      1  ...      0      0      0              00              00   \n",
      "2       2      2  ...      0      0      0              00              00   \n",
      "3       3      2  ...      1      0      0              00              00   \n",
      "4       2      2  ...      0      0      0              00              00   \n",
      "5       2      3  ...      0      0      0              00              00   \n",
      "6       2      2  ...      0      0      0              10              00   \n",
      "7       2      2  ...      0      0      0              00              10   \n",
      "8       2      2  ...      0      0      0              00              00   \n",
      "9       2      2  ...      0      0      0              00              00   \n",
      "10      2      2  ...      0      0      0              01              00   \n",
      "11      2      2  ...      0      0      0              00              00   \n",
      "12      3      8  ...      1      1      0              33              32   \n",
      "13      2      0  ...      0      0      0              00              10   \n",
      "14      0      0  ...      0      0      0              10              00   \n",
      "16      3      6  ...      0      0      0              04              71   \n",
      "17      4      3  ...      5      1      0              11              22   \n",
      "18      3      3  ...      3      1      0              22              12   \n",
      "19      3      1  ...      3      0      0              41              13   \n",
      "20      3      1  ...      3      0      0              21              25   \n",
      "21      3      3  ...      1      1      0              22              11   \n",
      "22      2      4  ...      2      0      0              10              11   \n",
      "23      4      2  ...      4      0      0              32              20   \n",
      "24      1      0  ...      5      1      0              20              01   \n",
      "25      2      3  ...      1      0      0              22              13   \n",
      "26      4      2  ...      3      0      0              30              12   \n",
      "27      2      3  ...      1      1      0              52              01   \n",
      "28      4      2  ...      0      0      0              20              10   \n",
      "29      3      3  ...      1      0      0              31              11   \n",
      "\n",
      "    Year  Day Hour Min  Sec  \n",
      "0   1922  421    0   1    0  \n",
      "1   1914  213   21   0    0  \n",
      "2   1903  222   24  10    0  \n",
      "3   1913  322   41   1    0  \n",
      "4   1903  222   31   0    0  \n",
      "5   1932  232   21   0    0  \n",
      "6   1905  222   51   0    0  \n",
      "7   1902  223   24  10    0  \n",
      "8   1904  222   51   0    0  \n",
      "9   1902  223   23  10    0  \n",
      "10  1903  223   22  10    0  \n",
      "11  1902  222   21  10    0  \n",
      "12  1924  384   45  11   10  \n",
      "13  1910  200    0   0    0  \n",
      "14  1900    0    0   0    0  \n",
      "16  1954  361   10   0    0  \n",
      "17  1934  432   13  35   10  \n",
      "18  1944  333   42  63   10  \n",
      "19  1914  314   33  23    0  \n",
      "20  1924  312   45  23    0  \n",
      "21  1905  334   36  31   10  \n",
      "22  1934  242   34  22    0  \n",
      "23  1913  421   33  34    0  \n",
      "24  1924  101   13  65   10  \n",
      "25  1944  233   35  21    0  \n",
      "26  1914  422   34  23    0  \n",
      "27  1926  235   25  31   10  \n",
      "28  1924  422   32   0    0  \n",
      "29  1944  333   13  11    0  \n",
      "\n",
      "[29 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spunchiwickrama\\AppData\\Local\\Temp\\ipykernel_21228\\1442895598.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_data['subdir_name'] = subdir_name\n",
      "C:\\Users\\spunchiwickrama\\AppData\\Local\\Temp\\ipykernel_21228\\1442895598.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_data[label] = df_final_data['dict_metadata'].map(\n",
      "C:\\Users\\spunchiwickrama\\AppData\\Local\\Temp\\ipykernel_21228\\1442895598.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_data[label] = df_final_data['dict_metadata'].map(\n",
      "C:\\Users\\spunchiwickrama\\AppData\\Local\\Temp\\ipykernel_21228\\1442895598.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_data[label] = df_final_data['dict_metadata'].map(\n",
      "C:\\Users\\spunchiwickrama\\AppData\\Local\\Temp\\ipykernel_21228\\1442895598.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_data[label] = df_final_data['dict_metadata'].map(\n",
      "C:\\Users\\spunchiwickrama\\AppData\\Local\\Temp\\ipykernel_21228\\1442895598.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_data[label] = df_final_data['dict_metadata'].map(\n"
     ]
    }
   ],
   "source": [
    "df_processed = process_subdir(SD_PATH, '/*')[0]\n",
    "print (process_df_bottomside_metadata(df_processed, SD_PATH, \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4210310140.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[75], line 12\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def get_peaks(list_coord):\n",
    "    \"\"\"\n",
    "#From a list of coordinates, return a list of the most common values through binning\n",
    "    \n",
    "    #:param list_coord: list of positions where metadata is detected\n",
    "    #:type list_coord: class: `list`\n",
    "    #:returns: peaks i.e. list of the most common values through binning\n",
    "    #:rtype: class: `numpy.ndarray`\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    idx_peaks,bin_edges,counts = indices_highest_bin(list_coord)\n",
    "    return bin_edges[np.array(idx_peaks)]\n",
    "\n",
    "def get_leftside_metadata_grid_peaks(regex_subdir, regex_images,\n",
    "                                     min_subset=10):\n",
    "    \"\"\"#Generates a dataframe containing information about peaks to generate metadata grids\n",
    "    \n",
    "    #:param regex_subdir: regular expression to extract subdirectory ex: 'E:/master/R*/[0-9]*/'\n",
    "    #:type regex_img: str\n",
    "    #:param regex_img: regular expression to extract images ex: '*.png'\n",
    "    #:type regex_img: str\n",
    "    #:param min_subset: minimum number of items extracted to be considered ,defaults to 10\n",
    "    #:type min_subset: int, optional\n",
    "    #:returns: df_summary_left_dot,df_summary_left_num,error_list: Dataframe listing peaks for dot metadata located on the left of ionogram,  Dataframe listing peaks for number metadata located on the left of ionogram, list of filenames leading to errors\n",
    "    #:rtype: `pandas.core.frame.DataFrame`,`pandas.core.frame.DataFrame`, list\n",
    "    \n",
    "    \"\"\"\n",
    "    # All the subdirectory i.e. R014207948/1743-9/\n",
    "    list_all_subdir = glob.glob(regex_subdir)\n",
    "\n",
    "    df_summary_left_dot = pd.DataFrame(columns=['meta_peaks_x','meta_peaks_y'] )\n",
    "    df_summary_left_num = pd.DataFrame(columns=['meta_peaks_x','meta_peaks_y'] )\n",
    "    error_list = []\n",
    "\n",
    "    for i,subdir_name in enumerate(list_all_subdir):\n",
    "        try:\n",
    "            df_img,_,_ =segment_metadata(subdir_name, regex_images)\n",
    "            df_img_subset = df_img[df_img['metadata_type']=='left']\n",
    "            if i % 50 == 0:\n",
    "                print(i)\n",
    "            \n",
    "            if len(df_img_subset.index) > min_subset + 1:\n",
    "            \n",
    "                '''\n",
    "                Following Code pasted from translate_leftside_metadata.get_leftside_metadata\n",
    "                '''\n",
    "                # Centroids extraction\n",
    "                df_img_subset['rotated_metadata'] = df_img_subset['trimmed_metadata'].map(lambda trimmed_meta: np.rot90(trimmed_meta,-1))\n",
    "                kernel_dilation = np.ones((1,1),np.uint8)\n",
    "                df_img_subset['dilated_metadata'] = df_img_subset['rotated_metadata'].map(lambda rotated_meta: cv2.dilate(rotated_meta,kernel_dilation))\n",
    "                df_img_subset['x_centroids'],df_img_subset['y_centroids'] = zip(*df_img_subset.apply(lambda row: extract_centroids(row['dilated_metadata'],row['file_name']),1))\n",
    "                _,loss_centroids_extraction = record_loss(df_img_subset,'metadata_translation.determine_leftside_metadata_grid_mapping.extract_centroids_and_determine_type',subdir_name)\n",
    "                  \n",
    "                # Remove files whose centroid metadata extraction was not successful\n",
    "                df_img_subset = df_img_subset[~loss_centroids_extraction]\n",
    "\n",
    "                '''\n",
    "                Above Code pasted from translate_leftside_metadata.get_leftside_metadata\n",
    "                '''\n",
    "                # Determine metadata mapping for dot-type metadata and num-type metadata\n",
    "                #df_dot_subset = df_img_subset[df_img_subset['is_dot'] == True]\n",
    "                #df_num_subset = df_img_subset[df_img_subset['is_dot'] == False]\n",
    "                \n",
    "\n",
    "                #list_x_dot, list_y_dot,\n",
    "                list_x_digit,list_y_digit = [0],[0]\n",
    "                #if not df_dot_subset.empty:\n",
    "                    #list_x_dot = list(chain(*df_dot_subset['x_centroids'].tolist()))\n",
    "                    #list_y_dot = list(chain(*df_dot_subset['y_centroids'].tolist()))\n",
    "                    #x_peaks_dot,y_peaks_dot = get_peaks(list_x_dot),get_peaks(list_y_dot )\n",
    "                    #to_apend_dot = pd.DataFrame({'meta_peaks_x':[x_peaks_dot],'meta_peaks_y':[y_peaks_dot]} )\n",
    "                    #df_summary_left_dot = df_summary_left_dot.append(to_apend_dot)\n",
    "                \n",
    "                if not df_num_subset.empty:\n",
    "                    list_x_digit = list(chain(*df_num_subset['x_centroids'].tolist()))\n",
    "                    list_y_digit = list(chain(*df_num_subset['y_centroids'].tolist()))\n",
    "                    x_peaks_num,y_peaks_num = get_peaks(list_x_digit),get_peaks(list_y_digit )\n",
    "                    to_apend_num = pd.DataFrame({'meta_peaks_x':[x_peaks_num],'meta_peaks_y':[y_peaks_num]} )\n",
    "                    df_summary_left_num = df_summary_left_num.append(to_apend_num)\n",
    "\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            error_list.append(subdir_name)\n",
    "            print(subdir_name )\n",
    "            \n",
    "    return df_summary_left_num,error_list\n",
    "\n",
    "def  plot_hist_peaks_grids(*all_df,\n",
    "                          nbins=500):\n",
    "    \"\"\" #Plots histogram to determine default values for the ionogram grids\n",
    "    \n",
    "    #:param *all_df: dataframes (from grid_default_values) whose values are to be plotted\n",
    "    #:type *all_df: tuple of or single class: `pandas.core.frame.DataFrame`\n",
    "    #:param nbins: number of bins used for histogram, defaults to 500\n",
    "    #:type nbins: int, optional \n",
    "    \"\"\"\n",
    "    nrow = len(*all_df)\n",
    "    fig,axes = plt.subplots(nrows=nrow,ncols=2)\n",
    "    ax = axes.ravel()\n",
    "\n",
    "    for i,df in enumerate(*all_df):\n",
    "        peaks_x = list(chain.from_iterable(df['meta_peaks_x']))\n",
    "        select_peaks_idx,bin_edges,counts = indices_highest_bin(peaks_x, 500, 0.2, 30)\n",
    "        bin_centers = (0.5*(bin_edges[1:] + bin_edges[:-1]))\n",
    "        peaks = bin_edges[np.array(select_peaks_idx)]\n",
    "        ax[2*i].plot(bin_centers,counts)\n",
    "        ax[2*i].plot(peaks,counts[select_peaks_idx], \"x\")\n",
    "        print(\"peaks\", peaks)\n",
    "        ax[2*i].set_title('meta_peaks_x')\n",
    "\n",
    "        peaks_y = list(chain.from_iterable(df['meta_peaks_y']))\n",
    "        select_peaks_idx,bin_edges,counts = indices_highest_bin(peaks_y, 500, 0.2, 30)\n",
    "        bin_centers = (0.5*(bin_edges[1:] + bin_edges[:-1]))\n",
    "        peaks = bin_edges[np.array(select_peaks_idx)]\n",
    "        ax[2*i+1].plot(bin_centers,counts)\n",
    "        ax[2*i+1].plot(peaks,counts[select_peaks_idx], \"x\")\n",
    "        print(\"peaks\", peaks)\n",
    "        ax[2*i+1].set_title('meta_peaks_y')\n",
    "\n",
    "\n",
    "df_summary_left_num1, error_list1 = get_leftside_metadata_grid_peaks(regex_subdir= SD_PATH, regex_images='*.png')\n",
    "#df_summary_left_dot2,df_summary_left_num2,error_list2 = get_leftside_metadata_grid_peaks(regex_subdir='G:/AlouetteData/Alouette Data/R*/[0-9]*[0-9]/', regex_images='Image*[0-9].png')\n",
    "print (plot_hist_peaks_grids((df_summary_left_num1)))\n",
    " #plot_hist_peaks_grids((df_summary_left_dot2,df_summary_left_num2)) \"\"\"\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
